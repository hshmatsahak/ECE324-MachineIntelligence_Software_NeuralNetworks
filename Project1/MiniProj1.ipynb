{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a logistic regression model to predict two-year recidivism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, import libraries\n",
    "- pandas for loading csv data into dataframe object, used to then convert into torch tensor which is the form our input assumes\n",
    "- numpy for linear algebra operations\n",
    "- sklearn.model_selection for its train_test_split function\n",
    "- torch for model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct train and test data sets\n",
    "- load from csv\n",
    "- use train_test_split function to create separate dataframe objects for the 2 sets\n",
    "- extract relevant columns to obtain X and y for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "According to Dressel, Julia, and Hany Farid. “The accuracy, fairness, and limits of predicting recidivism.”:\n",
    "Despite the impressive collection of 137 features, it would appear that a linear classifier based on only 2\n",
    "features—age and total number of previous convictions—is all that is\n",
    "required to yield the same prediction accuracy as COMPAS.\n",
    "'''\n",
    "\n",
    "# Extract data from .csv file related to Black and White defendants\n",
    "recidivism_df = pd.read_csv('compas-scores-two-years.csv') \n",
    "recidivism_df = recidivism_df[(recidivism_df[\"race\"] == \"Caucasian\") | (recidivism_df[\"race\"] == \"African-American\")]\n",
    "\n",
    "# From dataframe, create input and output data for train and test sets.\n",
    "train_df, test_df = train_test_split(recidivism_df, test_size=0.4)\n",
    "X_train = torch.tensor(train_df[['age', 'priors_count']].to_numpy()).type(torch.FloatTensor)\n",
    "y_train = torch.tensor(train_df['two_year_recid'].to_numpy()).type(torch.FloatTensor)\n",
    "X_test = torch.tensor(test_df[['age', 'priors_count']].to_numpy()).type(torch.FloatTensor)\n",
    "y_test = torch.tensor(test_df['two_year_recid'].to_numpy()).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create train_data and test_data.\n",
    "- each is an array of tuples (x, y) \n",
    "- each tuple (x, y) is a sample datapoint where x = input = [age, prior count] and y = output = two_year_recid result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "train_data, test_data = [], []\n",
    "\n",
    "# Create train data\n",
    "for i in range(len(X_train)):\n",
    "    train_data.append([X_train[i], y_train[i]])\n",
    "    \n",
    "# Create test data\n",
    "for i in range(len(X_test)):\n",
    "    test_data.append([X_test[i], y_test[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Sample Datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([28.,  9.]), tensor(1.)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0] # should gives us [tensor([?,?]), tensor(?)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare batch_size = 256 and construct dataloader objects for training and testing. Dataloader objects will allow us to go through dataset in batches during training and testing phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare + Initialize batch size\n",
    "batch_size = 256\n",
    "\n",
    "# Create train and test dataloaders\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare LogisticRegression Model. This consists of a linear function of input features, followed by a sigmoid activation function to produce an output. Sigmoid will return a value between 0 and 1, which is good because we want a measure of probability of recidivism. We interpret 1 as high risk of recidivism and 0 as low risk of recidivism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\" # Too poor for GPU\n",
    "\n",
    "# Logistic Regression Model\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim) # linear layer\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        logits_post_sigmoid = torch.sigmoid(self.linear(inp)) # sigmoid function\n",
    "        return logits_post_sigmoid\n",
    "\n",
    "# Create model\n",
    "model = LogisticRegression(2,1).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 7.983647  [    0/ 3690]\n",
      "loss: 4.998339  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 54.3%, Avg loss: 4.598137 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 52.0%, Avg loss: 4.800653 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 5.078629  [    0/ 3690]\n",
      "loss: 2.583717  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 54.3%, Avg loss: 1.983161 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 52.0%, Avg loss: 2.067289 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.190979  [    0/ 3690]\n",
      "loss: 0.668992  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 47.3%, Avg loss: 0.721968 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 49.7%, Avg loss: 0.705934 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.696549  [    0/ 3690]\n",
      "loss: 0.758593  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.638168 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.631734 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.640350  [    0/ 3690]\n",
      "loss: 0.617655  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.637697 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.641261 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.666013  [    0/ 3690]\n",
      "loss: 0.614671  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.623496 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.619117 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.632452  [    0/ 3690]\n",
      "loss: 0.605753  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.621060 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.621168 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.640319  [    0/ 3690]\n",
      "loss: 0.604636  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.619802 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.618752 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.636381  [    0/ 3690]\n",
      "loss: 0.606262  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.619774 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.619187 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.637370  [    0/ 3690]\n",
      "loss: 0.605101  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.619753 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.619289 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.637612  [    0/ 3690]\n",
      "loss: 0.605826  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.619729 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.619287 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.637627  [    0/ 3690]\n",
      "loss: 0.605599  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.619763 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.619430 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.637898  [    0/ 3690]\n",
      "loss: 0.605829  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.619767 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.619464 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.637963  [    0/ 3690]\n",
      "loss: 0.605847  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.619792 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.619549 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.638118  [    0/ 3690]\n",
      "loss: 0.605957  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.619809 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.619606 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.638221  [    0/ 3690]\n",
      "loss: 0.606023  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.619831 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.619673 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.638342  [    0/ 3690]\n",
      "loss: 0.606108  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.619852 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.619736 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.638454  [    0/ 3690]\n",
      "loss: 0.606184  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.619875 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.619801 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.638569  [    0/ 3690]\n",
      "loss: 0.606263  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.619899 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.619867 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.638685  [    0/ 3690]\n",
      "loss: 0.606340  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.619924 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.619934 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.638802  [    0/ 3690]\n",
      "loss: 0.606418  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.619950 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.620002 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.638920  [    0/ 3690]\n",
      "loss: 0.606495  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.619977 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.620071 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.639039  [    0/ 3690]\n",
      "loss: 0.606572  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620005 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.620142 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.639160  [    0/ 3690]\n",
      "loss: 0.606649  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620034 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.620213 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.639282  [    0/ 3690]\n",
      "loss: 0.606726  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620065 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.620286 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.639406  [    0/ 3690]\n",
      "loss: 0.606802  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620097 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.620361 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.639531  [    0/ 3690]\n",
      "loss: 0.606879  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620130 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.620437 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.639658  [    0/ 3690]\n",
      "loss: 0.606955  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620164 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.620514 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.639787  [    0/ 3690]\n",
      "loss: 0.607030  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620200 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.620592 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.639917  [    0/ 3690]\n",
      "loss: 0.607106  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620237 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.620673 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.640049  [    0/ 3690]\n",
      "loss: 0.607181  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.620275 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.620754 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.640183  [    0/ 3690]\n",
      "loss: 0.607256  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620315 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.620837 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.640319  [    0/ 3690]\n",
      "loss: 0.607331  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620356 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.620922 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.640456  [    0/ 3690]\n",
      "loss: 0.607405  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.620399 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.621009 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.640596  [    0/ 3690]\n",
      "loss: 0.607478  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.620443 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.621097 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.640737  [    0/ 3690]\n",
      "loss: 0.607552  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.620488 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.621186 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.640880  [    0/ 3690]\n",
      "loss: 0.607625  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.620535 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.621277 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.641025  [    0/ 3690]\n",
      "loss: 0.607697  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.620584 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.621370 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.641172  [    0/ 3690]\n",
      "loss: 0.607769  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.620633 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.621465 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.641321  [    0/ 3690]\n",
      "loss: 0.607840  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.620685 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.621561 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.641472  [    0/ 3690]\n",
      "loss: 0.607910  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620738 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.621658 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.641624  [    0/ 3690]\n",
      "loss: 0.607980  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620792 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.621758 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.641779  [    0/ 3690]\n",
      "loss: 0.608049  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620848 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.621859 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.641935  [    0/ 3690]\n",
      "loss: 0.608117  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620905 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.621961 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.642094  [    0/ 3690]\n",
      "loss: 0.608185  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.620964 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.622066 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.642254  [    0/ 3690]\n",
      "loss: 0.608252  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.621024 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.622172 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.642416  [    0/ 3690]\n",
      "loss: 0.608317  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.621086 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.622279 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.642579  [    0/ 3690]\n",
      "loss: 0.608382  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.621150 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.622388 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.642745  [    0/ 3690]\n",
      "loss: 0.608446  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.621215 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.622498 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.642912  [    0/ 3690]\n",
      "loss: 0.608509  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.621281 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.622610 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.643080  [    0/ 3690]\n",
      "loss: 0.608571  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.621349 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.622724 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.643251  [    0/ 3690]\n",
      "loss: 0.608631  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.621418 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.622839 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.643422  [    0/ 3690]\n",
      "loss: 0.608691  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.621488 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.622955 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.643596  [    0/ 3690]\n",
      "loss: 0.608749  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.621560 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.623072 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.643770  [    0/ 3690]\n",
      "loss: 0.608806  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.621633 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.623191 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.643946  [    0/ 3690]\n",
      "loss: 0.608862  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.621708 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.623311 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.644124  [    0/ 3690]\n",
      "loss: 0.608916  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.8%, Avg loss: 0.621784 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.623432 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.644302  [    0/ 3690]\n",
      "loss: 0.608969  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.621860 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.623555 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.644481  [    0/ 3690]\n",
      "loss: 0.609021  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.621939 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.623678 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.644662  [    0/ 3690]\n",
      "loss: 0.609071  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.7%, Avg loss: 0.622018 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.623802 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.644843  [    0/ 3690]\n",
      "loss: 0.609120  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.622098 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.623927 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.645025  [    0/ 3690]\n",
      "loss: 0.609167  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.622180 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.624053 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.645208  [    0/ 3690]\n",
      "loss: 0.609213  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.622262 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.624180 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.645391  [    0/ 3690]\n",
      "loss: 0.609257  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.622345 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.624307 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.645575  [    0/ 3690]\n",
      "loss: 0.609300  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.622429 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.624435 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.645759  [    0/ 3690]\n",
      "loss: 0.609341  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.622514 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.624564 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.645943  [    0/ 3690]\n",
      "loss: 0.609381  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.622599 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.624693 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.646127  [    0/ 3690]\n",
      "loss: 0.609419  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.622686 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.624822 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.646312  [    0/ 3690]\n",
      "loss: 0.609455  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.622772 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.624951 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.646496  [    0/ 3690]\n",
      "loss: 0.609490  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.622859 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.625081 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.646680  [    0/ 3690]\n",
      "loss: 0.609523  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.622947 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.625210 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.646863  [    0/ 3690]\n",
      "loss: 0.609555  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.623035 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.625340 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.647047  [    0/ 3690]\n",
      "loss: 0.609585  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.623123 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.625469 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.647229  [    0/ 3690]\n",
      "loss: 0.609613  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.623212 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.625598 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.647411  [    0/ 3690]\n",
      "loss: 0.609640  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.623300 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.625727 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.647592  [    0/ 3690]\n",
      "loss: 0.609666  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.623389 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.625856 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.647772  [    0/ 3690]\n",
      "loss: 0.609689  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.623478 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.625983 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.647951  [    0/ 3690]\n",
      "loss: 0.609712  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.623566 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.626111 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.648128  [    0/ 3690]\n",
      "loss: 0.609732  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.623655 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.626237 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.648304  [    0/ 3690]\n",
      "loss: 0.609751  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.623743 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.626363 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.648479  [    0/ 3690]\n",
      "loss: 0.609769  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.623830 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.626488 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.648653  [    0/ 3690]\n",
      "loss: 0.609785  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.623918 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.626612 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.648825  [    0/ 3690]\n",
      "loss: 0.609800  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.624005 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.626734 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.648995  [    0/ 3690]\n",
      "loss: 0.609814  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.624091 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.626856 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.649163  [    0/ 3690]\n",
      "loss: 0.609826  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.624177 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.626977 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.649329  [    0/ 3690]\n",
      "loss: 0.609837  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.624262 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.627096 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.649493  [    0/ 3690]\n",
      "loss: 0.609847  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.624346 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.627214 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.649656  [    0/ 3690]\n",
      "loss: 0.609855  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.624430 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.627331 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.649816  [    0/ 3690]\n",
      "loss: 0.609863  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.624513 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.627446 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.649974  [    0/ 3690]\n",
      "loss: 0.609869  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.624595 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.627559 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.650129  [    0/ 3690]\n",
      "loss: 0.609874  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.624676 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.627671 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.650283  [    0/ 3690]\n",
      "loss: 0.609878  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.624756 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.627782 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.650433  [    0/ 3690]\n",
      "loss: 0.609881  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.624835 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.627890 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.650582  [    0/ 3690]\n",
      "loss: 0.609883  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.624913 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.627998 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.650728  [    0/ 3690]\n",
      "loss: 0.609884  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.624989 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.628103 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.650871  [    0/ 3690]\n",
      "loss: 0.609885  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.625065 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.628206 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.651012  [    0/ 3690]\n",
      "loss: 0.609884  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.625140 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.628308 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.651150  [    0/ 3690]\n",
      "loss: 0.609883  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.625213 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.628408 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.651285  [    0/ 3690]\n",
      "loss: 0.609881  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.625285 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.628506 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.651418  [    0/ 3690]\n",
      "loss: 0.609878  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.625356 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.628602 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.651549  [    0/ 3690]\n",
      "loss: 0.609875  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.625425 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.628697 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.651676  [    0/ 3690]\n",
      "loss: 0.609871  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.625494 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.628789 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.651801  [    0/ 3690]\n",
      "loss: 0.609866  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.625561 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.628880 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.651923  [    0/ 3690]\n",
      "loss: 0.609861  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.625627 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.628968 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.652043  [    0/ 3690]\n",
      "loss: 0.609855  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.625691 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.629055 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.652160  [    0/ 3690]\n",
      "loss: 0.609850  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.625754 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.629140 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.652274  [    0/ 3690]\n",
      "loss: 0.609843  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.625816 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.629223 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.652386  [    0/ 3690]\n",
      "loss: 0.609836  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.625876 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.629304 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.652495  [    0/ 3690]\n",
      "loss: 0.609829  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.625935 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.629383 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.652601  [    0/ 3690]\n",
      "loss: 0.609822  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.625993 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.629461 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.652705  [    0/ 3690]\n",
      "loss: 0.609814  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.626050 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.629536 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.652806  [    0/ 3690]\n",
      "loss: 0.609806  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.626105 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.629610 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.652905  [    0/ 3690]\n",
      "loss: 0.609798  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.626159 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.629682 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.653001  [    0/ 3690]\n",
      "loss: 0.609789  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.626211 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.629752 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.653095  [    0/ 3690]\n",
      "loss: 0.609781  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.626263 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.629820 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.653186  [    0/ 3690]\n",
      "loss: 0.609772  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.626313 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.629887 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.653275  [    0/ 3690]\n",
      "loss: 0.609763  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.626362 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.629951 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.653362  [    0/ 3690]\n",
      "loss: 0.609754  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626409 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.630014 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.653446  [    0/ 3690]\n",
      "loss: 0.609745  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626456 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.630076 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.653528  [    0/ 3690]\n",
      "loss: 0.609736  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626501 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.630136 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.653608  [    0/ 3690]\n",
      "loss: 0.609727  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626545 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630194 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.653685  [    0/ 3690]\n",
      "loss: 0.609718  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626588 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630251 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.653761  [    0/ 3690]\n",
      "loss: 0.609709  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626629 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630306 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.653834  [    0/ 3690]\n",
      "loss: 0.609700  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626670 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630359 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.653905  [    0/ 3690]\n",
      "loss: 0.609690  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626709 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630411 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.653975  [    0/ 3690]\n",
      "loss: 0.609681  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626748 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630462 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.654042  [    0/ 3690]\n",
      "loss: 0.609672  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626785 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630511 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.654107  [    0/ 3690]\n",
      "loss: 0.609663  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626822 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630559 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.654171  [    0/ 3690]\n",
      "loss: 0.609654  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626857 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630605 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.654232  [    0/ 3690]\n",
      "loss: 0.609645  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626891 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630650 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.654292  [    0/ 3690]\n",
      "loss: 0.609637  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626924 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630694 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.654350  [    0/ 3690]\n",
      "loss: 0.609628  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626957 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630736 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.654406  [    0/ 3690]\n",
      "loss: 0.609619  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.626988 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630777 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.654461  [    0/ 3690]\n",
      "loss: 0.609611  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.9%, Avg loss: 0.627019 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630817 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.654514  [    0/ 3690]\n",
      "loss: 0.609602  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.627048 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630856 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.654565  [    0/ 3690]\n",
      "loss: 0.609594  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.627077 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630893 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.654615  [    0/ 3690]\n",
      "loss: 0.609586  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.627105 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630930 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.654664  [    0/ 3690]\n",
      "loss: 0.609578  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.627132 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.630965 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.654710  [    0/ 3690]\n",
      "loss: 0.609570  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.627158 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631000 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.654756  [    0/ 3690]\n",
      "loss: 0.609562  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.627184 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631033 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.654800  [    0/ 3690]\n",
      "loss: 0.609555  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.627209 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631065 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.654843  [    0/ 3690]\n",
      "loss: 0.609547  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.627233 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631097 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.654884  [    0/ 3690]\n",
      "loss: 0.609540  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.627256 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631127 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.654924  [    0/ 3690]\n",
      "loss: 0.609532  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.627279 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631156 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.654963  [    0/ 3690]\n",
      "loss: 0.609525  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.627300 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631185 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.655001  [    0/ 3690]\n",
      "loss: 0.609518  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.627322 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631212 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.655037  [    0/ 3690]\n",
      "loss: 0.609511  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.627342 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631239 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.655073  [    0/ 3690]\n",
      "loss: 0.609505  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627362 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631265 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.655107  [    0/ 3690]\n",
      "loss: 0.609498  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627382 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631290 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.655140  [    0/ 3690]\n",
      "loss: 0.609492  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627400 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631315 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.655173  [    0/ 3690]\n",
      "loss: 0.609486  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627419 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631338 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.655204  [    0/ 3690]\n",
      "loss: 0.609479  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627436 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631361 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.655234  [    0/ 3690]\n",
      "loss: 0.609473  [ 2560/ 3690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627453 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631383 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.655263  [    0/ 3690]\n",
      "loss: 0.609467  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627470 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631405 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.655292  [    0/ 3690]\n",
      "loss: 0.609462  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627486 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631426 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.655319  [    0/ 3690]\n",
      "loss: 0.609456  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627502 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631446 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.655346  [    0/ 3690]\n",
      "loss: 0.609451  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627517 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631465 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.655371  [    0/ 3690]\n",
      "loss: 0.609445  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627531 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631484 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.655396  [    0/ 3690]\n",
      "loss: 0.609440  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627545 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631503 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.655421  [    0/ 3690]\n",
      "loss: 0.609435  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627559 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631520 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.655444  [    0/ 3690]\n",
      "loss: 0.609430  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627572 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631537 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.655467  [    0/ 3690]\n",
      "loss: 0.609425  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627585 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631554 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.655489  [    0/ 3690]\n",
      "loss: 0.609420  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627598 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631570 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.655510  [    0/ 3690]\n",
      "loss: 0.609416  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627610 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631586 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.655530  [    0/ 3690]\n",
      "loss: 0.609411  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627622 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631601 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.655550  [    0/ 3690]\n",
      "loss: 0.609407  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627633 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631616 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.655570  [    0/ 3690]\n",
      "loss: 0.609402  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627644 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631630 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.655588  [    0/ 3690]\n",
      "loss: 0.609398  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627655 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631644 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.655606  [    0/ 3690]\n",
      "loss: 0.609394  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627665 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631657 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.655624  [    0/ 3690]\n",
      "loss: 0.609390  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627675 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631670 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.655641  [    0/ 3690]\n",
      "loss: 0.609386  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627685 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631682 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.655657  [    0/ 3690]\n",
      "loss: 0.609382  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627694 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631694 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.655673  [    0/ 3690]\n",
      "loss: 0.609379  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627703 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631706 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.655689  [    0/ 3690]\n",
      "loss: 0.609375  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627712 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631717 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.655704  [    0/ 3690]\n",
      "loss: 0.609372  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627720 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631728 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.655718  [    0/ 3690]\n",
      "loss: 0.609368  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627729 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631739 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.655732  [    0/ 3690]\n",
      "loss: 0.609365  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627737 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631749 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.655746  [    0/ 3690]\n",
      "loss: 0.609362  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627744 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631759 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.655759  [    0/ 3690]\n",
      "loss: 0.609358  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627752 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631769 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.655771  [    0/ 3690]\n",
      "loss: 0.609355  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627759 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631778 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.655784  [    0/ 3690]\n",
      "loss: 0.609352  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627766 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631787 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.655796  [    0/ 3690]\n",
      "loss: 0.609349  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627773 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631796 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.655807  [    0/ 3690]\n",
      "loss: 0.609347  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627780 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631805 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.655818  [    0/ 3690]\n",
      "loss: 0.609344  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627786 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631813 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.655829  [    0/ 3690]\n",
      "loss: 0.609341  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627792 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631821 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.655840  [    0/ 3690]\n",
      "loss: 0.609339  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627798 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631828 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.655850  [    0/ 3690]\n",
      "loss: 0.609336  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627804 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631836 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.655860  [    0/ 3690]\n",
      "loss: 0.609334  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627810 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631843 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.655869  [    0/ 3690]\n",
      "loss: 0.609331  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627815 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631850 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.655878  [    0/ 3690]\n",
      "loss: 0.609329  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627820 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631857 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.655887  [    0/ 3690]\n",
      "loss: 0.609327  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627826 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631864 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.655896  [    0/ 3690]\n",
      "loss: 0.609324  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627831 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631870 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.655904  [    0/ 3690]\n",
      "loss: 0.609322  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627835 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631876 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.655913  [    0/ 3690]\n",
      "loss: 0.609320  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627840 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631882 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.655920  [    0/ 3690]\n",
      "loss: 0.609318  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627845 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631888 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.655928  [    0/ 3690]\n",
      "loss: 0.609316  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627849 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631893 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.655935  [    0/ 3690]\n",
      "loss: 0.609314  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627853 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631899 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.655942  [    0/ 3690]\n",
      "loss: 0.609312  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627857 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631904 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.655949  [    0/ 3690]\n",
      "loss: 0.609311  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627861 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631909 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.655956  [    0/ 3690]\n",
      "loss: 0.609309  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627865 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631914 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.655963  [    0/ 3690]\n",
      "loss: 0.609307  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627869 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631919 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.655969  [    0/ 3690]\n",
      "loss: 0.609305  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627872 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631924 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.655975  [    0/ 3690]\n",
      "loss: 0.609304  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627876 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631928 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.655981  [    0/ 3690]\n",
      "loss: 0.609302  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627879 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631932 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.655986  [    0/ 3690]\n",
      "loss: 0.609301  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627883 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631937 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.655992  [    0/ 3690]\n",
      "loss: 0.609299  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627886 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631941 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.655997  [    0/ 3690]\n",
      "loss: 0.609298  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627889 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631945 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.656003  [    0/ 3690]\n",
      "loss: 0.609296  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627892 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631949 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.656008  [    0/ 3690]\n",
      "loss: 0.609295  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627895 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631952 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.656013  [    0/ 3690]\n",
      "loss: 0.609294  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627898 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631956 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.656017  [    0/ 3690]\n",
      "loss: 0.609292  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627900 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631959 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.656022  [    0/ 3690]\n",
      "loss: 0.609291  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627903 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631963 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.656026  [    0/ 3690]\n",
      "loss: 0.609290  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627906 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631966 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.656031  [    0/ 3690]\n",
      "loss: 0.609289  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627908 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631969 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.656035  [    0/ 3690]\n",
      "loss: 0.609287  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627911 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631972 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.656039  [    0/ 3690]\n",
      "loss: 0.609286  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627913 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631975 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.656043  [    0/ 3690]\n",
      "loss: 0.609285  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627915 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631978 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.656046  [    0/ 3690]\n",
      "loss: 0.609284  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627917 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631981 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.656050  [    0/ 3690]\n",
      "loss: 0.609283  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627919 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631983 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.656054  [    0/ 3690]\n",
      "loss: 0.609282  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627921 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631986 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.656057  [    0/ 3690]\n",
      "loss: 0.609281  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627923 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631989 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.656060  [    0/ 3690]\n",
      "loss: 0.609280  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627925 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631991 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.656064  [    0/ 3690]\n",
      "loss: 0.609279  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627927 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631993 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.656067  [    0/ 3690]\n",
      "loss: 0.609278  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627929 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631996 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.656070  [    0/ 3690]\n",
      "loss: 0.609277  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627931 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.631998 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.656073  [    0/ 3690]\n",
      "loss: 0.609276  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627933 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632000 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.656076  [    0/ 3690]\n",
      "loss: 0.609276  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627934 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632002 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.656078  [    0/ 3690]\n",
      "loss: 0.609275  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627936 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632004 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.656081  [    0/ 3690]\n",
      "loss: 0.609274  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627937 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632006 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.656084  [    0/ 3690]\n",
      "loss: 0.609273  [ 2560/ 3690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627939 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632008 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.656086  [    0/ 3690]\n",
      "loss: 0.609272  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627940 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632010 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.656088  [    0/ 3690]\n",
      "loss: 0.609272  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627942 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632012 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.656091  [    0/ 3690]\n",
      "loss: 0.609271  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627943 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632014 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.656093  [    0/ 3690]\n",
      "loss: 0.609270  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627944 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632015 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.656095  [    0/ 3690]\n",
      "loss: 0.609270  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627946 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632017 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.656098  [    0/ 3690]\n",
      "loss: 0.609269  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627947 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632018 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.656100  [    0/ 3690]\n",
      "loss: 0.609268  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627948 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632020 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.656102  [    0/ 3690]\n",
      "loss: 0.609268  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627949 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632021 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.656104  [    0/ 3690]\n",
      "loss: 0.609267  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627951 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632023 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.656105  [    0/ 3690]\n",
      "loss: 0.609267  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627952 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632024 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.656107  [    0/ 3690]\n",
      "loss: 0.609266  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627953 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632026 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.656109  [    0/ 3690]\n",
      "loss: 0.609265  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627954 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632027 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.656111  [    0/ 3690]\n",
      "loss: 0.609265  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627955 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632028 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.656112  [    0/ 3690]\n",
      "loss: 0.609264  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627956 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632029 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.656114  [    0/ 3690]\n",
      "loss: 0.609264  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627957 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632031 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.656116  [    0/ 3690]\n",
      "loss: 0.609263  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627958 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632032 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.656117  [    0/ 3690]\n",
      "loss: 0.609263  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627958 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632033 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.656119  [    0/ 3690]\n",
      "loss: 0.609262  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627959 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632034 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.656120  [    0/ 3690]\n",
      "loss: 0.609262  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627960 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632035 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.656121  [    0/ 3690]\n",
      "loss: 0.609262  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627961 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632036 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.656123  [    0/ 3690]\n",
      "loss: 0.609261  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627962 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632037 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.656124  [    0/ 3690]\n",
      "loss: 0.609261  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627963 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632038 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.656125  [    0/ 3690]\n",
      "loss: 0.609260  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627963 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632039 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.656127  [    0/ 3690]\n",
      "loss: 0.609260  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627964 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632040 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.656128  [    0/ 3690]\n",
      "loss: 0.609259  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627965 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632041 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.656129  [    0/ 3690]\n",
      "loss: 0.609259  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627965 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632042 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.656130  [    0/ 3690]\n",
      "loss: 0.609259  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627966 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632042 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.656131  [    0/ 3690]\n",
      "loss: 0.609258  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627967 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632043 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.656132  [    0/ 3690]\n",
      "loss: 0.609258  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627967 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632044 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.656133  [    0/ 3690]\n",
      "loss: 0.609258  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627968 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632045 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.656134  [    0/ 3690]\n",
      "loss: 0.609257  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627968 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632045 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.656135  [    0/ 3690]\n",
      "loss: 0.609257  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627969 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632046 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.656136  [    0/ 3690]\n",
      "loss: 0.609257  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627970 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632047 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.656137  [    0/ 3690]\n",
      "loss: 0.609256  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627970 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632047 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.656138  [    0/ 3690]\n",
      "loss: 0.609256  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627971 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632048 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.656139  [    0/ 3690]\n",
      "loss: 0.609256  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627971 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632049 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.656139  [    0/ 3690]\n",
      "loss: 0.609255  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627972 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632049 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.656140  [    0/ 3690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.609255  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627972 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632050 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.656141  [    0/ 3690]\n",
      "loss: 0.609255  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627972 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632051 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.656142  [    0/ 3690]\n",
      "loss: 0.609255  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627973 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632051 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.656142  [    0/ 3690]\n",
      "loss: 0.609254  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627973 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632052 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.656143  [    0/ 3690]\n",
      "loss: 0.609254  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627974 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632052 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.656144  [    0/ 3690]\n",
      "loss: 0.609254  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627974 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632053 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.656144  [    0/ 3690]\n",
      "loss: 0.609254  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627975 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632053 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.656145  [    0/ 3690]\n",
      "loss: 0.609253  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627975 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632054 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.656146  [    0/ 3690]\n",
      "loss: 0.609253  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627975 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632054 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.656146  [    0/ 3690]\n",
      "loss: 0.609253  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627976 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632054 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.656147  [    0/ 3690]\n",
      "loss: 0.609253  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627976 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632055 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.656147  [    0/ 3690]\n",
      "loss: 0.609253  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627976 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632055 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.656148  [    0/ 3690]\n",
      "loss: 0.609252  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627977 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632056 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.656148  [    0/ 3690]\n",
      "loss: 0.609252  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627977 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632056 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.656149  [    0/ 3690]\n",
      "loss: 0.609252  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627977 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632056 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.656149  [    0/ 3690]\n",
      "loss: 0.609252  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627978 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632057 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.656150  [    0/ 3690]\n",
      "loss: 0.609252  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627978 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632057 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.656150  [    0/ 3690]\n",
      "loss: 0.609251  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627978 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632057 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.656151  [    0/ 3690]\n",
      "loss: 0.609251  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627978 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632058 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.656151  [    0/ 3690]\n",
      "loss: 0.609251  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627979 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632058 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.656152  [    0/ 3690]\n",
      "loss: 0.609251  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627979 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632058 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.656152  [    0/ 3690]\n",
      "loss: 0.609251  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627979 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632059 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.656152  [    0/ 3690]\n",
      "loss: 0.609251  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627979 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632059 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.656153  [    0/ 3690]\n",
      "loss: 0.609250  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627980 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632059 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.656153  [    0/ 3690]\n",
      "loss: 0.609250  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627980 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632060 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.656154  [    0/ 3690]\n",
      "loss: 0.609250  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627980 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632060 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.656154  [    0/ 3690]\n",
      "loss: 0.609250  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627980 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632060 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.656154  [    0/ 3690]\n",
      "loss: 0.609250  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627980 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632060 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.656155  [    0/ 3690]\n",
      "loss: 0.609250  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627981 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632061 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.656155  [    0/ 3690]\n",
      "loss: 0.609250  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627981 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632061 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.656155  [    0/ 3690]\n",
      "loss: 0.609249  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627981 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632061 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.656156  [    0/ 3690]\n",
      "loss: 0.609249  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627981 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632061 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.656156  [    0/ 3690]\n",
      "loss: 0.609249  [ 2560/ 3690]\n",
      "Train Dataset Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.627981 \n",
      "\n",
      "Test Dataset Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.632062 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Binary Cross-Entropy loss function and Adam Optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# train function\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    \n",
    "    # loop through batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device) \n",
    "        \n",
    "        # Compute loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(torch.squeeze(pred).type(torch.FloatTensor), y.type(torch.FloatTensor))\n",
    "        \n",
    "        # Compute gradient with respect to model parameters and update parameters through step() function\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Output batch loss every 10 batches\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# Test function\n",
    "def test(dataloader, model, loss_fn, dataset):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    \n",
    "    # We want to return loss on test set, as well as accuracy on test set\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(torch.squeeze(pred).type(torch.FloatTensor), y.type(torch.FloatTensor)).item() # update loss\n",
    "            correct += np.sum(torch.squeeze(pred).round().numpy() == y.numpy()) # Update accuracy\n",
    "    \n",
    "    # Compute average loss and percentage accuracy\n",
    "    test_loss /= num_batches # average loss since loss_fn computes average for each batch, we only need to divide by batch size since it is a constant\n",
    "    correct /= size\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{dataset} Dataset Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return test_loss\n",
    "\n",
    "# Train for 300 epochs, and keep track of train and test losses. No need for validation set.\n",
    "epochs = 300\n",
    "train_losses_per_epoch = []\n",
    "test_losses_per_epoch = []\n",
    "\n",
    "# Execute training\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    train_loss = test(train_dataloader, model, loss_fn, \"Train\")\n",
    "    train_losses_per_epoch.append(train_loss)\n",
    "    test_loss = test(test_dataloader, model, loss_fn, \"Test\")\n",
    "    test_losses_per_epoch.append(test_loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average test loss per batch is around 0.62 for both train and test datasets. There is no sign of overfit, and stagnancy of the model accuracy at 62% suggests that there is no point of training for further epochs. This is likely due to a simple model. Note that there is no validation set here. There does not need to be, because we fix the hyperparameters. Validation set would be used if we were to adjust hyperparameters based on a separate validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot train and test losses against number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Epoch'),\n",
       " Text(0, 0.5, 'Loss'),\n",
       " Text(0.5, 1.0, 'Test Loss per Epoch')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 5)\n",
    "fig.suptitle(\"Train and Test Error per Epoch\")\n",
    "ax1.plot(list(range(1,epochs+1)), train_losses_per_epoch)\n",
    "ax1.set(xlabel='Epoch', ylabel='Loss', title=\"Train Loss per Epoch\")\n",
    "ax2.plot(list(range(1,epochs+1)), test_losses_per_epoch)\n",
    "ax2.set(xlabel='Epoch', ylabel='Loss', title=\"Test Loss per Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show that your model satisfies calibration, but fails to satisfy false-positive parity. Relate this to the difference in base rates in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we will only use the test dataset. The training dataset was used to improve the model, so we do not use it to evaluate the model. Instead, we compute the different fairness criteria by applying our model to the test dataset extracted at the beginning. This is a rule of thumb when assessing generalization of ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration\n",
      "P(recidivate | black, highrisk) = 0.7529644268774703\n",
      "P(recidivate | white, highrisk) = 0.75\n",
      "--------------------\n",
      "P(recidivate | black, lowrisk) = 0.39609053497942387\n",
      "P(recidivate | white, lowrisk) = 0.36634844868735084\n",
      "\n",
      "False Postive Parity\n",
      "P(highrisk | black, did not recidivate) = 0.175561797752809\n",
      "P(highrisk | white, did not recidivate) = 0.06349206349206349\n",
      "\n",
      "Base Rates\n",
      "P(recidivate | black) = 0.5182679296346414\n",
      "P(recidivate | white) = 0.4226069246435845\n"
     ]
    }
   ],
   "source": [
    "# Set threshold to 0.5 for this run. We will adjust it later.\n",
    "threshold = 0.5\n",
    "\n",
    "black_highrisk = 0 # person is black + model predicts high risk of recidivism\n",
    "black_highrisk_recidivate = 0 # person is black + model predicts high risk of recidivism + person actually recidivates\n",
    "\n",
    "white_highrisk = 0 # person is white + model predicts high risk of recidivism\n",
    "white_highrisk_recidivate = 0 # person is white + model predicts high risk of recidivism + person actually recidivates\n",
    "\n",
    "black_lowrisk = 0 # person is black + model predicts low risk of recidivism\n",
    "black_lowrisk_recidivate = 0 # person is black + model predicts low risk of recidivism + person actually recidivates\n",
    "\n",
    "white_lowrisk = 0 # person is white + model predicts low risk of recidivism\n",
    "white_lowrisk_recidivate = 0 # person is white + model predicts low risk of recidivism + person actually recidivates\n",
    "\n",
    "black_notrecidivate = 0 # person is black + does not actually recidivate\n",
    "black_highrisk_notrecidivate = 0 # person is black + model predicts high risk of recidivism + person does not actually recidivate\n",
    "\n",
    "white_notrecidivate = 0 # person is white + person does not actually recidivate\n",
    "white_highrisk_notrecidivate = 0 # person is white + model predicts high risk of recidivism + person does not actually recidivate\n",
    "\n",
    "# Base rates\n",
    "white = 0 # white count\n",
    "white_recidivate = 0 # white + recidivate count\n",
    "black = 0 # black\n",
    "black_recidivate = 0 # black + recidivate count\n",
    "\n",
    "# Loop through test set, increment each variable above when appropriate\n",
    "for _, row in  test_df.iterrows():\n",
    "    x = torch.tensor([row['age'], row['priors_count']]).type(torch.FloatTensor)\n",
    "    \n",
    "    # Extract race + true recidivism \n",
    "    if row['race'] == \"African-American\":\n",
    "        black+=1\n",
    "        if row['two_year_recid'] == 1:\n",
    "            black_recidivate += 1\n",
    "    else:\n",
    "        white+=1\n",
    "        if row['two_year_recid'] == 1:\n",
    "            white_recidivate += 1\n",
    "    \n",
    "    # Use trained model to make prediction\n",
    "    pred = 1 if model(x).item() >= threshold else 0\n",
    "    \n",
    "    # Update variable related to prediction, race and true recidivism \n",
    "    # Black + High Risk\n",
    "    if row['race'] == 'African-American' and pred == 1:\n",
    "        black_highrisk += 1\n",
    "        if row['two_year_recid'] == 1:\n",
    "            black_highrisk_recidivate += 1\n",
    "        else:\n",
    "            black_notrecidivate += 1\n",
    "            black_highrisk_notrecidivate += 1\n",
    "    \n",
    "    # White + High Risk\n",
    "    elif row['race'] == 'Caucasian' and pred == 1:\n",
    "        white_highrisk += 1\n",
    "        if row['two_year_recid'] == 1:\n",
    "            white_highrisk_recidivate += 1\n",
    "        else:\n",
    "            white_notrecidivate += 1\n",
    "            white_highrisk_notrecidivate += 1\n",
    "    \n",
    "    # Black + Low Risk\n",
    "    elif row['race'] == 'African-American' and pred == 0:\n",
    "        black_lowrisk += 1\n",
    "        if row['two_year_recid'] == 1:\n",
    "            black_lowrisk_recidivate += 1\n",
    "        else:\n",
    "            black_notrecidivate += 1\n",
    "    \n",
    "    # White + Low Risk\n",
    "    elif row['race'] == 'Caucasian' and pred == 0:\n",
    "        white_lowrisk += 1\n",
    "        if row['two_year_recid'] == 1:\n",
    "            white_lowrisk_recidivate += 1\n",
    "        else:\n",
    "            white_notrecidivate += 1\n",
    "\n",
    "# Print results. Use Bayes' rule for computations\n",
    "# Calibration metrics\n",
    "print(\"Calibration\")\n",
    "print(f\"P(recidivate | black, highrisk) = {black_highrisk_recidivate/black_highrisk}\")\n",
    "print(f\"P(recidivate | white, highrisk) = {white_highrisk_recidivate/white_highrisk}\")\n",
    "print(\"--------------------\")\n",
    "print(f\"P(recidivate | black, lowrisk) = {black_lowrisk_recidivate/black_lowrisk}\")\n",
    "print(f\"P(recidivate | white, lowrisk) = {white_lowrisk_recidivate/white_lowrisk}\")\n",
    "\n",
    "# False positive parity metric\n",
    "print(\"\\nFalse Postive Parity\")\n",
    "print(f\"P(highrisk | black, did not recidivate) = {black_highrisk_notrecidivate/black_notrecidivate}\")\n",
    "print(f\"P(highrisk | white, did not recidivate) = {white_highrisk_notrecidivate/white_notrecidivate}\")\n",
    "\n",
    "# Difference in base rate = difference between proportion that recidivate in the two groups\n",
    "print(\"\\nBase Rates\")\n",
    "print(f\"P(recidivate | black) = {black_recidivate/black}\")\n",
    "print(f\"P(recidivate | white) = {white_recidivate/white}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the numbers listed above suggest that calibration is satisfied, but false-positive parity is not. As we can see, the probability of recidivism given high risk are similar for both demographics (races); specifically, they are both close to 0.75, with deviation of 0.3%. Liekwise, the probability of recidivism given low risk are also similar for the demographics; both are around 0.38, with a deviation of 3% between the two measures. The reason for a greater deviation than positive parity values (3% vs 0.03%)  could be explained by an imperfect model and unexhaustive dataset. However, the deviation is still small enough to be considered insignificant. Combining these two results, we confirm calibration- our model's prediction can be considered to mean the same level of risk regardless of demographic.\n",
    "\n",
    "On the downside, our model clearly fails to satisfy false positive parity. The probability of being classified as high risk given a person does not end up recidivating after 2 years is significantly higher for black population than white population. According to the paper of Corbett-Davies and Goel, \"various classification parity constraints (specifically, equal positive/negative predictive values, and equal false positive/negative rates) are incompatible if base rates differ across groups. Kleinberg et al. (2017b) prove that except in degenerate cases, no algorithm can simultaneously satisfy calibration and a particular form of classification parity.\" This helps explain the discrepancy above. We have satisfied calibration (to a large extent), so we can only expect to fail with false parity. This is indeed the case, black people have false positive rates approximately 3 times larger than white people. \n",
    "\n",
    "Different base rates can explain the incompatibility of the two measures. Assume for sake of contradiction it is possible to achieve both calibration and false positive parity. If we reach false positive parity, that means the demographic with a higher portion of truly innocent (not recidivate) people has a higher portion who are innocent and predicted to recidivate, as well as innocent and predicted not to recidivate. Now, due to calibration, this demographic must in turn have a higher portion of people who are guilty(recidivate) and predicted to recidivate, as well as guilty and predicted not to recidivate. However, this would mean that the sum of all 4 probabilities for this demographic is > sum of all 4 demographics for other demographic, which is a contradiction since both must sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following the paper of Corbett-Davies and Goel, show that adjusting the thresholds can lead to an algorithm that does not satisfy calibration, but does satisfy false-positive parity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration\n",
      "P(recidivate | black, highrisk) = 0.8188976377952756\n",
      "P(recidivate | white, highrisk) = 0.75\n",
      "--------------------\n",
      "P(recidivate | black, lowrisk) = 0.45588235294117646\n",
      "P(recidivate | white, lowrisk) = 0.36634844868735084\n",
      "\n",
      "False Postive Parity\n",
      "P(highrisk | black, did not recidivate) = 0.06460674157303371\n",
      "P(highrisk | white, did not recidivate) = 0.06349206349206349\n",
      "\n",
      "Base Rates\n",
      "P(recidivate | black) = 0.5182679296346414\n",
      "P(recidivate | white) = 0.4226069246435845\n"
     ]
    }
   ],
   "source": [
    "# It is likely that many other tuples work, but we chose tuple that gave closest fpr for the two demographics after some time spent playing around randomly\n",
    "# Note: we want black threshold to be high, to make prediction = 1 more stricter so we can obtain less false recidivism predictions\n",
    "white_threshold = 0.5\n",
    "black_threshold = 0.62\n",
    "\n",
    "'''\n",
    "This is all the same as before\n",
    "'''\n",
    "black_highrisk = 0\n",
    "black_highrisk_recidivate = 0\n",
    "\n",
    "white_highrisk = 0\n",
    "white_highrisk_recidivate = 0\n",
    "\n",
    "black_lowrisk = 0\n",
    "black_lowrisk_recidivate = 0\n",
    "\n",
    "white_lowrisk = 0\n",
    "white_lowrisk_recidivate = 0\n",
    "\n",
    "black_notrecidivate = 0\n",
    "black_highrisk_notrecidivate = 0\n",
    "\n",
    "white_notrecidivate = 0\n",
    "white_highrisk_notrecidivate = 0\n",
    "\n",
    "white = 0\n",
    "white_recidivate = 0\n",
    "black = 0\n",
    "black_recidivate = 0\n",
    "\n",
    "for _, row in  test_df.iterrows():\n",
    "    x = torch.tensor([row['age'], row['priors_count']]).type(torch.FloatTensor)\n",
    "    \n",
    "    if row['race'] == \"African-American\":\n",
    "        black+=1\n",
    "        if row['two_year_recid'] == 1:\n",
    "            black_recidivate += 1\n",
    "    else:\n",
    "        white+=1\n",
    "        if row['two_year_recid'] == 1:\n",
    "            white_recidivate += 1\n",
    "            \n",
    "    # Only difference- prediction is race-dependent, threshold is a function of race\n",
    "    # Of course, we should expect calibration to break then\n",
    "    if row[\"race\"] == \"African-American\":\n",
    "        pred = 1 if model(x).item() >= black_threshold else 0\n",
    "    else:\n",
    "        pred = 1 if model(x).item() >= white_threshold else 0      \n",
    "        \n",
    "    if row['race'] == 'African-American' and pred == 1:\n",
    "        black_highrisk += 1\n",
    "        if row['two_year_recid'] == 1:\n",
    "            black_highrisk_recidivate += 1\n",
    "        else:\n",
    "            black_notrecidivate += 1\n",
    "            black_highrisk_notrecidivate += 1\n",
    "    elif row['race'] == 'Caucasian' and pred == 1:\n",
    "        white_highrisk += 1\n",
    "        if row['two_year_recid'] == 1:\n",
    "            white_highrisk_recidivate += 1\n",
    "        else:\n",
    "            white_notrecidivate += 1\n",
    "            white_highrisk_notrecidivate += 1\n",
    "    elif row['race'] == 'African-American' and pred == 0:\n",
    "        black_lowrisk += 1\n",
    "        if row['two_year_recid'] == 1:\n",
    "            black_lowrisk_recidivate += 1\n",
    "        else:\n",
    "            black_notrecidivate += 1\n",
    "    else:\n",
    "        white_lowrisk += 1\n",
    "        if row['two_year_recid'] == 1:\n",
    "            white_lowrisk_recidivate += 1\n",
    "        else:\n",
    "            white_notrecidivate += 1\n",
    "            \n",
    "print(\"Calibration\")\n",
    "print(f\"P(recidivate | black, highrisk) = {black_highrisk_recidivate/black_highrisk}\")\n",
    "print(f\"P(recidivate | white, highrisk) = {white_highrisk_recidivate/white_highrisk}\")\n",
    "print(\"--------------------\")\n",
    "print(f\"P(recidivate | black, lowrisk) = {black_lowrisk_recidivate/black_lowrisk}\")\n",
    "print(f\"P(recidivate | white, lowrisk) = {white_lowrisk_recidivate/white_lowrisk}\")\n",
    "\n",
    "print(\"\\nFalse Postive Parity\")\n",
    "print(f\"P(highrisk | black, did not recidivate) = {black_highrisk_notrecidivate/black_notrecidivate}\")\n",
    "print(f\"P(highrisk | white, did not recidivate) = {white_highrisk_notrecidivate/white_notrecidivate}\")\n",
    "\n",
    "# Difference in base rate = difference between proportion that recidivate in the two groups\n",
    "print(\"\\nBase Rates\")\n",
    "print(f\"P(recidivate | black) = {black_recidivate/black}\")\n",
    "print(f\"P(recidivate | white) = {white_recidivate/white}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous analysis, we had satisfied calibration, but false positive rates for blacks were much higher than corresponding rate for white people. In this analysis, we play with the threshold for black population, until we observe false positive parity. One set of thresholds where this happens is if we keep white population's threshold at 0.5 and increase black population's threshold to 0.62. It makes sense that we would have to set threshold for black demographic higher because they are facing more false positive, so we would like to reduce this number by imposing more stricter conditions on when a black person is predicted to recidivate.\n",
    "\n",
    "Adjusting the threshold has led to an algorithm that gives a false positive rate of about 6.4% for both populations. However, calibration no longer holds. Our model shows that given a person who is high risk, the probability of actually recidivating is higher for black people than white people. Similarly, given a person who is classified as low risk, the probability of actually recidivating is higher for black people again. So, the same score does not mean the same thing for different populations anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wadsworth et al. claim that it is possible to produce a more accurate classifier that satisfies properties such as false-positive parity, by using more features, and by using an adversarial learning procedure.\n",
    "\n",
    "Write PyTorch code to implement the adversarial learning procedure. Note that the adversarial learning procedure would need to be iterative: the adversary is optimized, and then the whole network N is optimized, in a loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt 1 code is not commented because it is very similar to attempt 2, which is heavily commented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "recidivism_df = pd.read_csv('compas-scores-two-years.csv')\n",
    "recidivism_df = recidivism_df[(recidivism_df[\"race\"] == \"Caucasian\") | (recidivism_df[\"race\"] == \"African-American\")]\n",
    "\n",
    "# want: sex, age, juv_fel_count, juv_misd_count, juv_other_count, priors count, c_charge_degree, c_charge_desc\n",
    "recidivism_df = recidivism_df[['sex', 'age', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count', 'c_charge_degree', 'c_charge_desc', 'race', 'two_year_recid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "recidivism_df['sex'].replace(['Female','Male'], [0,1], inplace=True)\n",
    "recidivism_df['c_charge_degree'].replace(['F','M'], [0,1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>c_charge_desc</th>\n",
       "      <th>race</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Felony Battery w/Prior Convict</td>\n",
       "      <td>African-American</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Possession of Cocaine</td>\n",
       "      <td>African-American</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Possession of Cannabis</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Possession Burglary Tools</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Battery</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  age  juv_fel_count  juv_misd_count  juv_other_count  priors_count  \\\n",
       "1    1   34              0               0                0             0   \n",
       "2    1   24              0               0                1             4   \n",
       "3    1   23              0               1                0             1   \n",
       "6    1   41              0               0                0            14   \n",
       "8    0   39              0               0                0             0   \n",
       "\n",
       "   c_charge_degree                   c_charge_desc              race  \\\n",
       "1                0  Felony Battery w/Prior Convict  African-American   \n",
       "2                0           Possession of Cocaine  African-American   \n",
       "3                0          Possession of Cannabis  African-American   \n",
       "6                0       Possession Burglary Tools         Caucasian   \n",
       "8                1                         Battery         Caucasian   \n",
       "\n",
       "   two_year_recid  \n",
       "1               1  \n",
       "2               1  \n",
       "3               0  \n",
       "6               1  \n",
       "8               0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recidivism_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(recidivism_df['c_charge_desc'])\n",
    "recidivism_df = recidivism_df.drop('c_charge_desc',axis = 1)\n",
    "recidivism_df = recidivism_df.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_val_df = train_test_split(recidivism_df, test_size=0.5)\n",
    "val_df, test_df = train_test_split(test_val_df, test_size=0.5)\n",
    "\n",
    "features = list(recidivism_df.columns)\n",
    "features.remove('race')\n",
    "features.remove('two_year_recid')\n",
    "\n",
    "num_features = len(features)\n",
    "\n",
    "predictor_X_train = torch.tensor(train_df[features].to_numpy()).type(torch.FloatTensor)\n",
    "predictor_y_train = torch.tensor(train_df['two_year_recid'].to_numpy()).type(torch.FloatTensor)\n",
    "\n",
    "predictor_X_val = torch.tensor(val_df[features].to_numpy()).type(torch.FloatTensor)\n",
    "predictor_y_val = torch.tensor(val_df['two_year_recid'].to_numpy()).type(torch.FloatTensor)\n",
    "\n",
    "predictor_X_test = torch.tensor(test_df[features].to_numpy()).type(torch.FloatTensor)\n",
    "predictor_y_test = torch.tensor(test_df['two_year_recid'].to_numpy()).type(torch.FloatTensor)\n",
    "\n",
    "adversary_y_train = torch.tensor([1 if race == 'African-American' else 0 for race in list(train_df['race'])]).type(torch.FloatTensor)\n",
    "adversary_y_val = torch.tensor([1 if race == 'African-American' else 0 for race in list(val_df['race'])]).type(torch.FloatTensor)\n",
    "adversary_y_test = torch.tensor([1 if race == 'African-American' else 0 for race in list(test_df['race'])]).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_train_data, predictor_val_data, predictor_test_data = [], [], []\n",
    "for i in range(len(predictor_X_train)):\n",
    "    predictor_train_data.append([predictor_X_train[i], predictor_y_train[i], adversary_y_train[i]])\n",
    "for i in range(len(predictor_X_val)):\n",
    "    predictor_val_data.append([predictor_X_val[i], predictor_y_val[i], adversary_y_val[i]])\n",
    "for i in range(len(predictor_X_test)):\n",
    "    predictor_test_data.append([predictor_X_test[i], predictor_y_test[i], adversary_y_test[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(predictor_train_data, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(predictor_val_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(predictor_test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "class RecidivismPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RecidivismPredictor, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        logits_post_sigmoid = torch.sigmoid(logits)\n",
    "        return logits_post_sigmoid\n",
    "\n",
    "class Adversary(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Adversary, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, output_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        logits_post_sigmoid = torch.sigmoid(logits)\n",
    "        return logits_post_sigmoid\n",
    "\n",
    "predictorf = RecidivismPredictor(num_features,1).to(device)\n",
    "adversaryf = Adversary(1,1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, predictor, adversary, loss_fn, predictor_optimizer, adversary_optimizer, alpha):\n",
    "    size = len(dataloader.dataset)\n",
    "    predictor.train()\n",
    "    adversary.train()\n",
    "    \n",
    "    # Training Set\n",
    "    print(\"IMPROVING ADVERSARY\")\n",
    "    for batch, (X, y, race) in enumerate(dataloader): # loop through batches\n",
    "        X, y, race = X.to(device), y.to(device), race.to(device)\n",
    "        # Part 1: backprop through adversarial net\n",
    "        pred = predictor.linear_relu_stack(X) # obtain logit \n",
    "        race_pred = adversary(pred) # from logit get race prediction through adversary\n",
    "        adversary_loss = loss_fn(torch.squeeze(race_pred).type(torch.FloatTensor), torch.squeeze(race).type(torch.FloatTensor))\n",
    "        adversary_optimizer.zero_grad()\n",
    "        adversary_loss.backward()\n",
    "        adversary_optimizer.step()        \n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            loss, current = adversary_loss.item(), batch * 64\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    test_adversary(val_dataloader, predictor, adversary, loss_fn) # should get better\n",
    "    \n",
    "    print(\"IMPROVING NETWORK\")\n",
    "    for batch, (X, y, race) in enumerate(dataloader):\n",
    "        X, y, race = X.to(device), y.to(device), race.to(device)\n",
    "        logit = predictor.linear_relu_stack(X)\n",
    "        pred = predictor(X)\n",
    "        race_pred = adversary(logit)\n",
    "        adversary_loss = loss_fn(torch.squeeze(race_pred).type(torch.FloatTensor), torch.squeeze(race).type(torch.FloatTensor))        \n",
    "        predictor_loss = loss_fn(torch.squeeze(pred).type(torch.FloatTensor), torch.squeeze(y).type(torch.FloatTensor))\n",
    "        network_loss = predictor_loss - alpha*adversary_loss\n",
    "        predictor_optimizer.zero_grad()\n",
    "        network_loss.backward()\n",
    "        predictor_optimizer.step()\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            loss, current = predictor_loss.item(), batch * 64\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    test_adversary(val_dataloader, predictor, adversary, loss_fn) # should have gotten worse\n",
    "    \n",
    "    # Validation Set\n",
    "    c, tl, n = inference(val_dataloader, predictor, loss_fn)\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100*c):>0.1f}%, Avg loss: {tl:>8f} \\n Num Batches: {n}\")\n",
    "\n",
    "def inference(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y, race in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(torch.squeeze(pred).type(torch.FloatTensor), torch.squeeze(y).type(torch.FloatTensor)).item()\n",
    "            correct += np.sum(torch.squeeze(pred).round().numpy() == torch.squeeze(y).numpy())    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    return correct, test_loss, num_batches\n",
    "\n",
    "def test_adversary(dataloader, predictor, adversary, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    predictor.eval()\n",
    "    adversary.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y, race in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logit = predictor.linear_relu_stack(X)\n",
    "            race_pred = adversary(logit)\n",
    "            test_loss += loss_fn(torch.squeeze(race_pred).type(torch.FloatTensor), torch.squeeze(race.type(torch.FloatTensor)))\n",
    "            correct += np.sum(torch.squeeze(race_pred).round().numpy() == torch.squeeze(race).numpy())    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Adversary Test Error: \\n Accuracy: {(100*correct):>0.5f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.678734  [    0/ 3075]\n",
      "loss: 0.688084  [  640/ 3075]\n",
      "loss: 0.684214  [ 1280/ 3075]\n",
      "loss: 0.680609  [ 1920/ 3075]\n",
      "loss: 0.670962  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.87378%, Avg loss: 0.682987 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.687569  [    0/ 3075]\n",
      "loss: 0.663443  [  640/ 3075]\n",
      "loss: 0.692867  [ 1280/ 3075]\n",
      "loss: 0.695962  [ 1920/ 3075]\n",
      "loss: 0.685933  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.52440%, Avg loss: 0.683347 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 53.8%, Avg loss: 0.688128 \n",
      " Num Batches: 25\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.676315  [    0/ 3075]\n",
      "loss: 0.685361  [  640/ 3075]\n",
      "loss: 0.683148  [ 1280/ 3075]\n",
      "loss: 0.681180  [ 1920/ 3075]\n",
      "loss: 0.671678  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.00390%, Avg loss: 0.682468 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.681142  [    0/ 3075]\n",
      "loss: 0.647353  [  640/ 3075]\n",
      "loss: 0.692477  [ 1280/ 3075]\n",
      "loss: 0.695742  [ 1920/ 3075]\n",
      "loss: 0.682222  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.89070%, Avg loss: 0.682518 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 53.8%, Avg loss: 0.685225 \n",
      " Num Batches: 25\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.674332  [    0/ 3075]\n",
      "loss: 0.683658  [  640/ 3075]\n",
      "loss: 0.681504  [ 1280/ 3075]\n",
      "loss: 0.680737  [ 1920/ 3075]\n",
      "loss: 0.671640  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.04489%, Avg loss: 0.681610 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.676926  [    0/ 3075]\n",
      "loss: 0.637455  [  640/ 3075]\n",
      "loss: 0.692165  [ 1280/ 3075]\n",
      "loss: 0.694919  [ 1920/ 3075]\n",
      "loss: 0.678832  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.15094%, Avg loss: 0.681433 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 53.8%, Avg loss: 0.682241 \n",
      " Num Batches: 25\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.672225  [    0/ 3075]\n",
      "loss: 0.682186  [  640/ 3075]\n",
      "loss: 0.679512  [ 1280/ 3075]\n",
      "loss: 0.679527  [ 1920/ 3075]\n",
      "loss: 0.670903  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.02082%, Avg loss: 0.680544 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.673247  [    0/ 3075]\n",
      "loss: 0.630675  [  640/ 3075]\n",
      "loss: 0.691544  [ 1280/ 3075]\n",
      "loss: 0.693345  [ 1920/ 3075]\n",
      "loss: 0.675753  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.41119%, Avg loss: 0.680164 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 53.8%, Avg loss: 0.679257 \n",
      " Num Batches: 25\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.670190  [    0/ 3075]\n",
      "loss: 0.680830  [  640/ 3075]\n",
      "loss: 0.677487  [ 1280/ 3075]\n",
      "loss: 0.678008  [ 1920/ 3075]\n",
      "loss: 0.669593  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.21601%, Avg loss: 0.679319 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.669998  [    0/ 3075]\n",
      "loss: 0.626094  [  640/ 3075]\n",
      "loss: 0.690728  [ 1280/ 3075]\n",
      "loss: 0.691366  [ 1920/ 3075]\n",
      "loss: 0.672744  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.28107%, Avg loss: 0.678832 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 54.1%, Avg loss: 0.676210 \n",
      " Num Batches: 25\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.668105  [    0/ 3075]\n",
      "loss: 0.679576  [  640/ 3075]\n",
      "loss: 0.675324  [ 1280/ 3075]\n",
      "loss: 0.676184  [ 1920/ 3075]\n",
      "loss: 0.667966  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.28107%, Avg loss: 0.678042 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.667136  [    0/ 3075]\n",
      "loss: 0.622720  [  640/ 3075]\n",
      "loss: 0.689668  [ 1280/ 3075]\n",
      "loss: 0.689223  [ 1920/ 3075]\n",
      "loss: 0.670016  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.15094%, Avg loss: 0.677528 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.673446 \n",
      " Num Batches: 25\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.666190  [    0/ 3075]\n",
      "loss: 0.678410  [  640/ 3075]\n",
      "loss: 0.673191  [ 1280/ 3075]\n",
      "loss: 0.674352  [ 1920/ 3075]\n",
      "loss: 0.666347  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.15094%, Avg loss: 0.676790 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.664425  [    0/ 3075]\n",
      "loss: 0.619979  [  640/ 3075]\n",
      "loss: 0.688653  [ 1280/ 3075]\n",
      "loss: 0.686881  [ 1920/ 3075]\n",
      "loss: 0.667145  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.41119%, Avg loss: 0.676263 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 56.3%, Avg loss: 0.670540 \n",
      " Num Batches: 25\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.664201  [    0/ 3075]\n",
      "loss: 0.677210  [  640/ 3075]\n",
      "loss: 0.671049  [ 1280/ 3075]\n",
      "loss: 0.672491  [ 1920/ 3075]\n",
      "loss: 0.664651  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.69551%, Avg loss: 0.675573 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.661699  [    0/ 3075]\n",
      "loss: 0.617451  [  640/ 3075]\n",
      "loss: 0.687687  [ 1280/ 3075]\n",
      "loss: 0.684759  [ 1920/ 3075]\n",
      "loss: 0.664438  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.15094%, Avg loss: 0.675082 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 57.0%, Avg loss: 0.667719 \n",
      " Num Batches: 25\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.662260  [    0/ 3075]\n",
      "loss: 0.676044  [  640/ 3075]\n",
      "loss: 0.669108  [ 1280/ 3075]\n",
      "loss: 0.670767  [ 1920/ 3075]\n",
      "loss: 0.663108  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.43526%, Avg loss: 0.674433 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.659095  [    0/ 3075]\n",
      "loss: 0.614827  [  640/ 3075]\n",
      "loss: 0.686759  [ 1280/ 3075]\n",
      "loss: 0.682485  [ 1920/ 3075]\n",
      "loss: 0.661706  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.82563%, Avg loss: 0.673974 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 57.5%, Avg loss: 0.665027 \n",
      " Num Batches: 25\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.660347  [    0/ 3075]\n",
      "loss: 0.674951  [  640/ 3075]\n",
      "loss: 0.667305  [ 1280/ 3075]\n",
      "loss: 0.669116  [ 1920/ 3075]\n",
      "loss: 0.661586  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.84971%, Avg loss: 0.673363 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.656661  [    0/ 3075]\n",
      "loss: 0.612397  [  640/ 3075]\n",
      "loss: 0.686007  [ 1280/ 3075]\n",
      "loss: 0.680379  [ 1920/ 3075]\n",
      "loss: 0.659104  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.50033%, Avg loss: 0.672944 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 58.4%, Avg loss: 0.662429 \n",
      " Num Batches: 25\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.658536  [    0/ 3075]\n",
      "loss: 0.673885  [  640/ 3075]\n",
      "loss: 0.665581  [ 1280/ 3075]\n",
      "loss: 0.667546  [ 1920/ 3075]\n",
      "loss: 0.660154  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.84971%, Avg loss: 0.672365 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.654369  [    0/ 3075]\n",
      "loss: 0.610027  [  640/ 3075]\n",
      "loss: 0.685292  [ 1280/ 3075]\n",
      "loss: 0.678151  [ 1920/ 3075]\n",
      "loss: 0.656464  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.17502%, Avg loss: 0.671923 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 59.3%, Avg loss: 0.659826 \n",
      " Num Batches: 25\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.656858  [    0/ 3075]\n",
      "loss: 0.672840  [  640/ 3075]\n",
      "loss: 0.663892  [ 1280/ 3075]\n",
      "loss: 0.665936  [ 1920/ 3075]\n",
      "loss: 0.658816  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.91477%, Avg loss: 0.671375 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.652053  [    0/ 3075]\n",
      "loss: 0.607492  [  640/ 3075]\n",
      "loss: 0.684697  [ 1280/ 3075]\n",
      "loss: 0.676131  [ 1920/ 3075]\n",
      "loss: 0.653858  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.71958%, Avg loss: 0.670946 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 60.0%, Avg loss: 0.657257 \n",
      " Num Batches: 25\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.655164  [    0/ 3075]\n",
      "loss: 0.671725  [  640/ 3075]\n",
      "loss: 0.662232  [ 1280/ 3075]\n",
      "loss: 0.664378  [ 1920/ 3075]\n",
      "loss: 0.657624  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.84971%, Avg loss: 0.670424 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.649716  [    0/ 3075]\n",
      "loss: 0.604902  [  640/ 3075]\n",
      "loss: 0.684113  [ 1280/ 3075]\n",
      "loss: 0.674042  [ 1920/ 3075]\n",
      "loss: 0.651357  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.84971%, Avg loss: 0.670039 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.654895 \n",
      " Num Batches: 25\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.653554  [    0/ 3075]\n",
      "loss: 0.670674  [  640/ 3075]\n",
      "loss: 0.660641  [ 1280/ 3075]\n",
      "loss: 0.662866  [ 1920/ 3075]\n",
      "loss: 0.656459  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.84971%, Avg loss: 0.669542 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.647555  [    0/ 3075]\n",
      "loss: 0.602559  [  640/ 3075]\n",
      "loss: 0.683599  [ 1280/ 3075]\n",
      "loss: 0.671992  [ 1920/ 3075]\n",
      "loss: 0.648889  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.78465%, Avg loss: 0.669161 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 61.4%, Avg loss: 0.652541 \n",
      " Num Batches: 25\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.651957  [    0/ 3075]\n",
      "loss: 0.669639  [  640/ 3075]\n",
      "loss: 0.659017  [ 1280/ 3075]\n",
      "loss: 0.661319  [ 1920/ 3075]\n",
      "loss: 0.655336  [ 2560/ 3075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversary Test Error: \n",
      " Accuracy: 62.91477%, Avg loss: 0.668687 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.645528  [    0/ 3075]\n",
      "loss: 0.600225  [  640/ 3075]\n",
      "loss: 0.683300  [ 1280/ 3075]\n",
      "loss: 0.670185  [ 1920/ 3075]\n",
      "loss: 0.646558  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.84971%, Avg loss: 0.668333 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 61.7%, Avg loss: 0.650350 \n",
      " Num Batches: 25\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.650402  [    0/ 3075]\n",
      "loss: 0.668591  [  640/ 3075]\n",
      "loss: 0.657449  [ 1280/ 3075]\n",
      "loss: 0.659869  [ 1920/ 3075]\n",
      "loss: 0.654343  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.84971%, Avg loss: 0.667880 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.643549  [    0/ 3075]\n",
      "loss: 0.597875  [  640/ 3075]\n",
      "loss: 0.683071  [ 1280/ 3075]\n",
      "loss: 0.668366  [ 1920/ 3075]\n",
      "loss: 0.644275  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.78465%, Avg loss: 0.667543 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 62.1%, Avg loss: 0.648240 \n",
      " Num Batches: 25\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.648927  [    0/ 3075]\n",
      "loss: 0.667534  [  640/ 3075]\n",
      "loss: 0.655941  [ 1280/ 3075]\n",
      "loss: 0.658489  [ 1920/ 3075]\n",
      "loss: 0.653415  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.78465%, Avg loss: 0.667110 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.641668  [    0/ 3075]\n",
      "loss: 0.595492  [  640/ 3075]\n",
      "loss: 0.682948  [ 1280/ 3075]\n",
      "loss: 0.666655  [ 1920/ 3075]\n",
      "loss: 0.642051  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.97983%, Avg loss: 0.666792 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 62.7%, Avg loss: 0.646206 \n",
      " Num Batches: 25\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.647410  [    0/ 3075]\n",
      "loss: 0.666468  [  640/ 3075]\n",
      "loss: 0.654456  [ 1280/ 3075]\n",
      "loss: 0.657152  [ 1920/ 3075]\n",
      "loss: 0.652534  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.84971%, Avg loss: 0.666376 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.639869  [    0/ 3075]\n",
      "loss: 0.593344  [  640/ 3075]\n",
      "loss: 0.682821  [ 1280/ 3075]\n",
      "loss: 0.664852  [ 1920/ 3075]\n",
      "loss: 0.639836  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.97983%, Avg loss: 0.666066 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.644196 \n",
      " Num Batches: 25\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.645833  [    0/ 3075]\n",
      "loss: 0.665410  [  640/ 3075]\n",
      "loss: 0.652961  [ 1280/ 3075]\n",
      "loss: 0.655791  [ 1920/ 3075]\n",
      "loss: 0.651670  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.78465%, Avg loss: 0.665668 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.638181  [    0/ 3075]\n",
      "loss: 0.591235  [  640/ 3075]\n",
      "loss: 0.682787  [ 1280/ 3075]\n",
      "loss: 0.663188  [ 1920/ 3075]\n",
      "loss: 0.637700  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.17502%, Avg loss: 0.665374 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.642296 \n",
      " Num Batches: 25\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.644291  [    0/ 3075]\n",
      "loss: 0.664340  [  640/ 3075]\n",
      "loss: 0.651477  [ 1280/ 3075]\n",
      "loss: 0.654502  [ 1920/ 3075]\n",
      "loss: 0.650834  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.04489%, Avg loss: 0.664994 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.636563  [    0/ 3075]\n",
      "loss: 0.589126  [  640/ 3075]\n",
      "loss: 0.682833  [ 1280/ 3075]\n",
      "loss: 0.661582  [ 1920/ 3075]\n",
      "loss: 0.635620  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.24008%, Avg loss: 0.664719 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 63.4%, Avg loss: 0.640476 \n",
      " Num Batches: 25\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.642762  [    0/ 3075]\n",
      "loss: 0.663283  [  640/ 3075]\n",
      "loss: 0.650047  [ 1280/ 3075]\n",
      "loss: 0.653247  [ 1920/ 3075]\n",
      "loss: 0.650039  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.17502%, Avg loss: 0.664356 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.635025  [    0/ 3075]\n",
      "loss: 0.587038  [  640/ 3075]\n",
      "loss: 0.682960  [ 1280/ 3075]\n",
      "loss: 0.660013  [ 1920/ 3075]\n",
      "loss: 0.633603  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.37020%, Avg loss: 0.664096 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 63.5%, Avg loss: 0.638725 \n",
      " Num Batches: 25\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.641202  [    0/ 3075]\n",
      "loss: 0.662226  [  640/ 3075]\n",
      "loss: 0.648677  [ 1280/ 3075]\n",
      "loss: 0.652026  [ 1920/ 3075]\n",
      "loss: 0.649289  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.24008%, Avg loss: 0.663747 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.633570  [    0/ 3075]\n",
      "loss: 0.584917  [  640/ 3075]\n",
      "loss: 0.683153  [ 1280/ 3075]\n",
      "loss: 0.658545  [ 1920/ 3075]\n",
      "loss: 0.631644  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.63045%, Avg loss: 0.663519 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.637018 \n",
      " Num Batches: 25\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.639666  [    0/ 3075]\n",
      "loss: 0.661189  [  640/ 3075]\n",
      "loss: 0.647367  [ 1280/ 3075]\n",
      "loss: 0.650814  [ 1920/ 3075]\n",
      "loss: 0.648603  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.30514%, Avg loss: 0.663186 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.632209  [    0/ 3075]\n",
      "loss: 0.582977  [  640/ 3075]\n",
      "loss: 0.683351  [ 1280/ 3075]\n",
      "loss: 0.656983  [ 1920/ 3075]\n",
      "loss: 0.629680  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.82563%, Avg loss: 0.662989 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.635366 \n",
      " Num Batches: 25\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.638207  [    0/ 3075]\n",
      "loss: 0.660153  [  640/ 3075]\n",
      "loss: 0.646090  [ 1280/ 3075]\n",
      "loss: 0.649626  [ 1920/ 3075]\n",
      "loss: 0.647966  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.76057%, Avg loss: 0.662671 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.630925  [    0/ 3075]\n",
      "loss: 0.581052  [  640/ 3075]\n",
      "loss: 0.683631  [ 1280/ 3075]\n",
      "loss: 0.655589  [ 1920/ 3075]\n",
      "loss: 0.627815  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.95576%, Avg loss: 0.662508 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.633777 \n",
      " Num Batches: 25\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.636846  [    0/ 3075]\n",
      "loss: 0.659137  [  640/ 3075]\n",
      "loss: 0.644862  [ 1280/ 3075]\n",
      "loss: 0.648482  [ 1920/ 3075]\n",
      "loss: 0.647382  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.82563%, Avg loss: 0.662205 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.629735  [    0/ 3075]\n",
      "loss: 0.579333  [  640/ 3075]\n",
      "loss: 0.683895  [ 1280/ 3075]\n",
      "loss: 0.654015  [ 1920/ 3075]\n",
      "loss: 0.625910  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.02082%, Avg loss: 0.662051 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.632204 \n",
      " Num Batches: 25\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.635542  [    0/ 3075]\n",
      "loss: 0.658162  [  640/ 3075]\n",
      "loss: 0.643730  [ 1280/ 3075]\n",
      "loss: 0.647342  [ 1920/ 3075]\n",
      "loss: 0.646786  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.89070%, Avg loss: 0.661765 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.628619  [    0/ 3075]\n",
      "loss: 0.577622  [  640/ 3075]\n",
      "loss: 0.684198  [ 1280/ 3075]\n",
      "loss: 0.652600  [ 1920/ 3075]\n",
      "loss: 0.624115  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.21601%, Avg loss: 0.661608 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.630750 \n",
      " Num Batches: 25\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.634282  [    0/ 3075]\n",
      "loss: 0.657194  [  640/ 3075]\n",
      "loss: 0.642673  [ 1280/ 3075]\n",
      "loss: 0.646324  [ 1920/ 3075]\n",
      "loss: 0.646302  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.08588%, Avg loss: 0.661336 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.627551  [    0/ 3075]\n",
      "loss: 0.575831  [  640/ 3075]\n",
      "loss: 0.684559  [ 1280/ 3075]\n",
      "loss: 0.651394  [ 1920/ 3075]\n",
      "loss: 0.622441  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.15094%, Avg loss: 0.661189 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.629389 \n",
      " Num Batches: 25\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.633080  [    0/ 3075]\n",
      "loss: 0.656254  [  640/ 3075]\n",
      "loss: 0.641631  [ 1280/ 3075]\n",
      "loss: 0.645358  [ 1920/ 3075]\n",
      "loss: 0.645876  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.21601%, Avg loss: 0.660930 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.626539  [    0/ 3075]\n",
      "loss: 0.573988  [  640/ 3075]\n",
      "loss: 0.684965  [ 1280/ 3075]\n",
      "loss: 0.650267  [ 1920/ 3075]\n",
      "loss: 0.620842  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.34613%, Avg loss: 0.660790 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.628104 \n",
      " Num Batches: 25\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.631932  [    0/ 3075]\n",
      "loss: 0.655318  [  640/ 3075]\n",
      "loss: 0.640572  [ 1280/ 3075]\n",
      "loss: 0.644432  [ 1920/ 3075]\n",
      "loss: 0.645524  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.02082%, Avg loss: 0.660544 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.625560  [    0/ 3075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.572284  [  640/ 3075]\n",
      "loss: 0.685360  [ 1280/ 3075]\n",
      "loss: 0.649070  [ 1920/ 3075]\n",
      "loss: 0.619263  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.28107%, Avg loss: 0.660418 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.626859 \n",
      " Num Batches: 25\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.630828  [    0/ 3075]\n",
      "loss: 0.654380  [  640/ 3075]\n",
      "loss: 0.639538  [ 1280/ 3075]\n",
      "loss: 0.643521  [ 1920/ 3075]\n",
      "loss: 0.645180  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.28107%, Avg loss: 0.660186 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.624654  [    0/ 3075]\n",
      "loss: 0.570591  [  640/ 3075]\n",
      "loss: 0.685800  [ 1280/ 3075]\n",
      "loss: 0.647983  [ 1920/ 3075]\n",
      "loss: 0.617775  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.28107%, Avg loss: 0.660079 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.625668 \n",
      " Num Batches: 25\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.629758  [    0/ 3075]\n",
      "loss: 0.653464  [  640/ 3075]\n",
      "loss: 0.638547  [ 1280/ 3075]\n",
      "loss: 0.642619  [ 1920/ 3075]\n",
      "loss: 0.644860  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.34613%, Avg loss: 0.659860 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.623829  [    0/ 3075]\n",
      "loss: 0.569114  [  640/ 3075]\n",
      "loss: 0.686174  [ 1280/ 3075]\n",
      "loss: 0.646740  [ 1920/ 3075]\n",
      "loss: 0.616284  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.28107%, Avg loss: 0.659775 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.624505 \n",
      " Num Batches: 25\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.628731  [    0/ 3075]\n",
      "loss: 0.652558  [  640/ 3075]\n",
      "loss: 0.637625  [ 1280/ 3075]\n",
      "loss: 0.641696  [ 1920/ 3075]\n",
      "loss: 0.644514  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.28107%, Avg loss: 0.659570 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.623061  [    0/ 3075]\n",
      "loss: 0.567676  [  640/ 3075]\n",
      "loss: 0.686583  [ 1280/ 3075]\n",
      "loss: 0.645665  [ 1920/ 3075]\n",
      "loss: 0.614872  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.41119%, Avg loss: 0.659486 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.623428 \n",
      " Num Batches: 25\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.627738  [    0/ 3075]\n",
      "loss: 0.651636  [  640/ 3075]\n",
      "loss: 0.636744  [ 1280/ 3075]\n",
      "loss: 0.640840  [ 1920/ 3075]\n",
      "loss: 0.644258  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.659294 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.622317  [    0/ 3075]\n",
      "loss: 0.566179  [  640/ 3075]\n",
      "loss: 0.687113  [ 1280/ 3075]\n",
      "loss: 0.644634  [ 1920/ 3075]\n",
      "loss: 0.613406  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.659235 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.5%, Avg loss: 0.622345 \n",
      " Num Batches: 25\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.626781  [    0/ 3075]\n",
      "loss: 0.650649  [  640/ 3075]\n",
      "loss: 0.635815  [ 1280/ 3075]\n",
      "loss: 0.640080  [ 1920/ 3075]\n",
      "loss: 0.644172  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.659054 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.621553  [    0/ 3075]\n",
      "loss: 0.564462  [  640/ 3075]\n",
      "loss: 0.687679  [ 1280/ 3075]\n",
      "loss: 0.643592  [ 1920/ 3075]\n",
      "loss: 0.611976  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.73650%, Avg loss: 0.658997 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.621293 \n",
      " Num Batches: 25\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.625881  [    0/ 3075]\n",
      "loss: 0.649742  [  640/ 3075]\n",
      "loss: 0.635012  [ 1280/ 3075]\n",
      "loss: 0.639239  [ 1920/ 3075]\n",
      "loss: 0.643899  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.73650%, Avg loss: 0.658829 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.620902  [    0/ 3075]\n",
      "loss: 0.563079  [  640/ 3075]\n",
      "loss: 0.688118  [ 1280/ 3075]\n",
      "loss: 0.642517  [ 1920/ 3075]\n",
      "loss: 0.610608  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.99675%, Avg loss: 0.658785 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.620313 \n",
      " Num Batches: 25\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.625033  [    0/ 3075]\n",
      "loss: 0.648842  [  640/ 3075]\n",
      "loss: 0.634254  [ 1280/ 3075]\n",
      "loss: 0.638451  [ 1920/ 3075]\n",
      "loss: 0.643664  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.86662%, Avg loss: 0.658628 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.620297  [    0/ 3075]\n",
      "loss: 0.561773  [  640/ 3075]\n",
      "loss: 0.688552  [ 1280/ 3075]\n",
      "loss: 0.641452  [ 1920/ 3075]\n",
      "loss: 0.609296  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.06181%, Avg loss: 0.658592 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.619368 \n",
      " Num Batches: 25\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.624204  [    0/ 3075]\n",
      "loss: 0.647962  [  640/ 3075]\n",
      "loss: 0.633563  [ 1280/ 3075]\n",
      "loss: 0.637683  [ 1920/ 3075]\n",
      "loss: 0.643432  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.06181%, Avg loss: 0.658448 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.619734  [    0/ 3075]\n",
      "loss: 0.560529  [  640/ 3075]\n",
      "loss: 0.688981  [ 1280/ 3075]\n",
      "loss: 0.640449  [ 1920/ 3075]\n",
      "loss: 0.608047  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.06181%, Avg loss: 0.658419 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.618489 \n",
      " Num Batches: 25\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.623419  [    0/ 3075]\n",
      "loss: 0.647098  [  640/ 3075]\n",
      "loss: 0.632933  [ 1280/ 3075]\n",
      "loss: 0.636974  [ 1920/ 3075]\n",
      "loss: 0.643264  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.06181%, Avg loss: 0.658286 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.619185  [    0/ 3075]\n",
      "loss: 0.559256  [  640/ 3075]\n",
      "loss: 0.689398  [ 1280/ 3075]\n",
      "loss: 0.639504  [ 1920/ 3075]\n",
      "loss: 0.606856  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.99675%, Avg loss: 0.658266 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.617661 \n",
      " Num Batches: 25\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.622662  [    0/ 3075]\n",
      "loss: 0.646267  [  640/ 3075]\n",
      "loss: 0.632347  [ 1280/ 3075]\n",
      "loss: 0.636291  [ 1920/ 3075]\n",
      "loss: 0.643146  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.99675%, Avg loss: 0.658143 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.618667  [    0/ 3075]\n",
      "loss: 0.558040  [  640/ 3075]\n",
      "loss: 0.689788  [ 1280/ 3075]\n",
      "loss: 0.638581  [ 1920/ 3075]\n",
      "loss: 0.605725  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.19193%, Avg loss: 0.658131 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.616881 \n",
      " Num Batches: 25\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.621933  [    0/ 3075]\n",
      "loss: 0.645474  [  640/ 3075]\n",
      "loss: 0.631806  [ 1280/ 3075]\n",
      "loss: 0.635625  [ 1920/ 3075]\n",
      "loss: 0.643042  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.19193%, Avg loss: 0.658018 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.618184  [    0/ 3075]\n",
      "loss: 0.556862  [  640/ 3075]\n",
      "loss: 0.690189  [ 1280/ 3075]\n",
      "loss: 0.637764  [ 1920/ 3075]\n",
      "loss: 0.604675  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.45218%, Avg loss: 0.658008 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.616152 \n",
      " Num Batches: 25\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.621222  [    0/ 3075]\n",
      "loss: 0.644706  [  640/ 3075]\n",
      "loss: 0.631284  [ 1280/ 3075]\n",
      "loss: 0.635004  [ 1920/ 3075]\n",
      "loss: 0.642971  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.45218%, Avg loss: 0.657904 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.617736  [    0/ 3075]\n",
      "loss: 0.555691  [  640/ 3075]\n",
      "loss: 0.690596  [ 1280/ 3075]\n",
      "loss: 0.636998  [ 1920/ 3075]\n",
      "loss: 0.603666  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.45218%, Avg loss: 0.657900 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.615460 \n",
      " Num Batches: 25\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.620529  [    0/ 3075]\n",
      "loss: 0.643955  [  640/ 3075]\n",
      "loss: 0.630793  [ 1280/ 3075]\n",
      "loss: 0.634412  [ 1920/ 3075]\n",
      "loss: 0.642899  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.45218%, Avg loss: 0.657804 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.617310  [    0/ 3075]\n",
      "loss: 0.554599  [  640/ 3075]\n",
      "loss: 0.690997  [ 1280/ 3075]\n",
      "loss: 0.636252  [ 1920/ 3075]\n",
      "loss: 0.602696  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.45218%, Avg loss: 0.657812 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.614796 \n",
      " Num Batches: 25\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.619852  [    0/ 3075]\n",
      "loss: 0.643231  [  640/ 3075]\n",
      "loss: 0.630350  [ 1280/ 3075]\n",
      "loss: 0.633823  [ 1920/ 3075]\n",
      "loss: 0.642824  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.32206%, Avg loss: 0.657725 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.616908  [    0/ 3075]\n",
      "loss: 0.553600  [  640/ 3075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.691387  [ 1280/ 3075]\n",
      "loss: 0.635490  [ 1920/ 3075]\n",
      "loss: 0.601735  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.38712%, Avg loss: 0.657742 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.614157 \n",
      " Num Batches: 25\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.619207  [    0/ 3075]\n",
      "loss: 0.642530  [  640/ 3075]\n",
      "loss: 0.629939  [ 1280/ 3075]\n",
      "loss: 0.633247  [ 1920/ 3075]\n",
      "loss: 0.642747  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.38712%, Avg loss: 0.657663 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.616532  [    0/ 3075]\n",
      "loss: 0.552618  [  640/ 3075]\n",
      "loss: 0.691777  [ 1280/ 3075]\n",
      "loss: 0.634764  [ 1920/ 3075]\n",
      "loss: 0.600795  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.25699%, Avg loss: 0.657687 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.613549 \n",
      " Num Batches: 25\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.618604  [    0/ 3075]\n",
      "loss: 0.641847  [  640/ 3075]\n",
      "loss: 0.629561  [ 1280/ 3075]\n",
      "loss: 0.632688  [ 1920/ 3075]\n",
      "loss: 0.642681  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.25699%, Avg loss: 0.657616 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.616183  [    0/ 3075]\n",
      "loss: 0.551675  [  640/ 3075]\n",
      "loss: 0.692158  [ 1280/ 3075]\n",
      "loss: 0.634065  [ 1920/ 3075]\n",
      "loss: 0.599882  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.64736%, Avg loss: 0.657644 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.612963 \n",
      " Num Batches: 25\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.618036  [    0/ 3075]\n",
      "loss: 0.641194  [  640/ 3075]\n",
      "loss: 0.629202  [ 1280/ 3075]\n",
      "loss: 0.632134  [ 1920/ 3075]\n",
      "loss: 0.642615  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.64736%, Avg loss: 0.657581 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.615853  [    0/ 3075]\n",
      "loss: 0.550830  [  640/ 3075]\n",
      "loss: 0.692511  [ 1280/ 3075]\n",
      "loss: 0.633338  [ 1920/ 3075]\n",
      "loss: 0.598987  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.77749%, Avg loss: 0.657617 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.612400 \n",
      " Num Batches: 25\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.617497  [    0/ 3075]\n",
      "loss: 0.640571  [  640/ 3075]\n",
      "loss: 0.628863  [ 1280/ 3075]\n",
      "loss: 0.631595  [ 1920/ 3075]\n",
      "loss: 0.642561  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.84255%, Avg loss: 0.657560 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.615535  [    0/ 3075]\n",
      "loss: 0.549988  [  640/ 3075]\n",
      "loss: 0.692822  [ 1280/ 3075]\n",
      "loss: 0.632664  [ 1920/ 3075]\n",
      "loss: 0.598127  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.77749%, Avg loss: 0.657613 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.611855 \n",
      " Num Batches: 25\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.616992  [    0/ 3075]\n",
      "loss: 0.639975  [  640/ 3075]\n",
      "loss: 0.628570  [ 1280/ 3075]\n",
      "loss: 0.631074  [ 1920/ 3075]\n",
      "loss: 0.642501  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.77749%, Avg loss: 0.657562 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.615246  [    0/ 3075]\n",
      "loss: 0.549221  [  640/ 3075]\n",
      "loss: 0.693121  [ 1280/ 3075]\n",
      "loss: 0.631986  [ 1920/ 3075]\n",
      "loss: 0.597284  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.77749%, Avg loss: 0.657621 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.611329 \n",
      " Num Batches: 25\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.616514  [    0/ 3075]\n",
      "loss: 0.639395  [  640/ 3075]\n",
      "loss: 0.628276  [ 1280/ 3075]\n",
      "loss: 0.630565  [ 1920/ 3075]\n",
      "loss: 0.642476  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.77749%, Avg loss: 0.657576 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.614962  [    0/ 3075]\n",
      "loss: 0.548479  [  640/ 3075]\n",
      "loss: 0.693482  [ 1280/ 3075]\n",
      "loss: 0.631328  [ 1920/ 3075]\n",
      "loss: 0.596451  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.84255%, Avg loss: 0.657649 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.610813 \n",
      " Num Batches: 25\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.616058  [    0/ 3075]\n",
      "loss: 0.638819  [  640/ 3075]\n",
      "loss: 0.627997  [ 1280/ 3075]\n",
      "loss: 0.630078  [ 1920/ 3075]\n",
      "loss: 0.642483  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.84255%, Avg loss: 0.657611 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.614678  [    0/ 3075]\n",
      "loss: 0.547718  [  640/ 3075]\n",
      "loss: 0.693825  [ 1280/ 3075]\n",
      "loss: 0.630709  [ 1920/ 3075]\n",
      "loss: 0.595649  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.97267%, Avg loss: 0.657676 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.610333 \n",
      " Num Batches: 25\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.615624  [    0/ 3075]\n",
      "loss: 0.638259  [  640/ 3075]\n",
      "loss: 0.627728  [ 1280/ 3075]\n",
      "loss: 0.629631  [ 1920/ 3075]\n",
      "loss: 0.642513  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.657643 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.614402  [    0/ 3075]\n",
      "loss: 0.546926  [  640/ 3075]\n",
      "loss: 0.694170  [ 1280/ 3075]\n",
      "loss: 0.630166  [ 1920/ 3075]\n",
      "loss: 0.594883  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.657710 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.609871 \n",
      " Num Batches: 25\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.615212  [    0/ 3075]\n",
      "loss: 0.637705  [  640/ 3075]\n",
      "loss: 0.627474  [ 1280/ 3075]\n",
      "loss: 0.629216  [ 1920/ 3075]\n",
      "loss: 0.642575  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.657682 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.614142  [    0/ 3075]\n",
      "loss: 0.546212  [  640/ 3075]\n",
      "loss: 0.694507  [ 1280/ 3075]\n",
      "loss: 0.629552  [ 1920/ 3075]\n",
      "loss: 0.594103  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.657769 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.609410 \n",
      " Num Batches: 25\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.614808  [    0/ 3075]\n",
      "loss: 0.637167  [  640/ 3075]\n",
      "loss: 0.627262  [ 1280/ 3075]\n",
      "loss: 0.628792  [ 1920/ 3075]\n",
      "loss: 0.642598  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.97267%, Avg loss: 0.657744 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.613904  [    0/ 3075]\n",
      "loss: 0.545492  [  640/ 3075]\n",
      "loss: 0.694875  [ 1280/ 3075]\n",
      "loss: 0.628969  [ 1920/ 3075]\n",
      "loss: 0.593314  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.657842 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.608906 \n",
      " Num Batches: 25\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.614392  [    0/ 3075]\n",
      "loss: 0.636606  [  640/ 3075]\n",
      "loss: 0.627030  [ 1280/ 3075]\n",
      "loss: 0.628393  [ 1920/ 3075]\n",
      "loss: 0.642625  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.657822 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.613634  [    0/ 3075]\n",
      "loss: 0.544707  [  640/ 3075]\n",
      "loss: 0.695272  [ 1280/ 3075]\n",
      "loss: 0.628258  [ 1920/ 3075]\n",
      "loss: 0.592424  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.657937 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.608419 \n",
      " Num Batches: 25\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.613979  [    0/ 3075]\n",
      "loss: 0.636060  [  640/ 3075]\n",
      "loss: 0.626874  [ 1280/ 3075]\n",
      "loss: 0.627991  [ 1920/ 3075]\n",
      "loss: 0.642634  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.657921 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.613401  [    0/ 3075]\n",
      "loss: 0.544032  [  640/ 3075]\n",
      "loss: 0.695624  [ 1280/ 3075]\n",
      "loss: 0.627656  [ 1920/ 3075]\n",
      "loss: 0.591664  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.658049 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.607993 \n",
      " Num Batches: 25\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.613620  [    0/ 3075]\n",
      "loss: 0.635561  [  640/ 3075]\n",
      "loss: 0.626729  [ 1280/ 3075]\n",
      "loss: 0.627636  [ 1920/ 3075]\n",
      "loss: 0.642667  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.658036 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.613194  [    0/ 3075]\n",
      "loss: 0.543362  [  640/ 3075]\n",
      "loss: 0.695968  [ 1280/ 3075]\n",
      "loss: 0.627132  [ 1920/ 3075]\n",
      "loss: 0.590949  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.16786%, Avg loss: 0.658158 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.607588 \n",
      " Num Batches: 25\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.613274  [    0/ 3075]\n",
      "loss: 0.635079  [  640/ 3075]\n",
      "loss: 0.626595  [ 1280/ 3075]\n",
      "loss: 0.627310  [ 1920/ 3075]\n",
      "loss: 0.642741  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.16786%, Avg loss: 0.658148 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.612983  [    0/ 3075]\n",
      "loss: 0.542619  [  640/ 3075]\n",
      "loss: 0.696306  [ 1280/ 3075]\n",
      "loss: 0.626686  [ 1920/ 3075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.590277  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.658270 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.607201 \n",
      " Num Batches: 25\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.612942  [    0/ 3075]\n",
      "loss: 0.634620  [  640/ 3075]\n",
      "loss: 0.626495  [ 1280/ 3075]\n",
      "loss: 0.627014  [ 1920/ 3075]\n",
      "loss: 0.642821  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.658262 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.612783  [    0/ 3075]\n",
      "loss: 0.541964  [  640/ 3075]\n",
      "loss: 0.696708  [ 1280/ 3075]\n",
      "loss: 0.626182  [ 1920/ 3075]\n",
      "loss: 0.589555  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.658395 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.606820 \n",
      " Num Batches: 25\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.612623  [    0/ 3075]\n",
      "loss: 0.634165  [  640/ 3075]\n",
      "loss: 0.626400  [ 1280/ 3075]\n",
      "loss: 0.626712  [ 1920/ 3075]\n",
      "loss: 0.642895  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.658390 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.612617  [    0/ 3075]\n",
      "loss: 0.541313  [  640/ 3075]\n",
      "loss: 0.697031  [ 1280/ 3075]\n",
      "loss: 0.625721  [ 1920/ 3075]\n",
      "loss: 0.588886  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.658518 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.606453 \n",
      " Num Batches: 25\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.612313  [    0/ 3075]\n",
      "loss: 0.633731  [  640/ 3075]\n",
      "loss: 0.626331  [ 1280/ 3075]\n",
      "loss: 0.626433  [ 1920/ 3075]\n",
      "loss: 0.642969  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.16786%, Avg loss: 0.658514 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.612427  [    0/ 3075]\n",
      "loss: 0.540639  [  640/ 3075]\n",
      "loss: 0.697335  [ 1280/ 3075]\n",
      "loss: 0.625283  [ 1920/ 3075]\n",
      "loss: 0.588216  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.16786%, Avg loss: 0.658653 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.606071 \n",
      " Num Batches: 25\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.611993  [    0/ 3075]\n",
      "loss: 0.633298  [  640/ 3075]\n",
      "loss: 0.626296  [ 1280/ 3075]\n",
      "loss: 0.626161  [ 1920/ 3075]\n",
      "loss: 0.643024  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.23292%, Avg loss: 0.658651 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.612209  [    0/ 3075]\n",
      "loss: 0.539876  [  640/ 3075]\n",
      "loss: 0.697573  [ 1280/ 3075]\n",
      "loss: 0.624848  [ 1920/ 3075]\n",
      "loss: 0.587569  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.23292%, Avg loss: 0.658779 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.605729 \n",
      " Num Batches: 25\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.611707  [    0/ 3075]\n",
      "loss: 0.632886  [  640/ 3075]\n",
      "loss: 0.626259  [ 1280/ 3075]\n",
      "loss: 0.625915  [ 1920/ 3075]\n",
      "loss: 0.643133  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.16786%, Avg loss: 0.658779 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.612016  [    0/ 3075]\n",
      "loss: 0.539210  [  640/ 3075]\n",
      "loss: 0.697843  [ 1280/ 3075]\n",
      "loss: 0.624466  [ 1920/ 3075]\n",
      "loss: 0.586990  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.23292%, Avg loss: 0.658882 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.605405 \n",
      " Num Batches: 25\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.611424  [    0/ 3075]\n",
      "loss: 0.632483  [  640/ 3075]\n",
      "loss: 0.626220  [ 1280/ 3075]\n",
      "loss: 0.625697  [ 1920/ 3075]\n",
      "loss: 0.643294  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.23292%, Avg loss: 0.658883 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.611805  [    0/ 3075]\n",
      "loss: 0.538462  [  640/ 3075]\n",
      "loss: 0.698243  [ 1280/ 3075]\n",
      "loss: 0.624137  [ 1920/ 3075]\n",
      "loss: 0.586373  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.16786%, Avg loss: 0.658999 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.605081 \n",
      " Num Batches: 25\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.611147  [    0/ 3075]\n",
      "loss: 0.632080  [  640/ 3075]\n",
      "loss: 0.626185  [ 1280/ 3075]\n",
      "loss: 0.625477  [ 1920/ 3075]\n",
      "loss: 0.643448  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.659001 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.611643  [    0/ 3075]\n",
      "loss: 0.537806  [  640/ 3075]\n",
      "loss: 0.698524  [ 1280/ 3075]\n",
      "loss: 0.623750  [ 1920/ 3075]\n",
      "loss: 0.585788  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.97267%, Avg loss: 0.659127 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.604768 \n",
      " Num Batches: 25\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.610891  [    0/ 3075]\n",
      "loss: 0.631711  [  640/ 3075]\n",
      "loss: 0.626193  [ 1280/ 3075]\n",
      "loss: 0.625265  [ 1920/ 3075]\n",
      "loss: 0.643547  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.97267%, Avg loss: 0.659129 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.611466  [    0/ 3075]\n",
      "loss: 0.537179  [  640/ 3075]\n",
      "loss: 0.698754  [ 1280/ 3075]\n",
      "loss: 0.623382  [ 1920/ 3075]\n",
      "loss: 0.585228  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.659255 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.604466 \n",
      " Num Batches: 25\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.610646  [    0/ 3075]\n",
      "loss: 0.631356  [  640/ 3075]\n",
      "loss: 0.626209  [ 1280/ 3075]\n",
      "loss: 0.625061  [ 1920/ 3075]\n",
      "loss: 0.643640  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.659258 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.611304  [    0/ 3075]\n",
      "loss: 0.536615  [  640/ 3075]\n",
      "loss: 0.699070  [ 1280/ 3075]\n",
      "loss: 0.622989  [ 1920/ 3075]\n",
      "loss: 0.584635  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.29798%, Avg loss: 0.659389 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.604170 \n",
      " Num Batches: 25\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.610404  [    0/ 3075]\n",
      "loss: 0.631002  [  640/ 3075]\n",
      "loss: 0.626224  [ 1280/ 3075]\n",
      "loss: 0.624854  [ 1920/ 3075]\n",
      "loss: 0.643732  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.29798%, Avg loss: 0.659393 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.611180  [    0/ 3075]\n",
      "loss: 0.536091  [  640/ 3075]\n",
      "loss: 0.699301  [ 1280/ 3075]\n",
      "loss: 0.622598  [ 1920/ 3075]\n",
      "loss: 0.584085  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.36304%, Avg loss: 0.659522 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.603882 \n",
      " Num Batches: 25\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.610170  [    0/ 3075]\n",
      "loss: 0.630669  [  640/ 3075]\n",
      "loss: 0.626263  [ 1280/ 3075]\n",
      "loss: 0.624668  [ 1920/ 3075]\n",
      "loss: 0.643815  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.29798%, Avg loss: 0.659526 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.611027  [    0/ 3075]\n",
      "loss: 0.535553  [  640/ 3075]\n",
      "loss: 0.699499  [ 1280/ 3075]\n",
      "loss: 0.622227  [ 1920/ 3075]\n",
      "loss: 0.583559  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.23292%, Avg loss: 0.659660 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.603606 \n",
      " Num Batches: 25\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.609946  [    0/ 3075]\n",
      "loss: 0.630346  [  640/ 3075]\n",
      "loss: 0.626317  [ 1280/ 3075]\n",
      "loss: 0.624489  [ 1920/ 3075]\n",
      "loss: 0.643895  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.23292%, Avg loss: 0.659664 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.610881  [    0/ 3075]\n",
      "loss: 0.535030  [  640/ 3075]\n",
      "loss: 0.699748  [ 1280/ 3075]\n",
      "loss: 0.621865  [ 1920/ 3075]\n",
      "loss: 0.583024  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.16786%, Avg loss: 0.659790 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.603337 \n",
      " Num Batches: 25\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.609726  [    0/ 3075]\n",
      "loss: 0.630034  [  640/ 3075]\n",
      "loss: 0.626371  [ 1280/ 3075]\n",
      "loss: 0.624313  [ 1920/ 3075]\n",
      "loss: 0.643958  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.16786%, Avg loss: 0.659794 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.610774  [    0/ 3075]\n",
      "loss: 0.534527  [  640/ 3075]\n",
      "loss: 0.699945  [ 1280/ 3075]\n",
      "loss: 0.621543  [ 1920/ 3075]\n",
      "loss: 0.582530  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.16786%, Avg loss: 0.659910 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.603080 \n",
      " Num Batches: 25\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.609512  [    0/ 3075]\n",
      "loss: 0.629728  [  640/ 3075]\n",
      "loss: 0.626428  [ 1280/ 3075]\n",
      "loss: 0.624163  [ 1920/ 3075]\n",
      "loss: 0.644062  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.659914 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.610625  [    0/ 3075]\n",
      "loss: 0.533917  [  640/ 3075]\n",
      "loss: 0.700120  [ 1280/ 3075]\n",
      "loss: 0.621274  [ 1920/ 3075]\n",
      "loss: 0.582064  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.660016 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.602834 \n",
      " Num Batches: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.609293  [    0/ 3075]\n",
      "loss: 0.629426  [  640/ 3075]\n",
      "loss: 0.626474  [ 1280/ 3075]\n",
      "loss: 0.624029  [ 1920/ 3075]\n",
      "loss: 0.644176  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.660020 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.610466  [    0/ 3075]\n",
      "loss: 0.533388  [  640/ 3075]\n",
      "loss: 0.700353  [ 1280/ 3075]\n",
      "loss: 0.620971  [ 1920/ 3075]\n",
      "loss: 0.581571  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.660136 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.602592 \n",
      " Num Batches: 25\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.609094  [    0/ 3075]\n",
      "loss: 0.629137  [  640/ 3075]\n",
      "loss: 0.626540  [ 1280/ 3075]\n",
      "loss: 0.623868  [ 1920/ 3075]\n",
      "loss: 0.644264  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.16786%, Avg loss: 0.660140 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.610341  [    0/ 3075]\n",
      "loss: 0.532834  [  640/ 3075]\n",
      "loss: 0.700503  [ 1280/ 3075]\n",
      "loss: 0.620705  [ 1920/ 3075]\n",
      "loss: 0.581121  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.16786%, Avg loss: 0.660241 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.602354 \n",
      " Num Batches: 25\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.608886  [    0/ 3075]\n",
      "loss: 0.628860  [  640/ 3075]\n",
      "loss: 0.626604  [ 1280/ 3075]\n",
      "loss: 0.623735  [ 1920/ 3075]\n",
      "loss: 0.644353  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.16786%, Avg loss: 0.660244 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.610173  [    0/ 3075]\n",
      "loss: 0.532343  [  640/ 3075]\n",
      "loss: 0.700666  [ 1280/ 3075]\n",
      "loss: 0.620404  [ 1920/ 3075]\n",
      "loss: 0.580647  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.660365 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.602122 \n",
      " Num Batches: 25\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.608703  [    0/ 3075]\n",
      "loss: 0.628604  [  640/ 3075]\n",
      "loss: 0.626692  [ 1280/ 3075]\n",
      "loss: 0.623597  [ 1920/ 3075]\n",
      "loss: 0.644403  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.660369 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.610062  [    0/ 3075]\n",
      "loss: 0.531900  [  640/ 3075]\n",
      "loss: 0.700756  [ 1280/ 3075]\n",
      "loss: 0.620125  [ 1920/ 3075]\n",
      "loss: 0.580215  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.660488 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.601895 \n",
      " Num Batches: 25\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.608526  [    0/ 3075]\n",
      "loss: 0.628359  [  640/ 3075]\n",
      "loss: 0.626794  [ 1280/ 3075]\n",
      "loss: 0.623476  [ 1920/ 3075]\n",
      "loss: 0.644467  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.660491 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.609915  [    0/ 3075]\n",
      "loss: 0.531441  [  640/ 3075]\n",
      "loss: 0.700922  [ 1280/ 3075]\n",
      "loss: 0.619843  [ 1920/ 3075]\n",
      "loss: 0.579755  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.660614 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.601671 \n",
      " Num Batches: 25\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.608348  [    0/ 3075]\n",
      "loss: 0.628115  [  640/ 3075]\n",
      "loss: 0.626900  [ 1280/ 3075]\n",
      "loss: 0.623345  [ 1920/ 3075]\n",
      "loss: 0.644521  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.660617 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.609804  [    0/ 3075]\n",
      "loss: 0.530996  [  640/ 3075]\n",
      "loss: 0.700973  [ 1280/ 3075]\n",
      "loss: 0.619568  [ 1920/ 3075]\n",
      "loss: 0.579352  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.660741 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.601452 \n",
      " Num Batches: 25\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.608183  [    0/ 3075]\n",
      "loss: 0.627908  [  640/ 3075]\n",
      "loss: 0.627021  [ 1280/ 3075]\n",
      "loss: 0.623230  [ 1920/ 3075]\n",
      "loss: 0.644560  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.660743 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.609675  [    0/ 3075]\n",
      "loss: 0.530548  [  640/ 3075]\n",
      "loss: 0.701086  [ 1280/ 3075]\n",
      "loss: 0.619268  [ 1920/ 3075]\n",
      "loss: 0.578898  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.660884 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.601232 \n",
      " Num Batches: 25\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.608020  [    0/ 3075]\n",
      "loss: 0.627721  [  640/ 3075]\n",
      "loss: 0.627139  [ 1280/ 3075]\n",
      "loss: 0.623101  [ 1920/ 3075]\n",
      "loss: 0.644591  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.660885 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.609617  [    0/ 3075]\n",
      "loss: 0.530160  [  640/ 3075]\n",
      "loss: 0.701135  [ 1280/ 3075]\n",
      "loss: 0.618967  [ 1920/ 3075]\n",
      "loss: 0.578473  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.661028 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.601011 \n",
      " Num Batches: 25\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.607866  [    0/ 3075]\n",
      "loss: 0.627509  [  640/ 3075]\n",
      "loss: 0.627296  [ 1280/ 3075]\n",
      "loss: 0.622987  [ 1920/ 3075]\n",
      "loss: 0.644602  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.661028 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.609469  [    0/ 3075]\n",
      "loss: 0.529729  [  640/ 3075]\n",
      "loss: 0.701236  [ 1280/ 3075]\n",
      "loss: 0.618738  [ 1920/ 3075]\n",
      "loss: 0.578074  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.661150 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.600814 \n",
      " Num Batches: 25\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.607716  [    0/ 3075]\n",
      "loss: 0.627328  [  640/ 3075]\n",
      "loss: 0.627407  [ 1280/ 3075]\n",
      "loss: 0.622881  [ 1920/ 3075]\n",
      "loss: 0.644659  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.661150 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.609381  [    0/ 3075]\n",
      "loss: 0.529329  [  640/ 3075]\n",
      "loss: 0.701267  [ 1280/ 3075]\n",
      "loss: 0.618456  [ 1920/ 3075]\n",
      "loss: 0.577682  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.661288 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.600609 \n",
      " Num Batches: 25\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.607583  [    0/ 3075]\n",
      "loss: 0.627146  [  640/ 3075]\n",
      "loss: 0.627552  [ 1280/ 3075]\n",
      "loss: 0.622776  [ 1920/ 3075]\n",
      "loss: 0.644679  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.661287 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.609262  [    0/ 3075]\n",
      "loss: 0.528899  [  640/ 3075]\n",
      "loss: 0.701396  [ 1280/ 3075]\n",
      "loss: 0.618267  [ 1920/ 3075]\n",
      "loss: 0.577300  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.661392 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.600415 \n",
      " Num Batches: 25\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.607428  [    0/ 3075]\n",
      "loss: 0.626968  [  640/ 3075]\n",
      "loss: 0.627643  [ 1280/ 3075]\n",
      "loss: 0.622686  [ 1920/ 3075]\n",
      "loss: 0.644783  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.661390 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.609164  [    0/ 3075]\n",
      "loss: 0.528421  [  640/ 3075]\n",
      "loss: 0.701413  [ 1280/ 3075]\n",
      "loss: 0.618060  [ 1920/ 3075]\n",
      "loss: 0.576947  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.97267%, Avg loss: 0.661500 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.600224 \n",
      " Num Batches: 25\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.607293  [    0/ 3075]\n",
      "loss: 0.626775  [  640/ 3075]\n",
      "loss: 0.627777  [ 1280/ 3075]\n",
      "loss: 0.622593  [ 1920/ 3075]\n",
      "loss: 0.644848  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.97267%, Avg loss: 0.661497 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.609007  [    0/ 3075]\n",
      "loss: 0.528017  [  640/ 3075]\n",
      "loss: 0.701522  [ 1280/ 3075]\n",
      "loss: 0.617828  [ 1920/ 3075]\n",
      "loss: 0.576554  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.661623 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.600042 \n",
      " Num Batches: 25\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.607171  [    0/ 3075]\n",
      "loss: 0.626627  [  640/ 3075]\n",
      "loss: 0.627904  [ 1280/ 3075]\n",
      "loss: 0.622494  [ 1920/ 3075]\n",
      "loss: 0.644881  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.661620 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.608964  [    0/ 3075]\n",
      "loss: 0.527632  [  640/ 3075]\n",
      "loss: 0.701525  [ 1280/ 3075]\n",
      "loss: 0.617613  [ 1920/ 3075]\n",
      "loss: 0.576216  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.97267%, Avg loss: 0.661740 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.599860 \n",
      " Num Batches: 25\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.607056  [    0/ 3075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.626480  [  640/ 3075]\n",
      "loss: 0.628048  [ 1280/ 3075]\n",
      "loss: 0.622412  [ 1920/ 3075]\n",
      "loss: 0.644916  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.97267%, Avg loss: 0.661736 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.608847  [    0/ 3075]\n",
      "loss: 0.527213  [  640/ 3075]\n",
      "loss: 0.701615  [ 1280/ 3075]\n",
      "loss: 0.617430  [ 1920/ 3075]\n",
      "loss: 0.575857  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.661854 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.599683 \n",
      " Num Batches: 25\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.606937  [    0/ 3075]\n",
      "loss: 0.626330  [  640/ 3075]\n",
      "loss: 0.628175  [ 1280/ 3075]\n",
      "loss: 0.622326  [ 1920/ 3075]\n",
      "loss: 0.644973  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.661849 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.608772  [    0/ 3075]\n",
      "loss: 0.526847  [  640/ 3075]\n",
      "loss: 0.701681  [ 1280/ 3075]\n",
      "loss: 0.617194  [ 1920/ 3075]\n",
      "loss: 0.575472  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.661996 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.599501 \n",
      " Num Batches: 25\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.606846  [    0/ 3075]\n",
      "loss: 0.626204  [  640/ 3075]\n",
      "loss: 0.628401  [ 1280/ 3075]\n",
      "loss: 0.622201  [ 1920/ 3075]\n",
      "loss: 0.644957  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.97267%, Avg loss: 0.661990 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.608694  [    0/ 3075]\n",
      "loss: 0.526589  [  640/ 3075]\n",
      "loss: 0.701629  [ 1280/ 3075]\n",
      "loss: 0.616984  [ 1920/ 3075]\n",
      "loss: 0.575117  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.662098 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.599329 \n",
      " Num Batches: 25\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.606722  [    0/ 3075]\n",
      "loss: 0.626066  [  640/ 3075]\n",
      "loss: 0.628538  [ 1280/ 3075]\n",
      "loss: 0.622128  [ 1920/ 3075]\n",
      "loss: 0.645015  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.662091 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.608582  [    0/ 3075]\n",
      "loss: 0.526073  [  640/ 3075]\n",
      "loss: 0.701702  [ 1280/ 3075]\n",
      "loss: 0.616860  [ 1920/ 3075]\n",
      "loss: 0.574793  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.662195 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.599158 \n",
      " Num Batches: 25\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.606605  [    0/ 3075]\n",
      "loss: 0.625931  [  640/ 3075]\n",
      "loss: 0.628657  [ 1280/ 3075]\n",
      "loss: 0.622058  [ 1920/ 3075]\n",
      "loss: 0.645088  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.10280%, Avg loss: 0.662188 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.608472  [    0/ 3075]\n",
      "loss: 0.525671  [  640/ 3075]\n",
      "loss: 0.701631  [ 1280/ 3075]\n",
      "loss: 0.616658  [ 1920/ 3075]\n",
      "loss: 0.574495  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.662303 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.599001 \n",
      " Num Batches: 25\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.606511  [    0/ 3075]\n",
      "loss: 0.625835  [  640/ 3075]\n",
      "loss: 0.628831  [ 1280/ 3075]\n",
      "loss: 0.621988  [ 1920/ 3075]\n",
      "loss: 0.645080  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.03774%, Avg loss: 0.662295 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.608355  [    0/ 3075]\n",
      "loss: 0.525317  [  640/ 3075]\n",
      "loss: 0.701648  [ 1280/ 3075]\n",
      "loss: 0.616468  [ 1920/ 3075]\n",
      "loss: 0.574174  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.662406 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.598839 \n",
      " Num Batches: 25\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.606411  [    0/ 3075]\n",
      "loss: 0.625728  [  640/ 3075]\n",
      "loss: 0.628974  [ 1280/ 3075]\n",
      "loss: 0.621915  [ 1920/ 3075]\n",
      "loss: 0.645097  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.97267%, Avg loss: 0.662397 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.608262  [    0/ 3075]\n",
      "loss: 0.524898  [  640/ 3075]\n",
      "loss: 0.701667  [ 1280/ 3075]\n",
      "loss: 0.616320  [ 1920/ 3075]\n",
      "loss: 0.573871  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.662501 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.598685 \n",
      " Num Batches: 25\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.606311  [    0/ 3075]\n",
      "loss: 0.625627  [  640/ 3075]\n",
      "loss: 0.629116  [ 1280/ 3075]\n",
      "loss: 0.621851  [ 1920/ 3075]\n",
      "loss: 0.645120  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.662492 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.608168  [    0/ 3075]\n",
      "loss: 0.524570  [  640/ 3075]\n",
      "loss: 0.701571  [ 1280/ 3075]\n",
      "loss: 0.616124  [ 1920/ 3075]\n",
      "loss: 0.573590  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.662607 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.598535 \n",
      " Num Batches: 25\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.606236  [    0/ 3075]\n",
      "loss: 0.625552  [  640/ 3075]\n",
      "loss: 0.629297  [ 1280/ 3075]\n",
      "loss: 0.621786  [ 1920/ 3075]\n",
      "loss: 0.645083  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.90761%, Avg loss: 0.662598 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.608051  [    0/ 3075]\n",
      "loss: 0.524181  [  640/ 3075]\n",
      "loss: 0.701563  [ 1280/ 3075]\n",
      "loss: 0.615962  [ 1920/ 3075]\n",
      "loss: 0.573282  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.84255%, Avg loss: 0.662702 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.598380 \n",
      " Num Batches: 25\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.606147  [    0/ 3075]\n",
      "loss: 0.625454  [  640/ 3075]\n",
      "loss: 0.629436  [ 1280/ 3075]\n",
      "loss: 0.621729  [ 1920/ 3075]\n",
      "loss: 0.645099  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.84255%, Avg loss: 0.662691 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.607957  [    0/ 3075]\n",
      "loss: 0.523807  [  640/ 3075]\n",
      "loss: 0.701528  [ 1280/ 3075]\n",
      "loss: 0.615815  [ 1920/ 3075]\n",
      "loss: 0.572997  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.84255%, Avg loss: 0.662783 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.598238 \n",
      " Num Batches: 25\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.606050  [    0/ 3075]\n",
      "loss: 0.625366  [  640/ 3075]\n",
      "loss: 0.629564  [ 1280/ 3075]\n",
      "loss: 0.621677  [ 1920/ 3075]\n",
      "loss: 0.645110  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.84255%, Avg loss: 0.662773 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.607850  [    0/ 3075]\n",
      "loss: 0.523359  [  640/ 3075]\n",
      "loss: 0.701426  [ 1280/ 3075]\n",
      "loss: 0.615728  [ 1920/ 3075]\n",
      "loss: 0.572760  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.662847 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.598103 \n",
      " Num Batches: 25\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605954  [    0/ 3075]\n",
      "loss: 0.625289  [  640/ 3075]\n",
      "loss: 0.629703  [ 1280/ 3075]\n",
      "loss: 0.621642  [ 1920/ 3075]\n",
      "loss: 0.645122  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.662837 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.607697  [    0/ 3075]\n",
      "loss: 0.522987  [  640/ 3075]\n",
      "loss: 0.701318  [ 1280/ 3075]\n",
      "loss: 0.615574  [ 1920/ 3075]\n",
      "loss: 0.572496  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.662928 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.597973 \n",
      " Num Batches: 25\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605882  [    0/ 3075]\n",
      "loss: 0.625229  [  640/ 3075]\n",
      "loss: 0.629861  [ 1280/ 3075]\n",
      "loss: 0.621592  [ 1920/ 3075]\n",
      "loss: 0.645086  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.662917 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.607595  [    0/ 3075]\n",
      "loss: 0.522592  [  640/ 3075]\n",
      "loss: 0.701282  [ 1280/ 3075]\n",
      "loss: 0.615430  [ 1920/ 3075]\n",
      "loss: 0.572194  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.64736%, Avg loss: 0.663019 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.597829 \n",
      " Num Batches: 25\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605792  [    0/ 3075]\n",
      "loss: 0.625139  [  640/ 3075]\n",
      "loss: 0.630011  [ 1280/ 3075]\n",
      "loss: 0.621549  [ 1920/ 3075]\n",
      "loss: 0.645082  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.64736%, Avg loss: 0.663007 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.607515  [    0/ 3075]\n",
      "loss: 0.522236  [  640/ 3075]\n",
      "loss: 0.701169  [ 1280/ 3075]\n",
      "loss: 0.615276  [ 1920/ 3075]\n",
      "loss: 0.571947  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.64736%, Avg loss: 0.663092 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.597701 \n",
      " Num Batches: 25\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605711  [    0/ 3075]\n",
      "loss: 0.625090  [  640/ 3075]\n",
      "loss: 0.630171  [ 1280/ 3075]\n",
      "loss: 0.621518  [ 1920/ 3075]\n",
      "loss: 0.645051  [ 2560/ 3075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversary Test Error: \n",
      " Accuracy: 65.64736%, Avg loss: 0.663079 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.607420  [    0/ 3075]\n",
      "loss: 0.521915  [  640/ 3075]\n",
      "loss: 0.700921  [ 1280/ 3075]\n",
      "loss: 0.615097  [ 1920/ 3075]\n",
      "loss: 0.571715  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.64736%, Avg loss: 0.663184 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.597565 \n",
      " Num Batches: 25\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605650  [    0/ 3075]\n",
      "loss: 0.625047  [  640/ 3075]\n",
      "loss: 0.630372  [ 1280/ 3075]\n",
      "loss: 0.621489  [ 1920/ 3075]\n",
      "loss: 0.644979  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.64736%, Avg loss: 0.663171 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.607317  [    0/ 3075]\n",
      "loss: 0.521603  [  640/ 3075]\n",
      "loss: 0.700772  [ 1280/ 3075]\n",
      "loss: 0.614969  [ 1920/ 3075]\n",
      "loss: 0.571461  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.71243%, Avg loss: 0.663261 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.597436 \n",
      " Num Batches: 25\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605575  [    0/ 3075]\n",
      "loss: 0.624993  [  640/ 3075]\n",
      "loss: 0.630527  [ 1280/ 3075]\n",
      "loss: 0.621460  [ 1920/ 3075]\n",
      "loss: 0.644957  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.71243%, Avg loss: 0.663248 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.607224  [    0/ 3075]\n",
      "loss: 0.521283  [  640/ 3075]\n",
      "loss: 0.700609  [ 1280/ 3075]\n",
      "loss: 0.614805  [ 1920/ 3075]\n",
      "loss: 0.571224  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.71243%, Avg loss: 0.663335 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.597320 \n",
      " Num Batches: 25\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605527  [    0/ 3075]\n",
      "loss: 0.624958  [  640/ 3075]\n",
      "loss: 0.630716  [ 1280/ 3075]\n",
      "loss: 0.621431  [ 1920/ 3075]\n",
      "loss: 0.644884  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.71243%, Avg loss: 0.663321 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.607126  [    0/ 3075]\n",
      "loss: 0.520971  [  640/ 3075]\n",
      "loss: 0.700462  [ 1280/ 3075]\n",
      "loss: 0.614681  [ 1920/ 3075]\n",
      "loss: 0.570985  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.64736%, Avg loss: 0.663408 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.597198 \n",
      " Num Batches: 25\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605465  [    0/ 3075]\n",
      "loss: 0.624921  [  640/ 3075]\n",
      "loss: 0.630884  [ 1280/ 3075]\n",
      "loss: 0.621412  [ 1920/ 3075]\n",
      "loss: 0.644859  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.64736%, Avg loss: 0.663393 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.607037  [    0/ 3075]\n",
      "loss: 0.520658  [  640/ 3075]\n",
      "loss: 0.700235  [ 1280/ 3075]\n",
      "loss: 0.614543  [ 1920/ 3075]\n",
      "loss: 0.570787  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.71243%, Avg loss: 0.663487 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.597077 \n",
      " Num Batches: 25\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605420  [    0/ 3075]\n",
      "loss: 0.624897  [  640/ 3075]\n",
      "loss: 0.631089  [ 1280/ 3075]\n",
      "loss: 0.621400  [ 1920/ 3075]\n",
      "loss: 0.644798  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.71243%, Avg loss: 0.663472 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.606892  [    0/ 3075]\n",
      "loss: 0.520415  [  640/ 3075]\n",
      "loss: 0.700050  [ 1280/ 3075]\n",
      "loss: 0.614386  [ 1920/ 3075]\n",
      "loss: 0.570536  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.77749%, Avg loss: 0.663565 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.596964 \n",
      " Num Batches: 25\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605381  [    0/ 3075]\n",
      "loss: 0.624881  [  640/ 3075]\n",
      "loss: 0.631277  [ 1280/ 3075]\n",
      "loss: 0.621390  [ 1920/ 3075]\n",
      "loss: 0.644722  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.77749%, Avg loss: 0.663549 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.606804  [    0/ 3075]\n",
      "loss: 0.520111  [  640/ 3075]\n",
      "loss: 0.699860  [ 1280/ 3075]\n",
      "loss: 0.614286  [ 1920/ 3075]\n",
      "loss: 0.570313  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.71243%, Avg loss: 0.663627 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.596853 \n",
      " Num Batches: 25\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605346  [    0/ 3075]\n",
      "loss: 0.624856  [  640/ 3075]\n",
      "loss: 0.631456  [ 1280/ 3075]\n",
      "loss: 0.621384  [ 1920/ 3075]\n",
      "loss: 0.644674  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.71243%, Avg loss: 0.663611 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.606680  [    0/ 3075]\n",
      "loss: 0.519832  [  640/ 3075]\n",
      "loss: 0.699631  [ 1280/ 3075]\n",
      "loss: 0.614159  [ 1920/ 3075]\n",
      "loss: 0.570129  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.64736%, Avg loss: 0.663692 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.596749 \n",
      " Num Batches: 25\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605310  [    0/ 3075]\n",
      "loss: 0.624845  [  640/ 3075]\n",
      "loss: 0.631650  [ 1280/ 3075]\n",
      "loss: 0.621376  [ 1920/ 3075]\n",
      "loss: 0.644596  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.64736%, Avg loss: 0.663676 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.606540  [    0/ 3075]\n",
      "loss: 0.519513  [  640/ 3075]\n",
      "loss: 0.699548  [ 1280/ 3075]\n",
      "loss: 0.614019  [ 1920/ 3075]\n",
      "loss: 0.569873  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.663759 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.596642 \n",
      " Num Batches: 25\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605259  [    0/ 3075]\n",
      "loss: 0.624809  [  640/ 3075]\n",
      "loss: 0.631821  [ 1280/ 3075]\n",
      "loss: 0.621364  [ 1920/ 3075]\n",
      "loss: 0.644546  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.64736%, Avg loss: 0.663741 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.606455  [    0/ 3075]\n",
      "loss: 0.519260  [  640/ 3075]\n",
      "loss: 0.699329  [ 1280/ 3075]\n",
      "loss: 0.613869  [ 1920/ 3075]\n",
      "loss: 0.569682  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.663824 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.596544 \n",
      " Num Batches: 25\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605224  [    0/ 3075]\n",
      "loss: 0.624804  [  640/ 3075]\n",
      "loss: 0.632012  [ 1280/ 3075]\n",
      "loss: 0.621350  [ 1920/ 3075]\n",
      "loss: 0.644436  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.663806 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.606334  [    0/ 3075]\n",
      "loss: 0.519004  [  640/ 3075]\n",
      "loss: 0.699146  [ 1280/ 3075]\n",
      "loss: 0.613726  [ 1920/ 3075]\n",
      "loss: 0.569482  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.663884 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.596447 \n",
      " Num Batches: 25\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605187  [    0/ 3075]\n",
      "loss: 0.624795  [  640/ 3075]\n",
      "loss: 0.632191  [ 1280/ 3075]\n",
      "loss: 0.621347  [ 1920/ 3075]\n",
      "loss: 0.644363  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.663866 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.606223  [    0/ 3075]\n",
      "loss: 0.518702  [  640/ 3075]\n",
      "loss: 0.699037  [ 1280/ 3075]\n",
      "loss: 0.613609  [ 1920/ 3075]\n",
      "loss: 0.569260  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.663952 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.596341 \n",
      " Num Batches: 25\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605140  [    0/ 3075]\n",
      "loss: 0.624781  [  640/ 3075]\n",
      "loss: 0.632364  [ 1280/ 3075]\n",
      "loss: 0.621344  [ 1920/ 3075]\n",
      "loss: 0.644301  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.663932 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.606130  [    0/ 3075]\n",
      "loss: 0.518457  [  640/ 3075]\n",
      "loss: 0.698913  [ 1280/ 3075]\n",
      "loss: 0.613444  [ 1920/ 3075]\n",
      "loss: 0.569031  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.664021 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.596244 \n",
      " Num Batches: 25\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605110  [    0/ 3075]\n",
      "loss: 0.624778  [  640/ 3075]\n",
      "loss: 0.632544  [ 1280/ 3075]\n",
      "loss: 0.621337  [ 1920/ 3075]\n",
      "loss: 0.644214  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.664001 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.606055  [    0/ 3075]\n",
      "loss: 0.518337  [  640/ 3075]\n",
      "loss: 0.698808  [ 1280/ 3075]\n",
      "loss: 0.613193  [ 1920/ 3075]\n",
      "loss: 0.568772  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.664122 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.596144 \n",
      " Num Batches: 25\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605111  [    0/ 3075]\n",
      "loss: 0.624797  [  640/ 3075]\n",
      "loss: 0.632755  [ 1280/ 3075]\n",
      "loss: 0.621321  [ 1920/ 3075]\n",
      "loss: 0.644073  [ 2560/ 3075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.664100 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.606019  [    0/ 3075]\n",
      "loss: 0.518079  [  640/ 3075]\n",
      "loss: 0.698680  [ 1280/ 3075]\n",
      "loss: 0.613068  [ 1920/ 3075]\n",
      "loss: 0.568559  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.664188 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.596040 \n",
      " Num Batches: 25\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605064  [    0/ 3075]\n",
      "loss: 0.624784  [  640/ 3075]\n",
      "loss: 0.632927  [ 1280/ 3075]\n",
      "loss: 0.621320  [ 1920/ 3075]\n",
      "loss: 0.644011  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.664166 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.605916  [    0/ 3075]\n",
      "loss: 0.517818  [  640/ 3075]\n",
      "loss: 0.698579  [ 1280/ 3075]\n",
      "loss: 0.612906  [ 1920/ 3075]\n",
      "loss: 0.568326  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.664245 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.595945 \n",
      " Num Batches: 25\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.605033  [    0/ 3075]\n",
      "loss: 0.624787  [  640/ 3075]\n",
      "loss: 0.633108  [ 1280/ 3075]\n",
      "loss: 0.621319  [ 1920/ 3075]\n",
      "loss: 0.643928  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.58230%, Avg loss: 0.664222 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.605829  [    0/ 3075]\n",
      "loss: 0.517527  [  640/ 3075]\n",
      "loss: 0.698415  [ 1280/ 3075]\n",
      "loss: 0.612818  [ 1920/ 3075]\n",
      "loss: 0.568151  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.45218%, Avg loss: 0.664288 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.595853 \n",
      " Num Batches: 25\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604994  [    0/ 3075]\n",
      "loss: 0.624787  [  640/ 3075]\n",
      "loss: 0.633285  [ 1280/ 3075]\n",
      "loss: 0.621328  [ 1920/ 3075]\n",
      "loss: 0.643867  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.45218%, Avg loss: 0.664265 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.605729  [    0/ 3075]\n",
      "loss: 0.517298  [  640/ 3075]\n",
      "loss: 0.698320  [ 1280/ 3075]\n",
      "loss: 0.612645  [ 1920/ 3075]\n",
      "loss: 0.567916  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.45218%, Avg loss: 0.664362 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.595754 \n",
      " Num Batches: 25\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604976  [    0/ 3075]\n",
      "loss: 0.624801  [  640/ 3075]\n",
      "loss: 0.633477  [ 1280/ 3075]\n",
      "loss: 0.621329  [ 1920/ 3075]\n",
      "loss: 0.643780  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.45218%, Avg loss: 0.664338 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.605675  [    0/ 3075]\n",
      "loss: 0.517055  [  640/ 3075]\n",
      "loss: 0.698135  [ 1280/ 3075]\n",
      "loss: 0.612521  [ 1920/ 3075]\n",
      "loss: 0.567740  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.38712%, Avg loss: 0.664408 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.595670 \n",
      " Num Batches: 25\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604952  [    0/ 3075]\n",
      "loss: 0.624827  [  640/ 3075]\n",
      "loss: 0.633670  [ 1280/ 3075]\n",
      "loss: 0.621337  [ 1920/ 3075]\n",
      "loss: 0.643681  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.38712%, Avg loss: 0.664384 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.605578  [    0/ 3075]\n",
      "loss: 0.516843  [  640/ 3075]\n",
      "loss: 0.697990  [ 1280/ 3075]\n",
      "loss: 0.612369  [ 1920/ 3075]\n",
      "loss: 0.567529  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.38712%, Avg loss: 0.664472 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.595573 \n",
      " Num Batches: 25\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604925  [    0/ 3075]\n",
      "loss: 0.624844  [  640/ 3075]\n",
      "loss: 0.633843  [ 1280/ 3075]\n",
      "loss: 0.621352  [ 1920/ 3075]\n",
      "loss: 0.643600  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.38712%, Avg loss: 0.664447 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.605516  [    0/ 3075]\n",
      "loss: 0.516596  [  640/ 3075]\n",
      "loss: 0.697807  [ 1280/ 3075]\n",
      "loss: 0.612254  [ 1920/ 3075]\n",
      "loss: 0.567358  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.32206%, Avg loss: 0.664515 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.595486 \n",
      " Num Batches: 25\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604890  [    0/ 3075]\n",
      "loss: 0.624863  [  640/ 3075]\n",
      "loss: 0.634023  [ 1280/ 3075]\n",
      "loss: 0.621372  [ 1920/ 3075]\n",
      "loss: 0.643528  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.38712%, Avg loss: 0.664490 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.605436  [    0/ 3075]\n",
      "loss: 0.516267  [  640/ 3075]\n",
      "loss: 0.697711  [ 1280/ 3075]\n",
      "loss: 0.612175  [ 1920/ 3075]\n",
      "loss: 0.567132  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.32206%, Avg loss: 0.664567 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.595382 \n",
      " Num Batches: 25\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604842  [    0/ 3075]\n",
      "loss: 0.624863  [  640/ 3075]\n",
      "loss: 0.634187  [ 1280/ 3075]\n",
      "loss: 0.621393  [ 1920/ 3075]\n",
      "loss: 0.643495  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.32206%, Avg loss: 0.664542 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.605367  [    0/ 3075]\n",
      "loss: 0.516087  [  640/ 3075]\n",
      "loss: 0.697413  [ 1280/ 3075]\n",
      "loss: 0.611986  [ 1920/ 3075]\n",
      "loss: 0.566984  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.32206%, Avg loss: 0.664617 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.595308 \n",
      " Num Batches: 25\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604843  [    0/ 3075]\n",
      "loss: 0.624927  [  640/ 3075]\n",
      "loss: 0.634423  [ 1280/ 3075]\n",
      "loss: 0.621418  [ 1920/ 3075]\n",
      "loss: 0.643346  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.32206%, Avg loss: 0.664591 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.605244  [    0/ 3075]\n",
      "loss: 0.515854  [  640/ 3075]\n",
      "loss: 0.697195  [ 1280/ 3075]\n",
      "loss: 0.611893  [ 1920/ 3075]\n",
      "loss: 0.566804  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.25699%, Avg loss: 0.664656 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.595221 \n",
      " Num Batches: 25\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604817  [    0/ 3075]\n",
      "loss: 0.624957  [  640/ 3075]\n",
      "loss: 0.634596  [ 1280/ 3075]\n",
      "loss: 0.621442  [ 1920/ 3075]\n",
      "loss: 0.643281  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.25699%, Avg loss: 0.664630 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.605152  [    0/ 3075]\n",
      "loss: 0.515552  [  640/ 3075]\n",
      "loss: 0.697057  [ 1280/ 3075]\n",
      "loss: 0.611810  [ 1920/ 3075]\n",
      "loss: 0.566613  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.25699%, Avg loss: 0.664684 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.595136 \n",
      " Num Batches: 25\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604775  [    0/ 3075]\n",
      "loss: 0.624970  [  640/ 3075]\n",
      "loss: 0.634763  [ 1280/ 3075]\n",
      "loss: 0.621470  [ 1920/ 3075]\n",
      "loss: 0.643249  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.19193%, Avg loss: 0.664658 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.605052  [    0/ 3075]\n",
      "loss: 0.515332  [  640/ 3075]\n",
      "loss: 0.696749  [ 1280/ 3075]\n",
      "loss: 0.611674  [ 1920/ 3075]\n",
      "loss: 0.566484  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.25699%, Avg loss: 0.664713 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.595069 \n",
      " Num Batches: 25\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604769  [    0/ 3075]\n",
      "loss: 0.625031  [  640/ 3075]\n",
      "loss: 0.634965  [ 1280/ 3075]\n",
      "loss: 0.621508  [ 1920/ 3075]\n",
      "loss: 0.643147  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.25699%, Avg loss: 0.664687 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.604933  [    0/ 3075]\n",
      "loss: 0.515061  [  640/ 3075]\n",
      "loss: 0.696545  [ 1280/ 3075]\n",
      "loss: 0.611667  [ 1920/ 3075]\n",
      "loss: 0.566334  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.19193%, Avg loss: 0.664713 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.594986 \n",
      " Num Batches: 25\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604705  [    0/ 3075]\n",
      "loss: 0.625041  [  640/ 3075]\n",
      "loss: 0.635091  [ 1280/ 3075]\n",
      "loss: 0.621544  [ 1920/ 3075]\n",
      "loss: 0.643135  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.19193%, Avg loss: 0.664687 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.604787  [    0/ 3075]\n",
      "loss: 0.514724  [  640/ 3075]\n",
      "loss: 0.696303  [ 1280/ 3075]\n",
      "loss: 0.611670  [ 1920/ 3075]\n",
      "loss: 0.566223  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.19193%, Avg loss: 0.664724 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.594915 \n",
      " Num Batches: 25\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604667  [    0/ 3075]\n",
      "loss: 0.625068  [  640/ 3075]\n",
      "loss: 0.635248  [ 1280/ 3075]\n",
      "loss: 0.621586  [ 1920/ 3075]\n",
      "loss: 0.643096  [ 2560/ 3075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversary Test Error: \n",
      " Accuracy: 65.19193%, Avg loss: 0.664699 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.604660  [    0/ 3075]\n",
      "loss: 0.514453  [  640/ 3075]\n",
      "loss: 0.696026  [ 1280/ 3075]\n",
      "loss: 0.611519  [ 1920/ 3075]\n",
      "loss: 0.566049  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.19193%, Avg loss: 0.664770 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.594837 \n",
      " Num Batches: 25\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604645  [    0/ 3075]\n",
      "loss: 0.625119  [  640/ 3075]\n",
      "loss: 0.635444  [ 1280/ 3075]\n",
      "loss: 0.621608  [ 1920/ 3075]\n",
      "loss: 0.642973  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.19193%, Avg loss: 0.664745 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.604572  [    0/ 3075]\n",
      "loss: 0.514256  [  640/ 3075]\n",
      "loss: 0.695733  [ 1280/ 3075]\n",
      "loss: 0.611404  [ 1920/ 3075]\n",
      "loss: 0.565899  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.12687%, Avg loss: 0.664780 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.594778 \n",
      " Num Batches: 25\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604617  [    0/ 3075]\n",
      "loss: 0.625164  [  640/ 3075]\n",
      "loss: 0.635628  [ 1280/ 3075]\n",
      "loss: 0.621638  [ 1920/ 3075]\n",
      "loss: 0.642873  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.19193%, Avg loss: 0.664755 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.604461  [    0/ 3075]\n",
      "loss: 0.514018  [  640/ 3075]\n",
      "loss: 0.695499  [ 1280/ 3075]\n",
      "loss: 0.611306  [ 1920/ 3075]\n",
      "loss: 0.565725  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.06181%, Avg loss: 0.664812 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.594702 \n",
      " Num Batches: 25\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604583  [    0/ 3075]\n",
      "loss: 0.625197  [  640/ 3075]\n",
      "loss: 0.635824  [ 1280/ 3075]\n",
      "loss: 0.621670  [ 1920/ 3075]\n",
      "loss: 0.642791  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.12687%, Avg loss: 0.664787 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.604388  [    0/ 3075]\n",
      "loss: 0.513858  [  640/ 3075]\n",
      "loss: 0.695153  [ 1280/ 3075]\n",
      "loss: 0.611139  [ 1920/ 3075]\n",
      "loss: 0.565558  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.12687%, Avg loss: 0.664844 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.9%, Avg loss: 0.594643 \n",
      " Num Batches: 25\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604582  [    0/ 3075]\n",
      "loss: 0.625255  [  640/ 3075]\n",
      "loss: 0.636054  [ 1280/ 3075]\n",
      "loss: 0.621697  [ 1920/ 3075]\n",
      "loss: 0.642649  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.06181%, Avg loss: 0.664818 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.604307  [    0/ 3075]\n",
      "loss: 0.513722  [  640/ 3075]\n",
      "loss: 0.694816  [ 1280/ 3075]\n",
      "loss: 0.610977  [ 1920/ 3075]\n",
      "loss: 0.565406  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.06181%, Avg loss: 0.664866 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.594586 \n",
      " Num Batches: 25\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604576  [    0/ 3075]\n",
      "loss: 0.625313  [  640/ 3075]\n",
      "loss: 0.636278  [ 1280/ 3075]\n",
      "loss: 0.621723  [ 1920/ 3075]\n",
      "loss: 0.642517  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.12687%, Avg loss: 0.664840 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.604189  [    0/ 3075]\n",
      "loss: 0.513513  [  640/ 3075]\n",
      "loss: 0.694507  [ 1280/ 3075]\n",
      "loss: 0.610873  [ 1920/ 3075]\n",
      "loss: 0.565276  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.99675%, Avg loss: 0.664877 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.594520 \n",
      " Num Batches: 25\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604554  [    0/ 3075]\n",
      "loss: 0.625365  [  640/ 3075]\n",
      "loss: 0.636460  [ 1280/ 3075]\n",
      "loss: 0.621759  [ 1920/ 3075]\n",
      "loss: 0.642441  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.99675%, Avg loss: 0.664851 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.604044  [    0/ 3075]\n",
      "loss: 0.513268  [  640/ 3075]\n",
      "loss: 0.694296  [ 1280/ 3075]\n",
      "loss: 0.610804  [ 1920/ 3075]\n",
      "loss: 0.565119  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.86662%, Avg loss: 0.664909 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.594442 \n",
      " Num Batches: 25\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604533  [    0/ 3075]\n",
      "loss: 0.625419  [  640/ 3075]\n",
      "loss: 0.636629  [ 1280/ 3075]\n",
      "loss: 0.621790  [ 1920/ 3075]\n",
      "loss: 0.642363  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.93169%, Avg loss: 0.664883 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.603929  [    0/ 3075]\n",
      "loss: 0.513097  [  640/ 3075]\n",
      "loss: 0.693969  [ 1280/ 3075]\n",
      "loss: 0.610644  [ 1920/ 3075]\n",
      "loss: 0.564990  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.86662%, Avg loss: 0.664932 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.594384 \n",
      " Num Batches: 25\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604532  [    0/ 3075]\n",
      "loss: 0.625497  [  640/ 3075]\n",
      "loss: 0.636841  [ 1280/ 3075]\n",
      "loss: 0.621821  [ 1920/ 3075]\n",
      "loss: 0.642236  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.86662%, Avg loss: 0.664906 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.603799  [    0/ 3075]\n",
      "loss: 0.512861  [  640/ 3075]\n",
      "loss: 0.693755  [ 1280/ 3075]\n",
      "loss: 0.610575  [ 1920/ 3075]\n",
      "loss: 0.564820  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.80156%, Avg loss: 0.664958 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.594307 \n",
      " Num Batches: 25\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604502  [    0/ 3075]\n",
      "loss: 0.625550  [  640/ 3075]\n",
      "loss: 0.637011  [ 1280/ 3075]\n",
      "loss: 0.621857  [ 1920/ 3075]\n",
      "loss: 0.642176  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.80156%, Avg loss: 0.664932 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.603682  [    0/ 3075]\n",
      "loss: 0.512703  [  640/ 3075]\n",
      "loss: 0.693421  [ 1280/ 3075]\n",
      "loss: 0.610422  [ 1920/ 3075]\n",
      "loss: 0.564695  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.80156%, Avg loss: 0.664986 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.594250 \n",
      " Num Batches: 25\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604500  [    0/ 3075]\n",
      "loss: 0.625633  [  640/ 3075]\n",
      "loss: 0.637229  [ 1280/ 3075]\n",
      "loss: 0.621882  [ 1920/ 3075]\n",
      "loss: 0.642041  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.80156%, Avg loss: 0.664959 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.603550  [    0/ 3075]\n",
      "loss: 0.512435  [  640/ 3075]\n",
      "loss: 0.693230  [ 1280/ 3075]\n",
      "loss: 0.610358  [ 1920/ 3075]\n",
      "loss: 0.564532  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665005 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.594173 \n",
      " Num Batches: 25\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604466  [    0/ 3075]\n",
      "loss: 0.625687  [  640/ 3075]\n",
      "loss: 0.637387  [ 1280/ 3075]\n",
      "loss: 0.621929  [ 1920/ 3075]\n",
      "loss: 0.641993  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.80156%, Avg loss: 0.664979 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.603427  [    0/ 3075]\n",
      "loss: 0.512265  [  640/ 3075]\n",
      "loss: 0.692949  [ 1280/ 3075]\n",
      "loss: 0.610217  [ 1920/ 3075]\n",
      "loss: 0.564376  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665035 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.594115 \n",
      " Num Batches: 25\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604466  [    0/ 3075]\n",
      "loss: 0.625774  [  640/ 3075]\n",
      "loss: 0.637592  [ 1280/ 3075]\n",
      "loss: 0.621958  [ 1920/ 3075]\n",
      "loss: 0.641861  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665008 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.603326  [    0/ 3075]\n",
      "loss: 0.512147  [  640/ 3075]\n",
      "loss: 0.692575  [ 1280/ 3075]\n",
      "loss: 0.610082  [ 1920/ 3075]\n",
      "loss: 0.564261  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665066 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.594048 \n",
      " Num Batches: 25\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604460  [    0/ 3075]\n",
      "loss: 0.625860  [  640/ 3075]\n",
      "loss: 0.637799  [ 1280/ 3075]\n",
      "loss: 0.621996  [ 1920/ 3075]\n",
      "loss: 0.641729  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665040 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.603185  [    0/ 3075]\n",
      "loss: 0.511918  [  640/ 3075]\n",
      "loss: 0.692303  [ 1280/ 3075]\n",
      "loss: 0.609985  [ 1920/ 3075]\n",
      "loss: 0.564105  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665087 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.593981 \n",
      " Num Batches: 25\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604449  [    0/ 3075]\n",
      "loss: 0.625937  [  640/ 3075]\n",
      "loss: 0.637980  [ 1280/ 3075]\n",
      "loss: 0.622045  [ 1920/ 3075]\n",
      "loss: 0.641644  [ 2560/ 3075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665060 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.603071  [    0/ 3075]\n",
      "loss: 0.511829  [  640/ 3075]\n",
      "loss: 0.691911  [ 1280/ 3075]\n",
      "loss: 0.609820  [ 1920/ 3075]\n",
      "loss: 0.563967  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665124 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.593922 \n",
      " Num Batches: 25\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604465  [    0/ 3075]\n",
      "loss: 0.626038  [  640/ 3075]\n",
      "loss: 0.638210  [ 1280/ 3075]\n",
      "loss: 0.622082  [ 1920/ 3075]\n",
      "loss: 0.641480  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.73650%, Avg loss: 0.665096 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.602953  [    0/ 3075]\n",
      "loss: 0.511613  [  640/ 3075]\n",
      "loss: 0.691599  [ 1280/ 3075]\n",
      "loss: 0.609729  [ 1920/ 3075]\n",
      "loss: 0.563825  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665142 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.593854 \n",
      " Num Batches: 25\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604444  [    0/ 3075]\n",
      "loss: 0.626100  [  640/ 3075]\n",
      "loss: 0.638379  [ 1280/ 3075]\n",
      "loss: 0.622128  [ 1920/ 3075]\n",
      "loss: 0.641415  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665115 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.602812  [    0/ 3075]\n",
      "loss: 0.511406  [  640/ 3075]\n",
      "loss: 0.691318  [ 1280/ 3075]\n",
      "loss: 0.609629  [ 1920/ 3075]\n",
      "loss: 0.563672  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665165 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.593790 \n",
      " Num Batches: 25\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604443  [    0/ 3075]\n",
      "loss: 0.626180  [  640/ 3075]\n",
      "loss: 0.638565  [ 1280/ 3075]\n",
      "loss: 0.622173  [ 1920/ 3075]\n",
      "loss: 0.641322  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665137 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.602697  [    0/ 3075]\n",
      "loss: 0.511297  [  640/ 3075]\n",
      "loss: 0.690923  [ 1280/ 3075]\n",
      "loss: 0.609470  [ 1920/ 3075]\n",
      "loss: 0.563575  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665182 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.593745 \n",
      " Num Batches: 25\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604458  [    0/ 3075]\n",
      "loss: 0.626279  [  640/ 3075]\n",
      "loss: 0.638787  [ 1280/ 3075]\n",
      "loss: 0.622220  [ 1920/ 3075]\n",
      "loss: 0.641164  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665154 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.602555  [    0/ 3075]\n",
      "loss: 0.511111  [  640/ 3075]\n",
      "loss: 0.690659  [ 1280/ 3075]\n",
      "loss: 0.609375  [ 1920/ 3075]\n",
      "loss: 0.563425  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665202 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.593680 \n",
      " Num Batches: 25\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604453  [    0/ 3075]\n",
      "loss: 0.626353  [  640/ 3075]\n",
      "loss: 0.638949  [ 1280/ 3075]\n",
      "loss: 0.622268  [ 1920/ 3075]\n",
      "loss: 0.641078  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665174 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.602450  [    0/ 3075]\n",
      "loss: 0.510936  [  640/ 3075]\n",
      "loss: 0.690255  [ 1280/ 3075]\n",
      "loss: 0.609251  [ 1920/ 3075]\n",
      "loss: 0.563322  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665210 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.593636 \n",
      " Num Batches: 25\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604451  [    0/ 3075]\n",
      "loss: 0.626444  [  640/ 3075]\n",
      "loss: 0.639153  [ 1280/ 3075]\n",
      "loss: 0.622317  [ 1920/ 3075]\n",
      "loss: 0.640942  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665182 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.602283  [    0/ 3075]\n",
      "loss: 0.510789  [  640/ 3075]\n",
      "loss: 0.689910  [ 1280/ 3075]\n",
      "loss: 0.609118  [ 1920/ 3075]\n",
      "loss: 0.563189  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665232 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.593582 \n",
      " Num Batches: 25\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604461  [    0/ 3075]\n",
      "loss: 0.626540  [  640/ 3075]\n",
      "loss: 0.639344  [ 1280/ 3075]\n",
      "loss: 0.622366  [ 1920/ 3075]\n",
      "loss: 0.640812  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665204 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.602172  [    0/ 3075]\n",
      "loss: 0.510612  [  640/ 3075]\n",
      "loss: 0.689556  [ 1280/ 3075]\n",
      "loss: 0.609026  [ 1920/ 3075]\n",
      "loss: 0.563079  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665230 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.593530 \n",
      " Num Batches: 25\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604451  [    0/ 3075]\n",
      "loss: 0.626622  [  640/ 3075]\n",
      "loss: 0.639520  [ 1280/ 3075]\n",
      "loss: 0.622427  [ 1920/ 3075]\n",
      "loss: 0.640722  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665203 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.602024  [    0/ 3075]\n",
      "loss: 0.510433  [  640/ 3075]\n",
      "loss: 0.689202  [ 1280/ 3075]\n",
      "loss: 0.608902  [ 1920/ 3075]\n",
      "loss: 0.562941  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665255 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.593465 \n",
      " Num Batches: 25\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604439  [    0/ 3075]\n",
      "loss: 0.626707  [  640/ 3075]\n",
      "loss: 0.639701  [ 1280/ 3075]\n",
      "loss: 0.622481  [ 1920/ 3075]\n",
      "loss: 0.640612  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.67144%, Avg loss: 0.665227 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.601913  [    0/ 3075]\n",
      "loss: 0.510223  [  640/ 3075]\n",
      "loss: 0.688864  [ 1280/ 3075]\n",
      "loss: 0.608795  [ 1920/ 3075]\n",
      "loss: 0.562811  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665259 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.593407 \n",
      " Num Batches: 25\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604433  [    0/ 3075]\n",
      "loss: 0.626800  [  640/ 3075]\n",
      "loss: 0.639870  [ 1280/ 3075]\n",
      "loss: 0.622543  [ 1920/ 3075]\n",
      "loss: 0.640509  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665232 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.601788  [    0/ 3075]\n",
      "loss: 0.510093  [  640/ 3075]\n",
      "loss: 0.688464  [ 1280/ 3075]\n",
      "loss: 0.608646  [ 1920/ 3075]\n",
      "loss: 0.562677  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665264 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.593360 \n",
      " Num Batches: 25\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604430  [    0/ 3075]\n",
      "loss: 0.626896  [  640/ 3075]\n",
      "loss: 0.640059  [ 1280/ 3075]\n",
      "loss: 0.622592  [ 1920/ 3075]\n",
      "loss: 0.640382  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665236 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.601652  [    0/ 3075]\n",
      "loss: 0.509825  [  640/ 3075]\n",
      "loss: 0.688059  [ 1280/ 3075]\n",
      "loss: 0.608624  [ 1920/ 3075]\n",
      "loss: 0.562600  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665249 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.593308 \n",
      " Num Batches: 25\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604408  [    0/ 3075]\n",
      "loss: 0.626977  [  640/ 3075]\n",
      "loss: 0.640211  [ 1280/ 3075]\n",
      "loss: 0.622657  [ 1920/ 3075]\n",
      "loss: 0.640305  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665222 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.601474  [    0/ 3075]\n",
      "loss: 0.509640  [  640/ 3075]\n",
      "loss: 0.687612  [ 1280/ 3075]\n",
      "loss: 0.608494  [ 1920/ 3075]\n",
      "loss: 0.562495  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665257 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.593260 \n",
      " Num Batches: 25\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604411  [    0/ 3075]\n",
      "loss: 0.627090  [  640/ 3075]\n",
      "loss: 0.640402  [ 1280/ 3075]\n",
      "loss: 0.622720  [ 1920/ 3075]\n",
      "loss: 0.640163  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665230 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.601342  [    0/ 3075]\n",
      "loss: 0.509483  [  640/ 3075]\n",
      "loss: 0.687190  [ 1280/ 3075]\n",
      "loss: 0.608407  [ 1920/ 3075]\n",
      "loss: 0.562419  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665258 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.593212 \n",
      " Num Batches: 25\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604416  [    0/ 3075]\n",
      "loss: 0.627197  [  640/ 3075]\n",
      "loss: 0.640577  [ 1280/ 3075]\n",
      "loss: 0.622784  [ 1920/ 3075]\n",
      "loss: 0.640047  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665231 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.601212  [    0/ 3075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.509352  [  640/ 3075]\n",
      "loss: 0.686770  [ 1280/ 3075]\n",
      "loss: 0.608294  [ 1920/ 3075]\n",
      "loss: 0.562311  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665266 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.593163 \n",
      " Num Batches: 25\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604424  [    0/ 3075]\n",
      "loss: 0.627303  [  640/ 3075]\n",
      "loss: 0.640755  [ 1280/ 3075]\n",
      "loss: 0.622848  [ 1920/ 3075]\n",
      "loss: 0.639934  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665239 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.601047  [    0/ 3075]\n",
      "loss: 0.509193  [  640/ 3075]\n",
      "loss: 0.686329  [ 1280/ 3075]\n",
      "loss: 0.608207  [ 1920/ 3075]\n",
      "loss: 0.562227  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665265 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.593120 \n",
      " Num Batches: 25\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604434  [    0/ 3075]\n",
      "loss: 0.627408  [  640/ 3075]\n",
      "loss: 0.640937  [ 1280/ 3075]\n",
      "loss: 0.622910  [ 1920/ 3075]\n",
      "loss: 0.639816  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665240 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.600871  [    0/ 3075]\n",
      "loss: 0.509073  [  640/ 3075]\n",
      "loss: 0.685927  [ 1280/ 3075]\n",
      "loss: 0.608092  [ 1920/ 3075]\n",
      "loss: 0.562113  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.41119%, Avg loss: 0.665290 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.593067 \n",
      " Num Batches: 25\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604456  [    0/ 3075]\n",
      "loss: 0.627521  [  640/ 3075]\n",
      "loss: 0.641131  [ 1280/ 3075]\n",
      "loss: 0.622967  [ 1920/ 3075]\n",
      "loss: 0.639679  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.41119%, Avg loss: 0.665263 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.600736  [    0/ 3075]\n",
      "loss: 0.508921  [  640/ 3075]\n",
      "loss: 0.685520  [ 1280/ 3075]\n",
      "loss: 0.607982  [ 1920/ 3075]\n",
      "loss: 0.562011  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.41119%, Avg loss: 0.665288 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.593025 \n",
      " Num Batches: 25\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604465  [    0/ 3075]\n",
      "loss: 0.627623  [  640/ 3075]\n",
      "loss: 0.641318  [ 1280/ 3075]\n",
      "loss: 0.623032  [ 1920/ 3075]\n",
      "loss: 0.639568  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.41119%, Avg loss: 0.665262 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.600571  [    0/ 3075]\n",
      "loss: 0.508772  [  640/ 3075]\n",
      "loss: 0.685104  [ 1280/ 3075]\n",
      "loss: 0.607846  [ 1920/ 3075]\n",
      "loss: 0.561900  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.41119%, Avg loss: 0.665292 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.592989 \n",
      " Num Batches: 25\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604482  [    0/ 3075]\n",
      "loss: 0.627742  [  640/ 3075]\n",
      "loss: 0.641506  [ 1280/ 3075]\n",
      "loss: 0.623085  [ 1920/ 3075]\n",
      "loss: 0.639423  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665266 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.600412  [    0/ 3075]\n",
      "loss: 0.508664  [  640/ 3075]\n",
      "loss: 0.684709  [ 1280/ 3075]\n",
      "loss: 0.607721  [ 1920/ 3075]\n",
      "loss: 0.561817  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665296 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.592955 \n",
      " Num Batches: 25\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604509  [    0/ 3075]\n",
      "loss: 0.627851  [  640/ 3075]\n",
      "loss: 0.641684  [ 1280/ 3075]\n",
      "loss: 0.623142  [ 1920/ 3075]\n",
      "loss: 0.639292  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665270 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.600275  [    0/ 3075]\n",
      "loss: 0.508517  [  640/ 3075]\n",
      "loss: 0.684360  [ 1280/ 3075]\n",
      "loss: 0.607604  [ 1920/ 3075]\n",
      "loss: 0.561680  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665316 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.592905 \n",
      " Num Batches: 25\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604524  [    0/ 3075]\n",
      "loss: 0.627960  [  640/ 3075]\n",
      "loss: 0.641862  [ 1280/ 3075]\n",
      "loss: 0.623203  [ 1920/ 3075]\n",
      "loss: 0.639176  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665290 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.600148  [    0/ 3075]\n",
      "loss: 0.508386  [  640/ 3075]\n",
      "loss: 0.683976  [ 1280/ 3075]\n",
      "loss: 0.607467  [ 1920/ 3075]\n",
      "loss: 0.561572  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665316 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.592874 \n",
      " Num Batches: 25\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604545  [    0/ 3075]\n",
      "loss: 0.628073  [  640/ 3075]\n",
      "loss: 0.642043  [ 1280/ 3075]\n",
      "loss: 0.623260  [ 1920/ 3075]\n",
      "loss: 0.639052  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665290 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.600004  [    0/ 3075]\n",
      "loss: 0.508315  [  640/ 3075]\n",
      "loss: 0.683545  [ 1280/ 3075]\n",
      "loss: 0.607312  [ 1920/ 3075]\n",
      "loss: 0.561481  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665331 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.592839 \n",
      " Num Batches: 25\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604578  [    0/ 3075]\n",
      "loss: 0.628195  [  640/ 3075]\n",
      "loss: 0.642240  [ 1280/ 3075]\n",
      "loss: 0.623323  [ 1920/ 3075]\n",
      "loss: 0.638901  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665304 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.599863  [    0/ 3075]\n",
      "loss: 0.508121  [  640/ 3075]\n",
      "loss: 0.683210  [ 1280/ 3075]\n",
      "loss: 0.607241  [ 1920/ 3075]\n",
      "loss: 0.561385  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665322 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.592796 \n",
      " Num Batches: 25\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604581  [    0/ 3075]\n",
      "loss: 0.628288  [  640/ 3075]\n",
      "loss: 0.642379  [ 1280/ 3075]\n",
      "loss: 0.623388  [ 1920/ 3075]\n",
      "loss: 0.638828  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665295 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.599704  [    0/ 3075]\n",
      "loss: 0.507969  [  640/ 3075]\n",
      "loss: 0.682782  [ 1280/ 3075]\n",
      "loss: 0.607106  [ 1920/ 3075]\n",
      "loss: 0.561322  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.60638%, Avg loss: 0.665318 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.592768 \n",
      " Num Batches: 25\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604619  [    0/ 3075]\n",
      "loss: 0.628407  [  640/ 3075]\n",
      "loss: 0.642566  [ 1280/ 3075]\n",
      "loss: 0.623452  [ 1920/ 3075]\n",
      "loss: 0.638694  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665292 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.599535  [    0/ 3075]\n",
      "loss: 0.507844  [  640/ 3075]\n",
      "loss: 0.682380  [ 1280/ 3075]\n",
      "loss: 0.606986  [ 1920/ 3075]\n",
      "loss: 0.561246  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665324 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.592736 \n",
      " Num Batches: 25\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604653  [    0/ 3075]\n",
      "loss: 0.628525  [  640/ 3075]\n",
      "loss: 0.642745  [ 1280/ 3075]\n",
      "loss: 0.623514  [ 1920/ 3075]\n",
      "loss: 0.638557  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665297 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.599387  [    0/ 3075]\n",
      "loss: 0.507750  [  640/ 3075]\n",
      "loss: 0.682023  [ 1280/ 3075]\n",
      "loss: 0.606825  [ 1920/ 3075]\n",
      "loss: 0.561116  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665327 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.592711 \n",
      " Num Batches: 25\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604679  [    0/ 3075]\n",
      "loss: 0.628638  [  640/ 3075]\n",
      "loss: 0.642920  [ 1280/ 3075]\n",
      "loss: 0.623571  [ 1920/ 3075]\n",
      "loss: 0.638435  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665300 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.599241  [    0/ 3075]\n",
      "loss: 0.507574  [  640/ 3075]\n",
      "loss: 0.681650  [ 1280/ 3075]\n",
      "loss: 0.606748  [ 1920/ 3075]\n",
      "loss: 0.561040  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665312 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.592683 \n",
      " Num Batches: 25\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604694  [    0/ 3075]\n",
      "loss: 0.628741  [  640/ 3075]\n",
      "loss: 0.643074  [ 1280/ 3075]\n",
      "loss: 0.623637  [ 1920/ 3075]\n",
      "loss: 0.638337  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665285 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.599076  [    0/ 3075]\n",
      "loss: 0.507453  [  640/ 3075]\n",
      "loss: 0.681275  [ 1280/ 3075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.606588  [ 1920/ 3075]\n",
      "loss: 0.560924  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665317 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.592655 \n",
      " Num Batches: 25\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604720  [    0/ 3075]\n",
      "loss: 0.628860  [  640/ 3075]\n",
      "loss: 0.643251  [ 1280/ 3075]\n",
      "loss: 0.623702  [ 1920/ 3075]\n",
      "loss: 0.638207  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665291 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.598933  [    0/ 3075]\n",
      "loss: 0.507363  [  640/ 3075]\n",
      "loss: 0.680879  [ 1280/ 3075]\n",
      "loss: 0.606452  [ 1920/ 3075]\n",
      "loss: 0.560832  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665328 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.592623 \n",
      " Num Batches: 25\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604757  [    0/ 3075]\n",
      "loss: 0.628977  [  640/ 3075]\n",
      "loss: 0.643424  [ 1280/ 3075]\n",
      "loss: 0.623767  [ 1920/ 3075]\n",
      "loss: 0.638084  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665301 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.598796  [    0/ 3075]\n",
      "loss: 0.507212  [  640/ 3075]\n",
      "loss: 0.680561  [ 1280/ 3075]\n",
      "loss: 0.606350  [ 1920/ 3075]\n",
      "loss: 0.560704  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665326 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.592590 \n",
      " Num Batches: 25\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604774  [    0/ 3075]\n",
      "loss: 0.629083  [  640/ 3075]\n",
      "loss: 0.643578  [ 1280/ 3075]\n",
      "loss: 0.623834  [ 1920/ 3075]\n",
      "loss: 0.637986  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665300 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.598647  [    0/ 3075]\n",
      "loss: 0.507084  [  640/ 3075]\n",
      "loss: 0.680196  [ 1280/ 3075]\n",
      "loss: 0.606234  [ 1920/ 3075]\n",
      "loss: 0.560594  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665327 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.592560 \n",
      " Num Batches: 25\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604797  [    0/ 3075]\n",
      "loss: 0.629196  [  640/ 3075]\n",
      "loss: 0.643740  [ 1280/ 3075]\n",
      "loss: 0.623899  [ 1920/ 3075]\n",
      "loss: 0.637874  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665301 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.598497  [    0/ 3075]\n",
      "loss: 0.506894  [  640/ 3075]\n",
      "loss: 0.679797  [ 1280/ 3075]\n",
      "loss: 0.606176  [ 1920/ 3075]\n",
      "loss: 0.560508  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665305 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.592534 \n",
      " Num Batches: 25\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604813  [    0/ 3075]\n",
      "loss: 0.629315  [  640/ 3075]\n",
      "loss: 0.643896  [ 1280/ 3075]\n",
      "loss: 0.623983  [ 1920/ 3075]\n",
      "loss: 0.637764  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665279 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.598317  [    0/ 3075]\n",
      "loss: 0.506872  [  640/ 3075]\n",
      "loss: 0.679395  [ 1280/ 3075]\n",
      "loss: 0.606014  [ 1920/ 3075]\n",
      "loss: 0.560359  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665309 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.592510 \n",
      " Num Batches: 25\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604853  [    0/ 3075]\n",
      "loss: 0.629443  [  640/ 3075]\n",
      "loss: 0.644067  [ 1280/ 3075]\n",
      "loss: 0.624059  [ 1920/ 3075]\n",
      "loss: 0.637640  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665283 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.598173  [    0/ 3075]\n",
      "loss: 0.506764  [  640/ 3075]\n",
      "loss: 0.678989  [ 1280/ 3075]\n",
      "loss: 0.605914  [ 1920/ 3075]\n",
      "loss: 0.560278  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665291 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.592491 \n",
      " Num Batches: 25\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604882  [    0/ 3075]\n",
      "loss: 0.629565  [  640/ 3075]\n",
      "loss: 0.644219  [ 1280/ 3075]\n",
      "loss: 0.624138  [ 1920/ 3075]\n",
      "loss: 0.637522  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665265 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.597990  [    0/ 3075]\n",
      "loss: 0.506613  [  640/ 3075]\n",
      "loss: 0.678625  [ 1280/ 3075]\n",
      "loss: 0.605820  [ 1920/ 3075]\n",
      "loss: 0.560161  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665296 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.592455 \n",
      " Num Batches: 25\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604910  [    0/ 3075]\n",
      "loss: 0.629671  [  640/ 3075]\n",
      "loss: 0.644369  [ 1280/ 3075]\n",
      "loss: 0.624220  [ 1920/ 3075]\n",
      "loss: 0.637428  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.54131%, Avg loss: 0.665270 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.597845  [    0/ 3075]\n",
      "loss: 0.506492  [  640/ 3075]\n",
      "loss: 0.678257  [ 1280/ 3075]\n",
      "loss: 0.605703  [ 1920/ 3075]\n",
      "loss: 0.560053  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665288 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.592433 \n",
      " Num Batches: 25\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604937  [    0/ 3075]\n",
      "loss: 0.629778  [  640/ 3075]\n",
      "loss: 0.644518  [ 1280/ 3075]\n",
      "loss: 0.624301  [ 1920/ 3075]\n",
      "loss: 0.637318  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665262 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.597691  [    0/ 3075]\n",
      "loss: 0.506426  [  640/ 3075]\n",
      "loss: 0.677781  [ 1280/ 3075]\n",
      "loss: 0.605578  [ 1920/ 3075]\n",
      "loss: 0.560007  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665267 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.4%, Avg loss: 0.592428 \n",
      " Num Batches: 25\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.604970  [    0/ 3075]\n",
      "loss: 0.629900  [  640/ 3075]\n",
      "loss: 0.644683  [ 1280/ 3075]\n",
      "loss: 0.624391  [ 1920/ 3075]\n",
      "loss: 0.637180  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665241 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.597520  [    0/ 3075]\n",
      "loss: 0.506307  [  640/ 3075]\n",
      "loss: 0.677447  [ 1280/ 3075]\n",
      "loss: 0.605501  [ 1920/ 3075]\n",
      "loss: 0.559885  [ 2560/ 3075]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.47625%, Avg loss: 0.665265 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.592395 \n",
      " Num Batches: 25\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "lrate = 0.00001\n",
    "predictor_optimizerf = torch.optim.Adam(predictorf.parameters(), lr=lrate)\n",
    "adversary_optimizerf = torch.optim.Adam(adversaryf.parameters(), lr=lrate)\n",
    "\n",
    "epochs = 180\n",
    "train_loss_per_epoch = []\n",
    "val_loss_per_epoch = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, predictorf, adversaryf, loss_fn, predictor_optimizerf, adversary_optimizerf, 1)\n",
    "    train_loss_per_epoch.append(inference(train_dataloader, predictorf, loss_fn)[1])\n",
    "    val_loss_per_epoch.append(inference(val_dataloader, predictorf, loss_fn)[1])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Epoch'),\n",
       " Text(0, 0.5, 'Loss'),\n",
       " Text(0.5, 1.0, 'Validation Loss per Epoch')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 5)\n",
    "fig.suptitle(\"Train and Val Error per Epoch\")\n",
    "ax1.plot(list(range(1,epochs+1)), train_loss_per_epoch)\n",
    "ax1.set(xlabel='Epoch', ylabel='Loss', title=\"Train Loss per Epoch\")\n",
    "ax2.plot(list(range(1,epochs+1)), val_loss_per_epoch)\n",
    "ax2.set(xlabel='Epoch', ylabel='Loss', title=\"Validation Loss per Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice looking curves! Unfortunately, not so nice with respect to false positive parity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.05\n",
      "0.1\n",
      "0.15\n",
      "0.75\n",
      "0.8\n",
      "0.85\n",
      "0.9\n",
      "0.95\n",
      "1.0\n",
      "[(0.0, 0.0), (0.05, 0.0), (0.1, 0.002873563218390829), (0.15, 0.04777951933124347), (0.2, 0.10624346917450367), (0.25, 0.21047544409613383), (0.3, 0.2478056426332288), (0.35, 0.22450365726227794), (0.4, 0.19605538140020898), (0.45, 0.12277951933124345), (0.5, 0.12055903866248693), (0.55, 0.11880877742946708), (0.6, 0.12087251828631138), (0.65, 0.10856844305120167), (0.7, 0.07460815047021943), (0.75, 0.03610240334378265), (0.8, 0.023798328108672936), (0.85, 0.013636363636363636), (0.9, 0.006818181818181818), (0.95, 0.0), (1.0, 0.0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d43a2babb0>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fpvcalc(threshold, predictor):\n",
    "    black_highrisk = 0\n",
    "    black_highrisk_recidivate = 0\n",
    "\n",
    "    white_highrisk = 0\n",
    "    white_highrisk_recidivate = 0\n",
    "\n",
    "    black_lowrisk = 0\n",
    "    black_lowrisk_recidivate = 0\n",
    "\n",
    "    white_lowrisk = 0\n",
    "    white_lowrisk_recidivate = 0\n",
    "\n",
    "    black_notrecidivate = 0\n",
    "    black_highrisk_notrecidivate = 0\n",
    "\n",
    "    white_notrecidivate = 0\n",
    "    white_highrisk_notrecidivate = 0\n",
    "\n",
    "    for _, row in test_df.iterrows():   \n",
    "        x = [row[x] for x in features]\n",
    "        x = torch.tensor(x).type(torch.FloatTensor)\n",
    "        pred = 1 if predictor(x).item() >= threshold else 0\n",
    "        if row['race'] == 'African-American' and pred == 1:\n",
    "            black_highrisk += 1\n",
    "            if row['two_year_recid'] == 1:\n",
    "                black_highrisk_recidivate += 1\n",
    "            else:\n",
    "                black_notrecidivate += 1\n",
    "                black_highrisk_notrecidivate += 1\n",
    "        elif row['race'] != 'African-American' and pred == 1:\n",
    "            white_highrisk += 1\n",
    "            if row['two_year_recid'] == 1:\n",
    "                white_highrisk_recidivate += 1\n",
    "            else:\n",
    "                white_notrecidivate += 1\n",
    "                white_highrisk_notrecidivate += 1\n",
    "        elif row['race'] == 'African-American' and pred == 0:\n",
    "            black_lowrisk += 1\n",
    "            if row['two_year_recid'] == 1:\n",
    "                black_lowrisk_recidivate += 1\n",
    "            else:\n",
    "                black_notrecidivate += 1\n",
    "        else:\n",
    "            white_lowrisk += 1\n",
    "            if row['two_year_recid'] == 1:\n",
    "                white_lowrisk_recidivate += 1\n",
    "            else:\n",
    "                white_notrecidivate += 1\n",
    "    return black_highrisk_notrecidivate/black_notrecidivate, white_highrisk_notrecidivate/white_notrecidivate\n",
    "            \n",
    "thresholds = [x / 20.0 for x in range(21)]\n",
    "fp_black = []\n",
    "fp_white = []\n",
    "for threshold in thresholds:\n",
    "    a,b = fpvcalc(threshold, predictorf)\n",
    "    fp_black.append(a)\n",
    "    fp_white.append(b)\n",
    "    if a-b < 0.05:\n",
    "        print(threshold)\n",
    "print([(i/20, fp_black[i]-fp_white[i]) for i in range(len(fp_white))])\n",
    "\n",
    "fig\n",
    "plt.plot(thresholds, fp_black)\n",
    "plt.plot(thresholds, fp_white)\n",
    "\n",
    "# print(\"\\nFalse Postive Parity\")\n",
    "# print(f\"P(highrisk | black, did not recidivate) = {black_highrisk_notrecidivate/black_notrecidivate}\")\n",
    "# print(f\"P(highrisk | white, did not recidivate) = {white_highrisk_notrecidivate/white_notrecidivate}\")\n",
    "\n",
    "# Difference in base rate = difference between proportion that recidivate in the two groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, false positive parity is not met at threshold of 0.5, or anywhere in the interval [0.2, 0.7]. So, we consider this attempt a fail. We continue playing with hyperparameters until we obtain false positive parity at threshold of 0.5. This is shown in the second attempt below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2\n",
    "The only difference here is using a larger learning rate, and training for smaller number of epochs. However, this model actually achieves false positive parity at threshold = 0.5, unlike attempt 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pandas for feature extraction\n",
    "- numpy for matrix/vector operations\n",
    "- train_test_split for splitting dataset to train, validation and test sets\n",
    "- pytorch for ML algorithm, including Dataloader module to go through train set in batches\n",
    "- matplotlib for plotting train and test loss per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, read dataset. Then, filter out any row where race is not either African American or Caucasian. Finally, extract only the features we are interested in. \n",
    "- Input features of network: 'sex', 'age', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count', 'c_charge_degree', 'c_charge_desc' \n",
    "- Output of network: two_year_recid prediction\n",
    "- Input of adversary: network logits (before applying sigmoid function)\n",
    "- Output of adversary: race prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, I continued adding variables/features until I observed false parity rate reach within 5%. This was done starting with just two variables, sex and age, and then adding in the order shown below. At the end, it was necessary to convert chargee descriptions to one-hot encodings in order to get the false positive rate within the desired range. The hyperparameters were kept as was in the paper because it would provide an interesting analysis to try and find out what varaibles they used / what was most important, and a way to verify that the features that were claimed to be most important in the paper really were the most indicative of recidivism.\n",
    "\n",
    "Our results support the claim of the paper. Our features are from their top 10 features list, and they provide a good accuracy (at least, doesnt harm it) while improving false postive at the cost of calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset from .csv file, extract data related strictly to white/black personnel\n",
    "recidivism_df = pd.read_csv('compas-scores-two-years.csv')\n",
    "recidivism_df = recidivism_df[(recidivism_df[\"race\"] == \"Caucasian\") | (recidivism_df[\"race\"] == \"African-American\")]\n",
    "\n",
    "recidivism_df = recidivism_df[['sex', 'age', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count', 'c_charge_degree', 'c_charge_desc', 'race', 'two_year_recid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert binary categorical features to one-hot integer representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to convert binary categorical variables to their one-hot vector representation. We can simply convert the two categories to 0/1 integer, so it can be used when training the network. Here, we convert male/female under the sex column to 1/0 and Misdemeanor/Felony under the c_charge_degree column to 1/0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recidivism_df['sex'].replace(['Female','Male'], [0,1], inplace=True) # Encode sex variable\n",
    "recidivism_df['c_charge_degree'].replace(['F','M'], [0,1], inplace=True) # Encode charge degree variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output head as quick visual verification that our encoding above was successful. Both sex and c_charge_degree columns must now have 2 possible values- 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>juv_fel_count</th>\n",
       "      <th>juv_misd_count</th>\n",
       "      <th>juv_other_count</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>c_charge_degree</th>\n",
       "      <th>c_charge_desc</th>\n",
       "      <th>race</th>\n",
       "      <th>two_year_recid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Felony Battery w/Prior Convict</td>\n",
       "      <td>African-American</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Possession of Cocaine</td>\n",
       "      <td>African-American</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Possession of Cannabis</td>\n",
       "      <td>African-American</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Possession Burglary Tools</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Battery</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  age  juv_fel_count  juv_misd_count  juv_other_count  priors_count  \\\n",
       "1    1   34              0               0                0             0   \n",
       "2    1   24              0               0                1             4   \n",
       "3    1   23              0               1                0             1   \n",
       "6    1   41              0               0                0            14   \n",
       "8    0   39              0               0                0             0   \n",
       "\n",
       "   c_charge_degree                   c_charge_desc              race  \\\n",
       "1                0  Felony Battery w/Prior Convict  African-American   \n",
       "2                0           Possession of Cocaine  African-American   \n",
       "3                0          Possession of Cannabis  African-American   \n",
       "6                0       Possession Burglary Tools         Caucasian   \n",
       "8                1                         Battery         Caucasian   \n",
       "\n",
       "   two_year_recid  \n",
       "1               1  \n",
       "2               1  \n",
       "3               0  \n",
       "6               1  \n",
       "8               0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recidivism_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert c_charge_desc to a form that could be passed as part of network input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use one hot vectors. There is a total of 410 unique charge descriptions. These will be converted to one-hot vectors because there is no known correlation between charges with closer integer mappings. Then they will be concatenated with the rest of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(recidivism_df['c_charge_desc']) # convert to one hot encodings\n",
    "recidivism_df = recidivism_df.drop('c_charge_desc',axis = 1) # get rid of chrage description column, and ...\n",
    "recidivism_df = recidivism_df.join(one_hot) # replace it with one hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "410"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_hot.columns) # Verify we have done so correctly. This should give 410."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct train, validation and test datasets. Then break each one to input and output tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use scikit-learn's train_test_split function to construct train, validation and test datasets. We use a 50%-25%-25% split ratio for train, validation, test data, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, validation and test datasets\n",
    "train_df, test_val_df = train_test_split(recidivism_df, test_size=0.4) # 60% for train\n",
    "val_df, test_df = train_test_split(test_val_df, test_size=0.5) # 50% of 40% = 20% for each of val, test\n",
    "\n",
    "# train features\n",
    "features = list(recidivism_df.columns)\n",
    "features.remove('race')\n",
    "features.remove('two_year_recid')\n",
    "\n",
    "# number of inputs\n",
    "num_features = len(features)\n",
    "\n",
    "# construct input and output for each of the 3 datasets\n",
    "predictor_X_train = torch.tensor(train_df[features].to_numpy()).type(torch.FloatTensor)\n",
    "predictor_y_train = torch.tensor(train_df['two_year_recid'].to_numpy()).type(torch.FloatTensor)\n",
    "\n",
    "predictor_X_val = torch.tensor(val_df[features].to_numpy()).type(torch.FloatTensor)\n",
    "predictor_y_val = torch.tensor(val_df['two_year_recid'].to_numpy()).type(torch.FloatTensor)\n",
    "\n",
    "predictor_X_test = torch.tensor(test_df[features].to_numpy()).type(torch.FloatTensor)\n",
    "predictor_y_test = torch.tensor(test_df['two_year_recid'].to_numpy()).type(torch.FloatTensor)\n",
    "\n",
    "# Input for adversary will come from output of predictor, so only define outputs\n",
    "adversary_y_train = torch.tensor([1 if race == 'African-American' else 0 for race in list(train_df['race'])]).type(torch.FloatTensor)\n",
    "adversary_y_val = torch.tensor([1 if race == 'African-American' else 0 for race in list(val_df['race'])]).type(torch.FloatTensor)\n",
    "adversary_y_test = torch.tensor([1 if race == 'African-American' else 0 for race in list(test_df['race'])]).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct train, validation and test data arrays. Each array's elements are lists, containing ith train data, ith prediction and ith entry's race."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arrays constructed here will be useful for constructing DataLoader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_train_data, predictor_val_data, predictor_test_data = [], [], []\n",
    "\n",
    "# Construct train data\n",
    "for i in range(len(predictor_X_train)):\n",
    "    predictor_train_data.append([predictor_X_train[i], predictor_y_train[i], adversary_y_train[i]])\n",
    "    \n",
    "# Construct validation data\n",
    "for i in range(len(predictor_X_val)):\n",
    "    predictor_val_data.append([predictor_X_val[i], predictor_y_val[i], adversary_y_val[i]])\n",
    "    \n",
    "# Construct test data\n",
    "for i in range(len(predictor_X_test)):\n",
    "    predictor_test_data.append([predictor_X_test[i], predictor_y_test[i], adversary_y_test[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataLoader objects for each dataset. Set batch size to 64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be useful in training with minibatch size of 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set batch size to 64, standard size for ML procedures\n",
    "batch_size = 64\n",
    "\n",
    "# Create train, val and test dataloaders. Will be used later in training\n",
    "train_dataloader = DataLoader(predictor_train_data, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(predictor_val_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(predictor_test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define architecture for network and adversary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had gpu, would set device = \"gpu\" for faster training.\n",
    "\n",
    "We use the same network architectures as defined in the paper \"Achieving Fairness through Adversarial Learning: an Application to Recidivism Prediction\" \n",
    "\n",
    "Specifically:\\\n",
    "\"Predictor N has 2 256-unit ReLU hidden layers.\\\n",
    "Adversary A has a single 100-unit ReLU hidden layer. We\\\n",
    "used a learning rate of e−4, binary cross entropy loss, a\\\n",
    "sigmoid output layer, an Adam optimizer, and an alpha value of 1.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" # Too poor for GPU\n",
    "\n",
    "# Predictor architecture\n",
    "class RecidivismPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RecidivismPredictor, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        logits_post_sigmoid = torch.sigmoid(logits)\n",
    "        return logits_post_sigmoid\n",
    "\n",
    "# Adversary architecture\n",
    "class Adversary(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Adversary, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, output_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        logits_post_sigmoid = torch.sigmoid(logits)\n",
    "        return logits_post_sigmoid\n",
    "\n",
    "# Create predictor and adversary models (initialized with random weights)\n",
    "predictor = RecidivismPredictor(num_features,1).to(device)\n",
    "adversary = Adversary(1,1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define train, inference and adversary test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training, we want to iteratively train our network such that we improve adversary, then network, and repeat in this manner until model loss on the validation set stops decreasing. This way, we achieve a model with high accuracy on validation set, without overfitting to the train set. \n",
    "\n",
    "The inference function gives us the accuracy and loss of our predictor model on any dataloader. It can be used to determine model accuracy on train set, validation set, or test set.\n",
    "\n",
    "The test_adversary function allows us to test adversary. As required, this is done by passing input through the predictor to obtain the logits, then passing logits to adversary to obtain race prediction. The goal is that adversary performs poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train(dataloader, predictor, adversary, loss_fn, predictor_optimizer, adversary_optimizer, alpha):\n",
    "    size = len(dataloader.dataset)\n",
    "    predictor.train()\n",
    "    adversary.train()\n",
    "    \n",
    "    # 1. Adversary\n",
    "    print(\"IMPROVING ADVERSARY\")\n",
    "    for batch, (X, y, race) in enumerate(dataloader): # loop through batches\n",
    "        X, y, race = X.to(device), y.to(device), race.to(device)\n",
    "        \n",
    "        # Forward prop\n",
    "        pred = predictor.linear_relu_stack(X) # obtain logit \n",
    "        race_pred = adversary(pred) # from logit get race prediction through adversary\n",
    "        adversary_loss = loss_fn(torch.squeeze(race_pred).type(torch.FloatTensor), race.type(torch.FloatTensor)) # compute loss\n",
    "        \n",
    "        # Backprop: Update adversarial net parameters\n",
    "        adversary_optimizer.zero_grad()\n",
    "        adversary_loss.backward()\n",
    "        adversary_optimizer.step()        \n",
    "        \n",
    "        # Print loss after every 10 batches\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = adversary_loss.item(), batch * 64\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    test_adversary(val_dataloader, predictor, adversary, loss_fn) # should get better\n",
    "    \n",
    "    # 2. Network \n",
    "    print(\"IMPROVING NETWORK\")\n",
    "    for batch, (X, y, race) in enumerate(dataloader): # loop through batches\n",
    "        X, y, race = X.to(device), y.to(device), race.to(device)\n",
    "        \n",
    "        # Forward prop\n",
    "        logit = predictor.linear_relu_stack(X) # obtain logit\n",
    "        pred = predictor(X) # recidivism prediction\n",
    "        race_pred = adversary(logit) # from logit get race prediction through adversary\n",
    "        \n",
    "        # Need both losses\n",
    "        adversary_loss = loss_fn(torch.squeeze(race_pred).type(torch.FloatTensor), race.type(torch.FloatTensor))        \n",
    "        predictor_loss = loss_fn(torch.squeeze(pred).type(torch.FloatTensor), y.type(torch.FloatTensor))\n",
    "        # Use formula given in paper, with same hyperparameters\n",
    "        network_loss = predictor_loss - alpha*adversary_loss\n",
    "        \n",
    "        # Backprop to update networks parameters\n",
    "        predictor_optimizer.zero_grad()\n",
    "        network_loss.backward()\n",
    "        predictor_optimizer.step()\n",
    "        \n",
    "        # Print predictor loss every 10 batches\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = predictor_loss.item(), batch * 64\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    test_adversary(val_dataloader, predictor, adversary, loss_fn) # should have gotten worse\n",
    "    \n",
    "    # Validation Set\n",
    "    c, tl, n = inference(val_dataloader, predictor, loss_fn)\n",
    "    # keep track of validation loss to monitor overfit\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100*c):>0.1f}%, Avg loss: {tl:>8f} \\n Num Batches: {n}\") \n",
    "\n",
    "# Predictor inference method\n",
    "def inference(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        # loop through batches\n",
    "        for X, y, race in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # Obtain prediction, then calculate loss and accuracy\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(torch.squeeze(pred).type(torch.FloatTensor), y.type(torch.FloatTensor)).item()\n",
    "            correct += np.sum(torch.squeeze(pred).round().numpy() == y.numpy())  \n",
    "            \n",
    "    # Determine overall loss and accuracy\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    return correct, test_loss, num_batches\n",
    "\n",
    "# How good can we predict race? Hopefully not too well...\n",
    "def test_adversary(dataloader, predictor, adversary, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    # Need both networks to predict race\n",
    "    predictor.eval()\n",
    "    adversary.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # loop through batches\n",
    "        for X, y, race in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # Obtain logit form predictor, then run through adversary\n",
    "            logit = predictor.linear_relu_stack(X)\n",
    "            race_pred = adversary(logit)\n",
    "            \n",
    "            # The usual thing...\n",
    "            test_loss += loss_fn(torch.squeeze(race_pred).type(torch.FloatTensor), race.type(torch.FloatTensor))\n",
    "            correct += np.sum(torch.squeeze(race_pred).round().numpy() == race.numpy())  \n",
    "    \n",
    "    # Compute overall loss and accuracy\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Adversary Test Error: \\n Accuracy: {(100*correct):>0.5f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively train adversary and predictor network. Use the following hyperparameter settings, mostly determined by the paper \"Achieving Fairness through Adversarial Learning: an Application to Recidivism Prediction\".\n",
    "- loss_fn = binary cross entropy loss (from paper)\n",
    "- learning rate = 1e-3 (tuned to particular dataset, e-3 was found to give better results than e-4)\n",
    "- adam optimizer (from paper)\n",
    "- 20 epochs (trained for more than 20 epochs and saw increase in validation set error after 25 epochs. So, changed to 25 epochs in next iteration)\n",
    "- alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.698834  [    0/ 3690]\n",
      "loss: 0.658693  [  640/ 3690]\n",
      "loss: 0.686142  [ 1280/ 3690]\n",
      "loss: 0.637115  [ 1920/ 3690]\n",
      "loss: 0.714719  [ 2560/ 3690]\n",
      "loss: 0.690223  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 60.08130%, Avg loss: 0.672179 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.671511  [    0/ 3690]\n",
      "loss: 0.681898  [  640/ 3690]\n",
      "loss: 0.695412  [ 1280/ 3690]\n",
      "loss: 0.633168  [ 1920/ 3690]\n",
      "loss: 0.609777  [ 2560/ 3690]\n",
      "loss: 0.590117  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 65.12195%, Avg loss: 0.643609 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.632027 \n",
      " Num Batches: 20\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.607986  [    0/ 3690]\n",
      "loss: 0.604622  [  640/ 3690]\n",
      "loss: 0.696407  [ 1280/ 3690]\n",
      "loss: 0.640363  [ 1920/ 3690]\n",
      "loss: 0.665490  [ 2560/ 3690]\n",
      "loss: 0.693406  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 66.09756%, Avg loss: 0.625896 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.702622  [    0/ 3690]\n",
      "loss: 0.545914  [  640/ 3690]\n",
      "loss: 0.757793  [ 1280/ 3690]\n",
      "loss: 0.743167  [ 1920/ 3690]\n",
      "loss: 0.649609  [ 2560/ 3690]\n",
      "loss: 0.675347  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 54.14634%, Avg loss: 0.775894 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.780530 \n",
      " Num Batches: 20\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.856437  [    0/ 3690]\n",
      "loss: 0.640935  [  640/ 3690]\n",
      "loss: 0.719256  [ 1280/ 3690]\n",
      "loss: 0.633762  [ 1920/ 3690]\n",
      "loss: 0.680513  [ 2560/ 3690]\n",
      "loss: 0.699642  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.87805%, Avg loss: 0.625066 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.866893  [    0/ 3690]\n",
      "loss: 0.670818  [  640/ 3690]\n",
      "loss: 0.698806  [ 1280/ 3690]\n",
      "loss: 0.633167  [ 1920/ 3690]\n",
      "loss: 0.631635  [ 2560/ 3690]\n",
      "loss: 0.601637  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 60.08130%, Avg loss: 0.702285 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.642984 \n",
      " Num Batches: 20\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.587794  [    0/ 3690]\n",
      "loss: 0.614581  [  640/ 3690]\n",
      "loss: 0.710828  [ 1280/ 3690]\n",
      "loss: 0.633470  [ 1920/ 3690]\n",
      "loss: 0.689715  [ 2560/ 3690]\n",
      "loss: 0.689169  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.86992%, Avg loss: 0.646949 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.739383  [    0/ 3690]\n",
      "loss: 0.535064  [  640/ 3690]\n",
      "loss: 0.752005  [ 1280/ 3690]\n",
      "loss: 0.625640  [ 1920/ 3690]\n",
      "loss: 0.593892  [ 2560/ 3690]\n",
      "loss: 0.574995  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.65854%, Avg loss: 0.645299 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.626131 \n",
      " Num Batches: 20\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.618975  [    0/ 3690]\n",
      "loss: 0.612064  [  640/ 3690]\n",
      "loss: 0.705088  [ 1280/ 3690]\n",
      "loss: 0.647385  [ 1920/ 3690]\n",
      "loss: 0.671879  [ 2560/ 3690]\n",
      "loss: 0.685499  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.30894%, Avg loss: 0.636729 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.691196  [    0/ 3690]\n",
      "loss: 0.524765  [  640/ 3690]\n",
      "loss: 0.746401  [ 1280/ 3690]\n",
      "loss: 0.715658  [ 1920/ 3690]\n",
      "loss: 0.607075  [ 2560/ 3690]\n",
      "loss: 0.549030  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 64.39024%, Avg loss: 0.654354 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.4%, Avg loss: 0.643001 \n",
      " Num Batches: 20\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.578408  [    0/ 3690]\n",
      "loss: 0.605349  [  640/ 3690]\n",
      "loss: 0.720894  [ 1280/ 3690]\n",
      "loss: 0.661494  [ 1920/ 3690]\n",
      "loss: 0.672184  [ 2560/ 3690]\n",
      "loss: 0.693471  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.60163%, Avg loss: 0.643129 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.772575  [    0/ 3690]\n",
      "loss: 0.502966  [  640/ 3690]\n",
      "loss: 0.786023  [ 1280/ 3690]\n",
      "loss: 0.630421  [ 1920/ 3690]\n",
      "loss: 0.595269  [ 2560/ 3690]\n",
      "loss: 0.574557  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 60.56911%, Avg loss: 0.648502 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.625723 \n",
      " Num Batches: 20\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.637889  [    0/ 3690]\n",
      "loss: 0.620527  [  640/ 3690]\n",
      "loss: 0.718657  [ 1280/ 3690]\n",
      "loss: 0.665643  [ 1920/ 3690]\n",
      "loss: 0.675638  [ 2560/ 3690]\n",
      "loss: 0.686753  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 63.82114%, Avg loss: 0.642603 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.697928  [    0/ 3690]\n",
      "loss: 0.502130  [  640/ 3690]\n",
      "loss: 0.732858  [ 1280/ 3690]\n",
      "loss: 0.640035  [ 1920/ 3690]\n",
      "loss: 0.599812  [ 2560/ 3690]\n",
      "loss: 0.614772  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.52033%, Avg loss: 0.650003 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.624672 \n",
      " Num Batches: 20\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.610233  [    0/ 3690]\n",
      "loss: 0.615243  [  640/ 3690]\n",
      "loss: 0.712088  [ 1280/ 3690]\n",
      "loss: 0.664137  [ 1920/ 3690]\n",
      "loss: 0.681386  [ 2560/ 3690]\n",
      "loss: 0.692069  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.92683%, Avg loss: 0.649558 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.686321  [    0/ 3690]\n",
      "loss: 0.480804  [  640/ 3690]\n",
      "loss: 0.730733  [ 1280/ 3690]\n",
      "loss: 0.621722  [ 1920/ 3690]\n",
      "loss: 0.607090  [ 2560/ 3690]\n",
      "loss: 0.612772  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.62602%, Avg loss: 0.651467 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.629537 \n",
      " Num Batches: 20\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.637668  [    0/ 3690]\n",
      "loss: 0.625529  [  640/ 3690]\n",
      "loss: 0.718616  [ 1280/ 3690]\n",
      "loss: 0.672766  [ 1920/ 3690]\n",
      "loss: 0.685264  [ 2560/ 3690]\n",
      "loss: 0.692837  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.76423%, Avg loss: 0.651350 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.682143  [    0/ 3690]\n",
      "loss: 0.490840  [  640/ 3690]\n",
      "loss: 0.722158  [ 1280/ 3690]\n",
      "loss: 0.625503  [ 1920/ 3690]\n",
      "loss: 0.596529  [ 2560/ 3690]\n",
      "loss: 0.606459  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.62602%, Avg loss: 0.652395 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.626534 \n",
      " Num Batches: 20\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.621863  [    0/ 3690]\n",
      "loss: 0.621083  [  640/ 3690]\n",
      "loss: 0.711814  [ 1280/ 3690]\n",
      "loss: 0.664979  [ 1920/ 3690]\n",
      "loss: 0.686051  [ 2560/ 3690]\n",
      "loss: 0.694676  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.46341%, Avg loss: 0.652531 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.676048  [    0/ 3690]\n",
      "loss: 0.479812  [  640/ 3690]\n",
      "loss: 0.723380  [ 1280/ 3690]\n",
      "loss: 0.617332  [ 1920/ 3690]\n",
      "loss: 0.592865  [ 2560/ 3690]\n",
      "loss: 0.601422  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 62.27642%, Avg loss: 0.652838 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.629312 \n",
      " Num Batches: 20\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.629618  [    0/ 3690]\n",
      "loss: 0.622445  [  640/ 3690]\n",
      "loss: 0.715564  [ 1280/ 3690]\n",
      "loss: 0.667152  [ 1920/ 3690]\n",
      "loss: 0.687656  [ 2560/ 3690]\n",
      "loss: 0.696828  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.21951%, Avg loss: 0.653657 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.672092  [    0/ 3690]\n",
      "loss: 0.480909  [  640/ 3690]\n",
      "loss: 0.720603  [ 1280/ 3690]\n",
      "loss: 0.608887  [ 1920/ 3690]\n",
      "loss: 0.586455  [ 2560/ 3690]\n",
      "loss: 0.593198  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.86992%, Avg loss: 0.653074 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.633263 \n",
      " Num Batches: 20\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.630268  [    0/ 3690]\n",
      "loss: 0.620393  [  640/ 3690]\n",
      "loss: 0.716544  [ 1280/ 3690]\n",
      "loss: 0.666836  [ 1920/ 3690]\n",
      "loss: 0.688136  [ 2560/ 3690]\n",
      "loss: 0.697411  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.46341%, Avg loss: 0.653814 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.672475  [    0/ 3690]\n",
      "loss: 0.480226  [  640/ 3690]\n",
      "loss: 0.714954  [ 1280/ 3690]\n",
      "loss: 0.598958  [ 1920/ 3690]\n",
      "loss: 0.578166  [ 2560/ 3690]\n",
      "loss: 0.587410  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.38211%, Avg loss: 0.654633 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.633163 \n",
      " Num Batches: 20\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.628454  [    0/ 3690]\n",
      "loss: 0.620311  [  640/ 3690]\n",
      "loss: 0.716126  [ 1280/ 3690]\n",
      "loss: 0.665488  [ 1920/ 3690]\n",
      "loss: 0.689263  [ 2560/ 3690]\n",
      "loss: 0.699693  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.95122%, Avg loss: 0.655021 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.672428  [    0/ 3690]\n",
      "loss: 0.470365  [  640/ 3690]\n",
      "loss: 0.702227  [ 1280/ 3690]\n",
      "loss: 0.586890  [ 1920/ 3690]\n",
      "loss: 0.567711  [ 2560/ 3690]\n",
      "loss: 0.586723  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.05691%, Avg loss: 0.655950 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.5%, Avg loss: 0.632252 \n",
      " Num Batches: 20\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.628696  [    0/ 3690]\n",
      "loss: 0.620154  [  640/ 3690]\n",
      "loss: 0.715217  [ 1280/ 3690]\n",
      "loss: 0.663260  [ 1920/ 3690]\n",
      "loss: 0.690246  [ 2560/ 3690]\n",
      "loss: 0.701568  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.13821%, Avg loss: 0.656160 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.674603  [    0/ 3690]\n",
      "loss: 0.458734  [  640/ 3690]\n",
      "loss: 0.697748  [ 1280/ 3690]\n",
      "loss: 0.579190  [ 1920/ 3690]\n",
      "loss: 0.559510  [ 2560/ 3690]\n",
      "loss: 0.590551  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.54472%, Avg loss: 0.656264 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.636716 \n",
      " Num Batches: 20\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.633495  [    0/ 3690]\n",
      "loss: 0.620445  [  640/ 3690]\n",
      "loss: 0.713616  [ 1280/ 3690]\n",
      "loss: 0.661283  [ 1920/ 3690]\n",
      "loss: 0.691483  [ 2560/ 3690]\n",
      "loss: 0.700560  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.46341%, Avg loss: 0.656752 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.673113  [    0/ 3690]\n",
      "loss: 0.456731  [  640/ 3690]\n",
      "loss: 0.682599  [ 1280/ 3690]\n",
      "loss: 0.574654  [ 1920/ 3690]\n",
      "loss: 0.551938  [ 2560/ 3690]\n",
      "loss: 0.603922  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.05691%, Avg loss: 0.657921 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.640411 \n",
      " Num Batches: 20\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.633490  [    0/ 3690]\n",
      "loss: 0.623697  [  640/ 3690]\n",
      "loss: 0.711699  [ 1280/ 3690]\n",
      "loss: 0.660174  [ 1920/ 3690]\n",
      "loss: 0.693224  [ 2560/ 3690]\n",
      "loss: 0.699876  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 60.48780%, Avg loss: 0.658353 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.671827  [    0/ 3690]\n",
      "loss: 0.455443  [  640/ 3690]\n",
      "loss: 0.677340  [ 1280/ 3690]\n",
      "loss: 0.567942  [ 1920/ 3690]\n",
      "loss: 0.542597  [ 2560/ 3690]\n",
      "loss: 0.592684  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 60.65041%, Avg loss: 0.657306 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.639207 \n",
      " Num Batches: 20\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.632090  [    0/ 3690]\n",
      "loss: 0.622373  [  640/ 3690]\n",
      "loss: 0.711374  [ 1280/ 3690]\n",
      "loss: 0.658488  [ 1920/ 3690]\n",
      "loss: 0.694219  [ 2560/ 3690]\n",
      "loss: 0.703597  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.13821%, Avg loss: 0.657039 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.668809  [    0/ 3690]\n",
      "loss: 0.447538  [  640/ 3690]\n",
      "loss: 0.667917  [ 1280/ 3690]\n",
      "loss: 0.564716  [ 1920/ 3690]\n",
      "loss: 0.542939  [ 2560/ 3690]\n",
      "loss: 0.599119  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 60.97561%, Avg loss: 0.658064 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.644031 \n",
      " Num Batches: 20\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.638485  [    0/ 3690]\n",
      "loss: 0.620999  [  640/ 3690]\n",
      "loss: 0.710243  [ 1280/ 3690]\n",
      "loss: 0.658688  [ 1920/ 3690]\n",
      "loss: 0.695816  [ 2560/ 3690]\n",
      "loss: 0.702593  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 60.56911%, Avg loss: 0.658368 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.655378  [    0/ 3690]\n",
      "loss: 0.448257  [  640/ 3690]\n",
      "loss: 0.655569  [ 1280/ 3690]\n",
      "loss: 0.563463  [ 1920/ 3690]\n",
      "loss: 0.541241  [ 2560/ 3690]\n",
      "loss: 0.602753  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 60.56911%, Avg loss: 0.657779 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.643463 \n",
      " Num Batches: 20\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.635318  [    0/ 3690]\n",
      "loss: 0.621636  [  640/ 3690]\n",
      "loss: 0.708723  [ 1280/ 3690]\n",
      "loss: 0.657778  [ 1920/ 3690]\n",
      "loss: 0.696978  [ 2560/ 3690]\n",
      "loss: 0.703762  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 60.97561%, Avg loss: 0.657236 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.644442  [    0/ 3690]\n",
      "loss: 0.445475  [  640/ 3690]\n",
      "loss: 0.649906  [ 1280/ 3690]\n",
      "loss: 0.556314  [ 1920/ 3690]\n",
      "loss: 0.539516  [ 2560/ 3690]\n",
      "loss: 0.606769  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 60.97561%, Avg loss: 0.657500 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.648008 \n",
      " Num Batches: 20\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "IMPROVING ADVERSARY\n",
      "loss: 0.637843  [    0/ 3690]\n",
      "loss: 0.620364  [  640/ 3690]\n",
      "loss: 0.708607  [ 1280/ 3690]\n",
      "loss: 0.656673  [ 1920/ 3690]\n",
      "loss: 0.697667  [ 2560/ 3690]\n",
      "loss: 0.704340  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 60.40650%, Avg loss: 0.657657 \n",
      "\n",
      "IMPROVING NETWORK\n",
      "loss: 0.636086  [    0/ 3690]\n",
      "loss: 0.437287  [  640/ 3690]\n",
      "loss: 0.646648  [ 1280/ 3690]\n",
      "loss: 0.554483  [ 1920/ 3690]\n",
      "loss: 0.540175  [ 2560/ 3690]\n",
      "loss: 0.599236  [ 3200/ 3690]\n",
      "Adversary Test Error: \n",
      " Accuracy: 61.13821%, Avg loss: 0.657875 \n",
      "\n",
      "Validation Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.654348 \n",
      " Num Batches: 20\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Specify hyperparamter\n",
    "loss_fn = nn.BCELoss() # binary cross entropy loss\n",
    "lrate = 1e-3 # 0.001 learning rate\n",
    "predictor_optimizer = torch.optim.Adam(predictor.parameters(), lr=lrate) # adam optmizer\n",
    "adversary_optimizer = torch.optim.Adam(adversary.parameters(), lr=lrate) # adam optimizer\n",
    "alpha = 1 # same as paper\n",
    "\n",
    "# Execute training for 20 epochs\n",
    "epochs = 20\n",
    "train_loss_per_epoch = []\n",
    "val_loss_per_epoch = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, predictor, adversary, loss_fn, predictor_optimizer, adversary_optimizer, alpha)\n",
    "    # keep track of train and val loss per epoch\n",
    "    train_loss_per_epoch.append(inference(train_dataloader, predictor, loss_fn)[1])\n",
    "    val_loss_per_epoch.append(inference(val_dataloader, predictor, loss_fn)[1])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training and validation losses after each epoch. Training loss decreases steadily with epochs. Validation loss decreases, but then begins to increase. On other trails (many many were done...), the validation loss usually didn't start increasing so early, so we do not want to reduce number of epochs. The final model, as we shall see, achieves good results, so we keep this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Epoch'),\n",
       " Text(0, 0.5, 'Loss'),\n",
       " Text(0.5, 1.0, 'Validation Loss per Epoch')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 5)\n",
    "fig.suptitle(\"Train and Val Error per Epoch\")\n",
    "ax1.plot(list(range(1,epochs+1)), train_loss_per_epoch)\n",
    "ax1.set(xlabel='Epoch', ylabel='Loss', title=\"Train Loss per Epoch\")\n",
    "ax2.plot(list(range(1,epochs+1)), val_loss_per_epoch)\n",
    "ax2.set(xlabel='Epoch', ylabel='Loss', title=\"Validation Loss per Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function that return false positive rates for black and white demographics, given a threshold, appropriately passed in as argument \"threshold\" to the fpvcalc function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Same as part 1 of this project\n",
    "'''\n",
    "def fpvcalc(threshold):\n",
    "    black_highrisk = 0\n",
    "    black_highrisk_recidivate = 0\n",
    "\n",
    "    white_highrisk = 0\n",
    "    white_highrisk_recidivate = 0\n",
    "\n",
    "    black_lowrisk = 0\n",
    "    black_lowrisk_recidivate = 0\n",
    "\n",
    "    white_lowrisk = 0\n",
    "    white_lowrisk_recidivate = 0\n",
    "\n",
    "    black_notrecidivate = 0\n",
    "    black_highrisk_notrecidivate = 0\n",
    "\n",
    "    white_notrecidivate = 0\n",
    "    white_highrisk_notrecidivate = 0\n",
    "\n",
    "    for _, row in test_df.iterrows():   \n",
    "        x = [row[x] for x in features]\n",
    "        x = torch.tensor(x).type(torch.FloatTensor)\n",
    "        pred = 1 if predictor(x).item() >= threshold else 0\n",
    "        if row['race'] == 'African-American' and pred == 1:\n",
    "            black_highrisk += 1\n",
    "            if row['two_year_recid'] == 1:\n",
    "                black_highrisk_recidivate += 1\n",
    "            else:\n",
    "                black_notrecidivate += 1\n",
    "                black_highrisk_notrecidivate += 1\n",
    "                \n",
    "        elif row['race'] != 'African-American' and pred == 1:\n",
    "            white_highrisk += 1\n",
    "            if row['two_year_recid'] == 1:\n",
    "                white_highrisk_recidivate += 1\n",
    "            else:\n",
    "                white_notrecidivate += 1\n",
    "                white_highrisk_notrecidivate += 1\n",
    "                \n",
    "        elif row['race'] == 'African-American' and pred == 0:\n",
    "            black_lowrisk += 1\n",
    "            if row['two_year_recid'] == 1:\n",
    "                black_lowrisk_recidivate += 1\n",
    "            else:\n",
    "                black_notrecidivate += 1\n",
    "                \n",
    "        else:\n",
    "            white_lowrisk += 1\n",
    "            if row['two_year_recid'] == 1:\n",
    "                white_lowrisk_recidivate += 1\n",
    "            else:\n",
    "                white_notrecidivate += 1\n",
    "    return black_highrisk_notrecidivate/black_notrecidivate, white_highrisk_notrecidivate/white_notrecidivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each threshold from 0 to 1, with increments of 0.05, determine and store false positive rates for each demographic. Plot results in one figure to determine if false positive parity is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.05\n",
      "0.1\n",
      "0.4\n",
      "0.5\n",
      "0.65\n",
      "0.7\n",
      "0.75\n",
      "0.8\n",
      "0.85\n",
      "0.9\n",
      "0.95\n",
      "1.0\n",
      "[(0.0, 0.0), (0.05, 0.011184913727286583), (0.1, 0.013837990532905797), (0.15, 0.06373110398534132), (0.2, 0.08255077111009312), (0.25, 0.07317911131470456), (0.3, 0.11515116811726983), (0.35, 0.07199572453809738), (0.4, 0.049721331500992505), (0.45, 0.050484806840739055), (0.5, 0.04344174683157734), (0.55, 0.054015880287066734), (0.6, 0.05899755687891281), (0.65, 0.03578790655061841), (0.7, 0.043040922278210415), (0.75, 0.02210261108566193), (0.8, 0.005706978164605282), (0.85, 0.0011642998931134518), (0.9, 0.002271339135745915), (0.95, 0.0), (1.0, 0.0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d43b462f10>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thresholds = [x / 20.0 for x in range(21)]\n",
    "fp_black = []\n",
    "fp_white = []\n",
    "for threshold in thresholds:\n",
    "    a,b = fpvcalc(threshold)\n",
    "    fp_black.append(a)\n",
    "    fp_white.append(b)\n",
    "    if a-b < 0.05:\n",
    "        print(threshold)\n",
    "print([(i/20, fp_black[i]-fp_white[i]) for i in range(len(fp_white))])\n",
    "\n",
    "fig\n",
    "plt.plot(thresholds, fp_black)\n",
    "plt.plot(thresholds, fp_white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, false postive parity is achieved at nearly every threshold, including threashold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show accuracy is held"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.627049\n"
     ]
    }
   ],
   "source": [
    "c, tl, n = inference(test_dataloader, predictor, loss_fn)\n",
    "print(f\"Test Error: \\n Accuracy: {(100*c):>0.1f}%, Avg loss: {tl:>8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, accuracy has improved. We have gone from accuracy of 65.3% in part 1 to 67.1%. \n",
    "\n",
    "Thus, FPR parity is satisfied without harming accuracy. Infact, our experiments confirm that using Wadsworth et al.’s network with more features than part 1 produces a more accurate classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
